<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
    type="text/xml"
    title="Profiling step"
?>
<!DOCTYPE article
[

]>

<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en" xml:id="rnotes">
  <title>Release Notes</title>
  <info>
    <releaseinfo>12.2.20170602</releaseinfo>
    <productname><phrase os="sles">SUSE Linux Enterprise Server</phrase></productname>
    <productnumber>12 SP2</productnumber>
    <date>2017-02-12</date>
    <revhistory>
      <revision>
        <date>2017-02-12</date>
      </revision>
    </revhistory>
    <meta name="series">Release Notes</meta>
    <meta name="maintainer" content="lukas.kucharczyk@suse.com"/>
    <meta name="productname">
      <productname version="12 SP2">SUSE Linux Enterprise Server</productname>
    </meta>
    <meta name="description">Latest information about new and deprecated features, improvements, and known-issues</meta>
    <meta name="social-descr">New and deprecated features, improvements, known-issues</meta>

    <abstract>
      <para>
	This document provides guidance and an overview to high level general
	features and updates for SUSE Linux Enterprise Server 12 SP2. Besides
	architecture or product-specific information, it also describes the
	capabilities and limitations of SUSE Linux Enterprise Server 12 SP2.
      </para>
      <para>
	If you are skipping one or more service packs, check the release notes
	of the skipped service packs as well. Release notes usually only list
	changes that happened between two subsequent releases. If you are only
	reading the release notes of the current release, you could miss
	important changes.
      </para>
      <para>
	General documentation can be found at:
	<link xlink:href="https://documentation.suse.com/sles/12/"/>.
      </para>
    </abstract>
  </info>
  
 <section xml:id="Intro">
  <title>SUSE Linux Enterprise Server</title>
  <remark> SLE definition: general information </remark>



  <para>
   SUSE Linux Enterprise Server is a highly reliable, scalable, and secure
   server operating system, built to power mission-critical workloads in
   both physical and virtual environments. It is an affordable,
   interoperable, and manageable open source foundation. With it,
   enterprises can cost-effectively deliver core business services, enable
   secure networks, and simplify the management of their heterogeneous IT
   infrastructure, maximizing efficiency and value.
  </para>
  <para>
   The only enterprise Linux recommended by Microsoft and SAP, SUSE Linux
   Enterprise Server is optimized to deliver high-performance
   mission-critical services, as well as edge of network, and web
   infrastructure workloads.
  </para>
  <section xml:id="Intro-Interoperability">
   <title>Interoperability and Hardware Support</title>
   <para>
    Designed for interoperability, SUSE Linux Enterprise Server integrates
    into classical Unix as well as Windows environments, supports open
    standard interfaces for systems management, and has been certified for
    IPv6 compatibility.
   </para>
   <para>
    This modular, general purpose operating system runs on four processor
    architectures and is available with optional extensions that provide
    advanced capabilities for tasks such as real time computing and high
    availability clustering.
   </para>
   <para>
    SUSE Linux Enterprise Server is optimized to run as a high performing
    guest on leading hypervisors and supports an unlimited number of virtual
    machines per physical system with a single subscription, making it the
    perfect guest operating system for virtual computing.
   </para>
  </section>
  <section xml:id="Intro-Lifecycle">
   <title>Support and Life Cycle</title>
   <para>
    SUSE Linux Enterprise Server is backed by award-winning support from
    SUSE, an established technology leader with a proven history of
    delivering enterprise-quality support services.
   </para>

   <para>
    SUSE Linux Enterprise Server 12 has a 13-year life cycle, with 10 years
    of General Support and 3 years of Extended Support. The current version
    (SP2) will be fully maintained and supported until 6 months after the
    release of SUSE Linux Enterprise Server 12 SP3.
   </para>
   <para>
    If you need additional time to design, validate and test your upgrade
    plans, Long Term Service Pack Support can extend the support you get an
    additional 12 to 36 months in twelve month increments, providing a total
    of 3 to 5 years of support on any given service pack.
   </para>
   <para>
    For more information, check our Support Policy page
    <link xlink:href="https://www.suse.com/support/policy-products/"/> or the Long Term
    Service Pack Support Page
    <link xlink:href="https://www.suse.com/products/long-term-service-pack-support/"/>.
   </para>
  </section>
  <section xml:id="Intro-New">
   <title>What Is New?</title>
   <remark> Marketing POV. </remark>
   <remark> ** Update information : link to severity ratings documentation ** Installation : change,
    new kernel, compatibility ** features : features description (not technical) ** System update :
    link to the documentation (not just the tittle) ** Update related notes : specific change to
    focus on.
    BNC#898123
   </remark>
   <para>
    SUSE Linux Enterprise Server 12 introduces a number of innovative
    changes. Here are some of the highlights:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Robustness on administrative errors and improved management
      capabilities with full system rollback based on Btrfs as the default
      file system for the operating system partition and the Snapper
      technology of SUSE.
     </para>
    </listitem>
    <listitem>
     <para>
      An overhaul of the installer introduces a new workflow that allows you
      to register your system and receive all available maintenance updates
      as part of the installation.
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Linux Enterprise Server Modules offer a choice of supplemental
      packages, ranging from tools for Web Development and Scripting,
      through a Cloud Management module, all the way to a sneak preview of
      upcoming management tooling called Advanced Systems Management.
      Modules are part of your SUSE Linux Enterprise Server subscription,
      are technically delivered as online repositories, and differ from the
      base of SUSE Linux Enterprise Server only by their life cycle. For
      more information about modules, see <xref linkend="Intro-Module"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      New core technologies like systemd (replacing the time-honored System
      V-based init process) and Wicked (introducing a modern, dynamic
      network configuration infrastructure).
     </para>
    </listitem>
    <listitem>
     <para>
      The open-source database system MariaDB is fully supported now.
     </para>
    </listitem>
    <listitem>
     <para>
      Support for open-vm-tools together with VMware for better integration
      into VMware-based hypervisor environments.
     </para>
    </listitem>
    <listitem>
     <para>
      Linux Containers are integrated into the virtualization management
      infrastructure (libvirt). Docker is provided as a fully supported
      technology.
     </para>
    </listitem>
    <listitem>
     <para>
      Support for the AArch64 architecture (64-bit ARMv8) and the 64-bit
      Little-Endian variant of the IBM POWER architecture. Additionally, we
      continue to support the Intel 64/AMD64 and IBM z Systems
      architectures.
     </para>
    </listitem>
    <listitem>
     <para>
      GNOME 3.20 gives users a modern desktop environment with a choice of
      several different look and feel options, including a special
      <emphasis>SUSE Linux Enterprise Classic</emphasis> mode for easier
      migration from earlier SUSE Linux Enterprise Desktop environments.
     </para>
    </listitem>
    <listitem>
     <para>
      For users wishing to use the full range of productivity applications
      of a Desktop with their SUSE Linux Enterprise Server, we are now
      offering SUSE Linux Enterprise Workstation Extension (requires a SUSE
      Linux Enterprise Desktop subscription).
     </para>
    </listitem>
    <listitem>
     <para>
      Integration with the new SUSE Customer Center, the new central web
      portal from SUSE to manage Subscriptions, Entitlements, and provide
      access to Support.
     </para>
    </listitem>
   </itemizedlist>

   <remark> This is Read Me First information. </remark>
   <para>
    If you are upgrading from a previous SUSE Linux Enterprise Server
    release, you should review at least the following sections:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="Intro-Support"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="InstUpgrade-Upgrade"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="TechInfo"/>
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section xml:id="Intro-Documentation">
   <title>Documentation and Other Information</title>
   <para/>
   <section>
    <title>Available on the Product Media</title>
    <itemizedlist>
     <listitem>
      <para>
       Read the READMEs on the media.
      </para>
     </listitem>
     <listitem>
      <para>
       Get the detailed change log information about a particular package
       from the RPM (where <filename>&lt;FILENAME&gt;.rpm</filename> is the
       name of the RPM):
      </para>
<screen>rpm --changelog -qp &lt;FILENAME&gt;.rpm</screen>
     </listitem>
     <listitem>
      <para>
       Check the <filename>ChangeLog</filename> file in the top level of the
       media for a chronological log of all changes made to the updated
       packages.
      </para>
     </listitem>
     <listitem>
      <para>
       Find more information in the <filename>docu</filename> directory of
       the media of SUSE Linux Enterprise Server 12 SP2. This directory
       includes PDF versions of the SUSE Linux Enterprise Server 12 SP2
       Installation Quick Start and Deployment Guides. Documentation (if
       installed) is available below the
       <filename>/usr/share/doc/</filename> directory of an installed
       system.
      </para>
     </listitem>
     <listitem>


      <para>

       These Release Notes are identical across all architectures, and the
       most recent version is always available online at
       <link xlink:href="https://documentation.suse.com/releasenotes/"/>. Some entries are
       listed twice, if they are important and belong to more than one
       section.





      </para>
     </listitem>
    </itemizedlist>
   </section>
  </section>
  <section xml:id="Intro-Sourcecode">
   <title>How to Obtain Source Code</title>


   <para>
    This SUSE product includes materials licensed to SUSE under the GNU
    General Public License (GPL). The GPL requires SUSE to provide the
    source code that corresponds to the GPL-licensed material. The source
    code is available for download at
    <link xlink:href="https://www.suse.com/source-code/"/>.
    Also, for up to three years after distribution of the SUSE product, upon
    request, SUSE will mail a copy of the source code. Requests should be
    sent by e-mail to <link xlink:href="mailto:sle_source_request@suse.com"/> or
    as otherwise instructed at
    <link xlink:href="https://www.suse.com/source-code/"/>. SUSE
    may charge a reasonable fee to recover distribution costs.
   </para>
  </section>
  <section xml:id="Intro-Support">
   <title>Support Statement for SUSE Linux Enterprise Server</title>
   <para>
    To receive support, customers need an appropriate subscription with
    SUSE. For more information, see
    <link xlink:href="https://www.suse.com/support//"/>.
   </para>
   <para>
    For information about Java versions supported in this product, see
    <xref linkend="TechInfo-Java"/>.
   </para>
   <section xml:id="Intro-Support-General" remap="Intro:Support:General">
    <title>General Support Statement</title>
    <para>
     The following definitions apply:
    </para>
    <variablelist>
     <varlistentry>
      <term>L1</term>
      <listitem>
       <para>
        Problem determination, which means technical support designed to
        provide compatibility information, usage support, ongoing
        maintenance, information gathering and basic troubleshooting using
        available documentation.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>L2</term>
      <listitem>
       <para>
        Problem isolation, which means technical support designed to analyze
        data, duplicate customer problems, isolate problem area and provide
        resolution for problems not resolved by Level 1 or alternatively
        prepare for Level 3.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>L3</term>
      <listitem>
       <para>
        Problem resolution, which means technical support designed to
        resolve problems by engaging engineering to resolve product defects
        which have been identified by Level 2 Support.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For contracted customers and partners, SUSE Linux Enterprise Server 12
     SP2 and its Modules are delivered with L3 support for all packages,
     except the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Technology Previews
      </para>
     </listitem>
     <listitem>
      <para>
       sound, graphics, fonts and artwork
      </para>
     </listitem>
     <listitem>
      <para>
       packages that require an additional customer contract
      </para>
     </listitem>
     <listitem>
      <para>
       packages provided as part of the Software Development Kit (SDK)
      </para>
     </listitem>
    </itemizedlist>
    <para>
     SUSE will only support the usage of original (e.g., unchanged or
     un-recompiled) packages.
    </para>

    <section role="notoc" xml:id="fate-321136" remap="Intro:Support:General">


     <title>Docker Orchestration Is Not Supported</title>
     <para>
      Starting with Docker 1.12, orchestration (swarm) is now a part of the
      Docker engine, as available from the SLES Containers module. This
      feature is not supported.
     </para>
    </section>

   </section>
   <section xml:id="Intro-Support-Techpreviews" remap="Intro:Support:Techpreviews">
    <title>Technology Previews</title>
    <remark> ** clear statement of TP (no support) ** Title of the TP ** full description of the TP
     ** link to official documentation (external) ** link to the SUSE's documentation </remark>

    <para>
     Technology previews are packages, stacks, or features delivered by
     SUSE. These features are not supported. They may be functionally
     incomplete, unstable or in other ways not suitable for production use.
     They are mainly included for customer convenience and give customers a
     chance to test new technologies within an enterprise environment.
    </para>
    <para>
     Technology previews can be dropped at any time and SUSE does not commit
     to providing a supported version of such technologies in the future.
    </para>
    <para>
     Give your SUSE representative feedback, including your experience and
     use case.
    </para>

    <section role="notoc" xml:id="fate-320388" remap="Intro:Support:Techpreviews">


     <title>Support for Current AMD Radeon GPUs</title>
     <para>
      <emphasis> As a technical preview, SUSE Linux Enterprise ships the
      graphics driver
      </emphasis><literal>xf86-video-amdgpu</literal><emphasis> for current
      AMD Radeon GPUs. </emphasis>
     </para>
     <para>
      <emphasis>Since this driver is still in an experimental state, it is
      not installed by default. By default, it is only enabled for one GPU
      on which it was tested successfully.</emphasis>
     </para>
     <para>
      <emphasis role="bold">Important:</emphasis> At this stage, this driver
      is not supported.
     </para>
     <para>
      To be able to use the driver, first install the package
      <literal>xf86-video-amd</literal>. Then, enable it for your GPU by
      editing <literal>/etc/X11/xorg_pci_ids</literal>.
     </para>
     <para>
      The required format is:
      <literal>\&lt;VendorID\&gt;\&lt;DeviceID\&gt;</literal>. It is also
      described in the configuration file itself.
     </para>
     <para>
      To find vendor ID and device ID, use the command:
     </para>
<screen>lspci -n | grep 0300</screen>
     <para>
      All supported vendor IDs/device IDs are already in the file but are
      commented out. For your vendor ID/device ID combination, remove the
      comment character <literal>#</literal> from the beginning of the line.
     </para>
    </section>
    <section role="notoc" xml:id="fate-316354" remap="Intro:Support:Techpreviews">


     <title>KVM Nested Virtualization</title>
     <para>
      KVM Nested Virtualization is available in SLE 12 as a technology
      preview.
     </para>
    </section>
    <section role="notoc" xml:id="fate-316274" remap="Intro:Support:Techpreviews">


     <title>Converting Physical Machines to KVM Virtual Machines</title>
     <para>
      <literal>libguestfs</literal><emphasis> has the tool
      </emphasis><literal>virt-v2v</literal><emphasis> to convert virtual
      machines from Xen to KVM. However, previously, it was not possible to
      convert physical installations to virtual machine installations.
      </emphasis>
     </para>
     <para>
      As a technology preview, SLES 12 SP2 now ships the tool
      <literal>virt-p2v</literal> in <literal>libguestfs</literal>.
      <literal>virt-p2v</literal> allows converting physical machines into
      KVM guests.
     </para>
     <para>
      This also means that <literal>libguestfs</literal> has been updated to
      a more recent version, bringing new features and fixes.
     </para>
    </section>

    <section xml:id="Intro-Support-Techpreviews-AArch64" remap="Intro:Support:Techpreviews:AArch64">
     <title>Technology Previews: AArch64 (ARMv8)</title>
     <para/>

     <section role="notoc" xml:id="fate-320649" remap="Intro:Support:Techpreviews:AArch64">


      <title>GNOME Desktop Environment as a Technology Preview on AArch64</title>
      <para>
       The GNOME desktop environment (including GNOME Shell and GDM) is now
       available on the AArch64 architecture as an unsupported technology
       preview.
      </para>
      <para>
       The only supported graphical environment on the AArch64 architecture
       is IceWM with XDM as the display manager.
      </para>
     </section>

    </section>
    <section xml:id="Intro-Support-Techpreviews-Power" remap="Intro:Support:Techpreviews:Power">
     <title>Technology Previews: POWER (ppc64le)</title>
     <para/>

     <section role="notoc" xml:id="fate-318726" remap="Intro:Support:Techpreviews:Power">


      <title>Device Driver ibmvnic Has Been Added</title>
      <para>
       vNIC (Virtual Network Interface Controller) is a new PowerVM virtual
       networking technology that delivers enterprise capabilities and
       simplifies network management. It is a high-performance, efficient
       technology that when combined with SR-IOV NIC provides bandwidth
       control Quality of Service (QoS) capabilities at the virtual NIC
       level. vNIC significantly reduces virtualization overhead resulting
       in lower latencies and fewer server resources (CPU, memory) required
       for network virtualization.
      </para>
      <para>
       This driver is a Technology Preview in SLES 12 SP2
      </para>
     </section>

    </section>
    <section xml:id="Intro-Support-Techpreviews-x86-64" remap="Intro:Support:Techpreviews:x86_64">
     <title>Technology Previews: AMD64/Intel 64 64-Bit (x86_64)</title>
     <para/>

     <section role="notoc" xml:id="fate-319792" remap="Intro:Support:Techpreviews:x86_64">


      <title>NVDIMM Support</title>
      <para>
       In SLES 12 SP2, NVDIMM support has been added as a Technology
       Preview. While many of its subsystems are stable, we recommend
       testing your specific use case and workload before using it in
       production environments.
      </para>
      <para>
       NVDIMMs have two major use cases:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         NVDIMM as a disk device for high-performance tier, metadata in
         memory, and caching
        </para>
       </listitem>
       <listitem>
        <para>
         NVDIMM as system memory to process storage data and for volatile
         caching
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Usage of NVDIMM as a disk device has been tested by SUSE on HPE Gen9
       servers and there are currently no known issues. Therefore, we plan
       to support customers running this scenario on certified systems which
       includes HPE Gen9 servers.
      </para>
      <para>
       SUSE will work together with partners to support additional use cases
       in the future.
      </para>
     </section>
     <section role="notoc" xml:id="fate-319660" remap="Intro:Support:Techpreviews:x86_64">


      <title>Guest 3D Acceleration With virtio-gpu</title>
      <para>
       <emphasis>Prior to QEMU version 2.5, virtual graphical cards had no
       3D support. Therefore, QEMU guests could not use 3D
       acceleration.</emphasis>
      </para>
      <para>
       From the perspective of the host, QEMU 2.5 and later include
       <literal>virtio-gpu</literal>. <literal>virtio-gpu</literal> allows
       rendering OpenGL commands from the guest on the GPU of the host. This
       results in a large improvement of the OpenGL 3D performance of the
       guest.
      </para>
      <para>
       From the perspective of the guest, the Linux kernel 4.4 and higher
       include the <literal>virtio-gpu</literal> driver.
      </para>
      <para>
       When attaching a <literal>virtio-gpu</literal> to a guest which has
       the Linux kernel 4.4 or higher and supports OpenGL 3D 3.x
       acceleration, the guest can use 3D acceleration and should achieve
       approximately 50 percent of native performance.
      </para>
      <para>
       Unlike VGA pass-through or using an NVIDIA GRID card,
       <literal>virtio-gpu</literal> does not need a dedicated graphical
       card or special hardware. Depending on the performance of the GPU of
       the host, virtio-gpu can also provide OpenGL 3D acceleration for
       multiple guests.
      </para>
     </section>

    </section>
   </section>

   <section xml:id="Intro-Support-External" remap="Intro:Support:External">
    <title>Software Requiring Specific Contracts</title>




    <para>
     The following packages require additional support contracts to be
     obtained by the customer in order to receive full support:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       PostgreSQL Database
      </para>
     </listitem>
     <listitem>
      <para>
       LibreOffice
      </para>
     </listitem>
    </itemizedlist>

   </section>
  </section>
  <section xml:id="Intro-ModuleExtensionRelated">
   <title>Modules, Extensions, and Related Products</title>
   <para>
    This section comprises information about modules and extensions for SUSE
    Linux Enterprise Server 12 SP2. Modules and extensions add parts or
    functionality to the system.
   </para>
   <section xml:id="Intro-Module">
    <title>Available Modules</title>
    <para>
     Modules are fully supported parts of SUSE Linux Enterprise Server with
     a different life cycle and update timeline. They are a set of packages,
     have a clearly defined scope and are delivered via an online channel
     only. Release notes for modules are contained in this document, see
     <xref linkend="Packages-Modules"/>.
    </para>
    <para>
     The following modules are available for SUSE Linux Enterprise Server 12
     SP2:
    </para>
    <informaltable>
     <tgroup cols="3">
      <colspec colnum="1" colname="name" colwidth="40*"/>
      <colspec colnum="2" colname="content" colwidth="50*"/>
      <colspec colnum="3" colname="cycle" colwidth="25*"/>
      <thead>
       <row>
        <entry>Name</entry>
        <entry>Content</entry>
        <entry>Life Cycle</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Advanced Systems Management Module</entry>
        <entry>CFEngine, Puppet, Salt and the Machinery tool</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>Certifications Module<emphasis role="bold">*</emphasis>
        </entry>
        <entry>FIPS 140-2 certification-specific packages</entry>
        <entry>Certification-dependant</entry>
       </row>
       <row>
        <entry>Containers Module</entry>
        <entry>Docker, tools, prepackaged images</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>HPC Module</entry>
        <entry>Tools and libraries related to High Performance Computing (HPC)</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>Legacy Module<emphasis role="bold">*</emphasis>
        </entry>
        <entry>Sendmail, old IMAP stack, old Java, …</entry>
        <entry>Until September 2017</entry>
       </row>
       <row>
        <entry>Public Cloud Module</entry>
        <entry>Public cloud initialization code and tools</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>Toolchain Module</entry>
        <entry>GNU Compiler Collection (GCC)</entry>
        <entry>Yearly delivery</entry>
       </row>
       <row>
        <entry>Web and Scripting Module</entry>
        <entry>PHP, Python, Ruby on Rails</entry>
        <entry>3 years, ~18 months overlap</entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
    <para>
     <emphasis role="bold">*</emphasis> Module is not available for the
     AArch64 architecture.
    </para>
    <para>
     For more information about the life cycle of packages contained in
     modules, see
     <link xlink:href="https://scc.suse.com/docs/lifecycle/sle/12/modules"/>.
    </para>
   </section>
   <section xml:id="Intro-Extension">
    <title>Available Extensions</title>
    <para>
     Extensions add extra functionality to the system and require their own
     registration key, usually at additional cost. Extensions are delivered
     via an online channel or physical media. In many cases, extensions have
     their own release notes documents that are available from
     <link xlink:href="https://documentation.suse.com/releasenotes/"/>.
    </para>
    <para>
     The following extensions are available for SUSE Linux Enterprise Server
     12 SP2:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SUSE Linux Enterprise Live Patching:
       <link xlink:href="https://www.suse.com/products/live-patching/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise High Availability Extension:
       <link xlink:href="https://www.suse.com/products/highavailability/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       Geo Clustering for SUSE Linux Enterprise High Availability Extension:
       <link xlink:href="https://www.suse.com/products/highavailability/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Real Time:
       <link xlink:href="https://www.suse.com/products/realtime/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Workstation Extension
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Additionally, there are the following extensions which are not covered
     by SUSE support agreements, available at no additional cost and without
     an extra registration key:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SUSE Package Hub: <link xlink:href="https://packagehub.suse.com/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Software Development Kit
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section xml:id="Intro-Derived-Products">
    <title>Derived and Related Products</title>
    <remark> Marketing POV. </remark>
    <para>
     This sections lists derived and related products. In many cases, these
     products have their own release notes documents that are available from
     <link xlink:href="https://documentation.suse.com/releasenotes/"/>.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SUSE Enterprise Storage
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Desktop:
       <link xlink:href="https://www.suse.com/products/desktop/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Server for SAP Applications:
       <link xlink:href="https://www.suse.com/products/sles-for-sap/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Manager:
       <link xlink:href="https://www.suse.com/products/multi-linux-manager/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE OpenStack Cloud
      </para>
     </listitem>
    </itemizedlist>
   </section>
  </section>
  <section xml:id="Intro-Certification">
   <title>Security, Standards, and Certification</title>
   <para>
    SUSE Linux Enterprise Server 12 SP2 has been submitted to the
    certification bodies for:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.commoncriteriaportal.org/">Common Criteria
      Certification</link>
     </para>
    </listitem>
    <listitem>
     <para>
      FIPS 140-2 validation, see:
      <link xlink:href="https://csrc.nist.gov/projects/cryptographic-module-validation-program/modules-in-process/modules-in-process-list"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information about certification, see
    <link xlink:href="https://www.suse.com/support/security/certifications/"/>.
   </para>
  </section>
 </section>



 <section xml:id="InstUpgrade" remap="InstUpgrade">
  <title>Installation and Upgrade</title>
  <para>
   SUSE Linux Enterprise Server can be deployed in several ways:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Physical machine
    </para>
   </listitem>
   <listitem>
    <para>
     Virtual host
    </para>
   </listitem>
   <listitem>
    <para>
     Virtual machine
    </para>
   </listitem>
   <listitem>
    <para>
     System containers
    </para>
   </listitem>
   <listitem>
    <para>
     Application containers
    </para>
   </listitem>
  </itemizedlist>

  <section role="notoc" xml:id="fate-319716" remap="InstUpgrade">


   <title>Updating the Installer at the Beginning of the Installation or Upgrade</title>
   <para>
    <emphasis>Until SLES 12 SP1, the only method of updating the installer
    was through the use of a driver update disk. This required manual work
    such as downloading the driver update and explicitly pointing the
    installer at it.</emphasis>
   </para>
   <para>
    Starting with the SLES 12 SP2 installer, at the beginning of the
    installation or upgrade, the installer can contact the update server to
    find out whether updates for the installer are available. If there are,
    they are automatically applied and YaST is restarted. The installer is
    able to download the updates from the regular update server, a local SMT
    server, or a custom URL.
   </para>
   <para>
    By default, this functionality is off. Enable this feature using the
    boot option <literal>self_update=1</literal>.
   </para>
   <para>
    For more information, see the documentation at
    <link xlink:href="https://github.com/yast/yast-installation/blob/SLE-12-SP2/doc/SELF_UPDATE.md">https://github.com/yast/yast-installation/blob/SLE-12-SP2/doc/SELF_UPDATE.md</link>.
   </para>
  </section>

  <section xml:id="InstUpgrade-Installation" remap="InstUpgrade:Installation">
   <title>Installation</title>
   <para>
    This section includes information related to the initial installation of
    SUSE Linux Enterprise Server 12 SP2. For information about installing,
    see <citetitle>Deployment Guide</citetitle> at
    <link xlink:href="https://documentation.suse.com/sles/12-SP2/html/SLES-all/book-sle-deployment.html"/>.
   </para>

   <section role="notoc" xml:id="fate-322147" remap="InstUpgrade:Installation">


    <title>Installer Crashes When Set to Mount by Label by Default</title>
    <para>
     <emphasis> When setting the default mount value to </emphasis>By
     Label<emphasis> during partitioning, the installer will report an error
     and crash. </emphasis>
    </para>
    <para>
     As a workaround, use another option for installation. If needed, switch
     back to <emphasis>By Label</emphasis> on the running system.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321358" remap="InstUpgrade:Installation">


    <title>Network Interfaces Configured via linuxrc Take Precendence</title>
    <para>
     <emphasis> For some configurations with many network interfaces, it can
     take several hours until all network interfaces are initialized (see
     </emphasis><link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=988157"><emphasis>https://bugzilla.suse.com/show_bug.cgi?id=988157</emphasis></link><emphasis>). In such cases, the installation is blocked. SLE 12 SP1 and earlier
     did not offer a workaround for this behavior. </emphasis>
    </para>
    <para>
     Starting with SLE 12 SP2, you can speed up interactive installations on
     systems with many network interfaces by configuring them via linuxrc.
     When a network interface is configured via linuxrc, YaST will not
     perform automatic DHCP configuration for any interface. Instead, YaST
     will continue to use the configuration from linuxrc.
    </para>
    <para>
     To configure a particular interface via linuxrc, add the following to
     the boot command line before starting the installation:
    </para>
<screen>ifcfg=eth0=dhcp</screen>
    <para>
     In the parameter, replace <literal>eth0</literal> with the name of the
     appropriate network interface. The <literal>ifcfg</literal> option can
     be used multiple times.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320494" remap="InstUpgrade:Installation">


    <title>Media-based Sources Are Disabled After Installation If They Are Not Needed</title>
    <para>
     <emphasis>Previously, when installing from local media, like a CD/DVD
     or USB drive, these sources remained enabled after the
     installation.</emphasis>
    </para>
    <para>
     <emphasis>This could cause problems during software installation,
     upgrade or migration because an old or obsolete installation source
     remained there. Additionally, if the source was physically removed (for
     instance, by ejecting the CD/DVD), Zypper would complain about the
     source not being available.</emphasis>
    </para>
    <para>
     After the installation, YaST will now check every local source to
     determine if the product they provide is also available through a
     remote repository. In that case, the local source will be disabled.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320111" remap="InstUpgrade:Installation">


    <title>Partitioning Proposal: "Flexible Partitioning" Feature Has Been Removed</title>
    <para>
     <emphasis>YaST is a highly configurable installer that allows setting
     very different behaviors for each product using it (SUSE Enterprise
     Linux, openSUSE, etc.). In previous versions of YaST, it was possible
     to use a feature called "Flexible Partitioning". This feature has
     become obsolete, as the more standard proposal mechanism has been used
     by SLE and openSUSE in all recent releases.</emphasis>
    </para>
    <para>
     The new version of YaST detects when a (modified) installer tries to
     use the obsolete "Flexible Partitioning" feature, alerts the user and
     falls back to the standard proposal mechanism automatically.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319893" remap="InstUpgrade:Installation">


    <title>YaST Clears New Partitions</title>
    <para>
     <emphasis>Previously, when YaST created a new partition, there could be
     signatures of previous MD RAIDs on the partition. That caused the MD
     RAID to be auto-assembled which made the partition busy. Thus,
     subsequent commands on the new partition failed.</emphasis>
    </para>
    <para>
     When creating partitions with YaST, storage signatures are now deleted
     before auto-assembly takes place.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319639" remap="InstUpgrade:Installation">


    <title>Host Name Setting During Installation</title>
    <para>
     During installation, the hostname is set to <literal>install</literal>, the DHCP-provided value, if any, or the value of the boot option
     <literal>hostname</literal>. The host name used during installation is
     not propagated to <literal>/etc/hostname</literal> of the installed
     system except when set using the boot option
     <literal>hostname</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319624" remap="InstUpgrade:Installation">


    <title>More Explicit and Configurable Importing of SSH Host Keys</title>
    <para>
     <emphasis>During an installation of SUSE Linux Enterprise, existing SSH
     host keys from a previous installation were imported into the new
     system. This is convenient in some network scenarios, but as it was
     done without explicitly informing the user, it could lead to undesired
     situations.</emphasis>
    </para>
    <para>
     The installer no longer silently imports SSH host keys from the most
     recent Linux installation on the disk. It now allows you to choose
     whether to import SSH host keys and from which partition they should be
     imported. It is now also possible to import the rest of the SSH
     configuration in addition to the keys.
    </para>
    <para>
     To import previous SSH host keys and configuration during the
     installation, proceed until the page <emphasis>Installation
     Summary</emphasis>, then choose <emphasis>Import SSH Host Keys and
     Configuration</emphasis>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-317970" remap="InstUpgrade:Installation">


    <title>Option to Create AutoYaST Profile During Installation Has Been Removed</title>
    <para>
     <emphasis>In earlier versions of SUSE Linux Enterprise, you could clone
     the system configuration as an AutoYaST profile during installation.
     However, many services and system parameters can only be configured
     after the installation process has completed and the system is up and
     running. This can result in an AutoYaST profile missing parts of the
     desired configuration.</emphasis>
    </para>
    <para>
     The option of creating an AutoYaST profile during installation has been
     removed. However, you can still create an AutoYaST profile from the
     running system, after you have made sure that the system configuration
     fits your needs.
    </para>
   </section>
   <section role="notoc" xml:id="fate-316796" remap="InstUpgrade:Installation">


    <title>Reading Registration Codes from a USB Drive</title>
    <para>
     <emphasis>During the installation of SUSE products, it can be tedious
     to remember and type in registration codes.</emphasis>
    </para>
    <para>
     You can now save the registration codes to a USB drive and have YaST
     read them automatically.
    </para>
    <para>
     For more information, see:
     <link xlink:href="https://github.com/yast/yast-registration/wiki/Loading-Registration-Codes-From-an-USB-Storage-%28Flash-Drive-HDD%29"/>.
    </para>
   </section>

  </section>
  <section xml:id="InstUpgrade-Upgrade" remap="InstUpgrade:Upgrade">
   <title>Upgrade-Related Notes</title>
   <para>
    This section includes upgrade-related information for SUSE Linux
    Enterprise Server 12 SP2. For information about general preparations and
    supported upgrade methods and paths, see the documentation at
    <link xlink:href="https://documentation.suse.com/sles/12-SP2/"/>.
   </para>

   <section role="notoc" xml:id="fate-326567" remap="InstUpgrade:Upgrade">


    <title>Product Registration Changes for HPC Customers</title>
    <para>
     <emphasis>For SUSE Linux Enterprise 12, there was a High Performance
     Computing subscription named "SUSE Linux Enterprise Server for HPC"
     (SLES for HPC). With SLE 15, this subscription does not exist anymore
     and has been replaced. The equivalent subscription is named "SUSE Linux
     Enterprise High Performance Computing" (SLE-HPC) and requires a
     different license key. Because of this requirement, a SLES for HPC 12
     system will by default upgrade to a regular "SUSE Linux Enterprise
     Server".</emphasis>
    </para>
    <para>
     To properly upgrade a SLES for HPC system to a SLE-HPC, the system
     needs to be converted to SLE-HPC first. SUSE provides a tool to
     simplify this conversion by performing the product conversion and
     switch to the SLE-HPC subscription. However, the tool does not perform
     the upgrade itself.
    </para>
    <para>
     When run without extra parameters, the script assumes that the SLES for
     HPC subscription is valid and not expired. If the subscription has
     expired, you need to provide a valid registration key for SLE-HPC.
    </para>
    <para>
     The script reads the current set of registered modules and extensions
     and after the system has been converted to SLE-HPC, it tries to add
     them again.
    </para>
    <important>
     <title>Providing a Registration Key to the Conversion Script</title>
     <para>
      The script cannot restore the previous registration state if the
      supplied registration key is incorrect or invalid.
     </para>
    </important>
    <orderedlist>
     <listitem>
      <para>
       To install the script, run <literal>zypper in
       switch_sles_sle-hpc</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Execute the script from the command line as <literal>root</literal>:
      </para>
<screen>switch_sles_sle-hpc -e &lt;REGISTRATION_EMAIL&gt; -r &lt;NEW_REGISTRATION_KEY&gt;</screen>
      <para>
       The parameters <literal>-e</literal> and <literal>-r</literal> are
       only required if the previous registration has expired, otherwise
       they are optional. To run the script in batch mode, add the option
       <literal>-y</literal>. It answers all questions with
       <emphasis>yes</emphasis>.
      </para>
     </listitem>
    </orderedlist>
    <para>
     For more information, see the man page
     <literal>switch_sles_sle-hpc(8)</literal> and
     <literal>README.SUSE</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321493" remap="InstUpgrade:Upgrade">


    <title>Online Migration with Live Patching Enabled</title>
    <para>
     <emphasis>The SLES online migration process reports package conflicts
     when Live Patching is enabled and the kernel is being upgraded. This
     applies when crossing the boundary between two Service
     Packs.</emphasis>
    </para>
    <para>
     To prevent the conflicts, before starting the migration, execute the
     following as a super user:
    </para>
<screen>zypper rm $(rpm -qa kgraft-patch-*)</screen>
   </section>
   <section role="notoc" xml:id="fate-320358" remap="InstUpgrade:Upgrade">


    <title>Support for PIDs cgroup Controller</title>
    <para>
     <emphasis> The version of systemd shipped in SLES 12 SP2 uses the PIDs
     cgroup controller. This provides some per-service
     </emphasis><literal>fork()</literal><emphasis> bomb protection, leading
     to a safer system. </emphasis>
    </para>
    <para>
     <emphasis>However, under certain circumstances you may notice
     regressions. The limits have already been raised above the upstream
     default values to avoid this but the risk remains.</emphasis>
    </para>
    <para>
     If you notice regressions, you can change a number of
     <literal>TasksMax</literal> settings.
    </para>
    <para>
     To control the default <literal>TasksMax=</literal> setting for
     services and scopes running on the system, use the
     <literal>system.conf</literal> setting
     <literal>DefaultTasksMax=</literal>. This setting defaults to
     <literal>512</literal>, which means services that are not explicitly
     configured otherwise will only be able to create <literal>512</literal>
     processes or threads at maximum.
    </para>
    <para>
     For thread- or process-heavy services, you may need to set a higher
     <literal>TasksMax</literal> value. In such cases, set
     <literal>TasksMax</literal> directly in the specific unit files. Either
     choose a numeric value or even <literal>infinity</literal>.
    </para>
    <para>
     Similarly, you can limit the total number of processes or tasks each
     user can own concurrently. To do so, use the
     <literal>logind.conf</literal> setting <literal>UserTasksMax</literal>
     (the default is <literal>12288</literal>).
    </para>
    <para>
     <literal>nspawn</literal> containers now also have a
     <literal>TasksMax</literal> value set, with a default of
     <literal>16384</literal>.
    </para>
   </section>

  </section>
  <section>
   <title>For More Information</title>
   <para>
    For more information, see
    <xref linkend="InfraPackArch-ArchIndependent"/> and the sections
    relating to your respective hardware architecture.
   </para>
  </section>
 </section>

 <section xml:id="InfraPackArch-ArchIndependent">
  <title>Architecture Independent Information</title>
  <remark> This is highly technical * Know issues categorized ** full description of the problem **
   how to reproduce the issue ** link to Bugzilla number ** command line change/configuration change
   ** work around if available
  </remark>
  <para>
   Information in this section pertains to all architectures supported by
   SUSE Linux Enterprise Server 12 SP2.
  </para>
  <section xml:id="InfraPackArch-ArchIndependent-Kernel" remap="InfraPackArch:ArchIndependent:Kernel">
   <title>Kernel</title>
   <para/>

   <section role="notoc" xml:id="fate-323648" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>MSR Cannot Be Modified When UEFI Secure Boot Is On</title>
    <para>
     <emphasis> Write access to MSRs (model-specific registers) allows
     userspace applications to modify the running kernel which runs counter
     to the goals of UEFI Secure Boot. Therefore, MSRs cannot be written to
     by tools like </emphasis><literal>cpupower</literal><emphasis> to
     control processor performance when UEFI Secure Boot is enabled on a
     system. </emphasis>
    </para>
    <para>
     <emphasis> This also prevents tuning tools, such as
     </emphasis><literal>saptune</literal><emphasis> and
     </emphasis><literal>sapconf</literal><emphasis> from working correctly.
     </emphasis>
    </para>
    <para>
     The only current workaround is disabling Secure Boot.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320874" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>ACPI Power Meter Driver Is Disabled by Default</title>
    <para>
     <emphasis> The ACPI power meter device
     </emphasis><literal>acpi_power_meter</literal><emphasis> requires
     processing of AML code from the ACPI tables to update the average power
     measurement. This can interrupt the CPU at relatively high frequency
     and has a noticeable impact to latency-sensitive applications.
     </emphasis>
    </para>
    <para>
     There are cluster monitoring applications that consume information from
     <literal>acpi_power_meter</literal>, so the driver is not removed.
     However, in SLE 12 SP2, it is blacklisted by default.
    </para>
    <para>
     In the event that a monitoring application requires it, it can be
     re-enabled by removing the driver from the blacklist file
     <literal>/etc/modprobe.d/50-blacklist.conf</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320635" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Transparent Huge Page Defragmentation Disabled by Default</title>
    <para>
     <emphasis> Transparent Huge Pages (THP) are an important alternative to
     </emphasis><literal>hugetlbfs</literal><emphasis> that boosts
     performance for some applications by reducing the amount of work a CPU
     must do when translating virtual to physical addresses. It is
     particularly important for virtual machine performance where there are
     two translation layers. </emphasis>
    </para>
    <para>
     <emphasis>Early in the lifetime of the system, there is enough free
     memory that these pages can be allocated cheaply. When the system is
     running for long enough, memory must be reclaimed and compacted to
     allocate the THP. This forces applications to stall for potentially
     long periods of time which many applications cannot tolerate. Many
     tuning guides recommend disabling THP in these types of
     cases.</emphasis>
    </para>
    <para>
     SLE 12 SP2 disables THP defragmentation by default. THPs will only be
     used if they are available, instead of stalling on defragmentation.
     Normally, the defragmentation work is deferred and THPs will be created
     in the future. However, if an application explicitly requests such
     behavior via <literal>madvise()</literal>, it will stall.
    </para>
    <para>
     If a system has many applications that are willing to stall while
     allocating THP, it is possible to restore the previous behavior of SLE
     via <literal>sysfs</literal>:
    </para>
<screen>echo always &gt; /sys/kernel/mm/transparent_hugepage/defrag</screen>
   </section>
   <section role="notoc" xml:id="fate-320496" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Enabling Enhanced Information About Physical Memory Page Ownership and Status</title>
    <para>
     <emphasis>Detailed information about physical memory pages can help
     answer questions such as:</emphasis>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Which kernel subsystem or driver has allocated which
       pages?</emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>What page status flags are set?</emphasis>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     <emphasis>This is useful for L3 support of the kernel and during
     development and testing of out-of-tree kernel modules, for example, to
     debug memory leaks. Previously, kernel interfaces could only provide a
     subset of the page status flags, and only provide a summary about
     generic memory usage categories.</emphasis>
    </para>
    <para>
     The Linux kernel shipped with SLE 12 SP2 can provide more detailed
     information. However, tracking extra information about each page that
     the kernel allocates creates overhead in terms of code to be executed
     and memory used. Therefore, this feature is disabled by default.
    </para>
    <para>
     This feature is shipped with all kernel versions of SLE 12 SP2 and can
     be enabled during boot using the kernel parameter
     <literal>page_owner=on</literal>.
    </para>
    <para>
     To obtain the status of all pages, use:
    </para>
<screen>cat /sys/kernel/debug/page_owner &gt; file</screen>
    <para>
     The file contains the following for each physical page:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Allocation flags
      </para>
     </listitem>
     <listitem>
      <para>
       Status flags
      </para>
     </listitem>
     <listitem>
      <para>
       Page migration status
      </para>
     </listitem>
     <listitem>
      <para>
       Backtrace leading to the allocation
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Additional postprocessing of the output can be used, for example, to
     count the number of pages for each unique backtrace which can help
     discover a code path that leaks memory.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320486" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Subset of Scheduler Debugging Statistics Disabled by Default</title>
    <para>
     <emphasis>The CPU scheduler maintains a number of statistics for the
     purposes of debugging, some tracepoints and sleep profiling. They are
     only useful for detailed analysis but they incur an overhead for all
     users. They may be disabled at kernel build time but they are enabled
     as debugging in the field is important and tools like latencytop depend
     on them.</emphasis>
    </para>
    <para>
     Some expensive scheduler debugging statistics are disabled by default.
     Enabling sleep profiling or running <literal>latencytop</literal> will
     activate them automatically but activating the tracepoints will require
     user intervention. The affected tracepoints are
     <literal>sched_stat_wait</literal>,
     <literal>sched_stat_sleep</literal>,
     <literal>sched_stat_iowait</literal>,
     <literal>sched_stat_blocked</literal> and
     <literal>sched_stat_runtime</literal>.
    </para>
    <para>
     They can be activated at runtime using:
    </para>
<screen>echo 1 &gt; /sys/kernel/debug/tracing/events/sched/enable</screen>
    <para>
     They can be disabled at runtime using:
    </para>
<screen>echo 0 &gt; /sys/kernel/debug/tracing/events/sched/enable</screen>
    <para>
     The first number of tracepoint activations may contain stale data until
     the necessary data is collected. If this is undesirable, it is possible
     to activate them at boot time via the kernel parameter
     <literal>schedstats=enable</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320305" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Incompatible Changes in the New 4.4 Kernel</title>
    <para>
     The following minor changes have been identified in the 4.4 kernel:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Support for TCP Limited Slow Start (RFC3742) has been removed. This
       feature had multiple drawbacks and questionable benefit. Its
       implementation was inefficient and difficult to configure. The
       problem that Limited Slow Start was trying to solve is now better
       covered by the Hybrid Slow Start algorithm which is part of default
       congestion control algorithm, CUBIC.
      </para>
     </listitem>
     <listitem>
      <para>
       The <literal>kernel.blk_iopoll</literal> sysctl has been removed.
       This setting allowed toggling some block device drivers between
       iopoll and non-iopoll mode. This allowed for easier debugging of
       these drivers during early development. Since using this toggle was
       dangerous and the toggle is not needed for production setups, it has
       been removed.
      </para>
     </listitem>
     <listitem>
      <para>
       The <literal>cgroup.event_control</literal> file is only available in
       cgroups with a memcg attached to it. There was no code using this
       interface outside of memcg, so this change is considered harmless.
      </para>
     </listitem>
     <listitem>
      <para>
       The <literal>vm.scan_unevictable_pages</literal> sysctl has been
       removed because the functionality it was backing had been removed in
       2011. Any usage of the file has been reported to the kernel log with
       an explanation that the file has no effect. There were no reports
       about a use case requiring this functionality.
      </para>
     </listitem>
     <listitem>
      <para>
       The
       <literal>/sys/devices/system/memory/memory%d/end_phys_index</literal>
       file has been removed, because the information it exposed is
       considered internal to the kernel and an implementation detail. This
       information is not required for the memory hotplug functionality.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-319143" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Partial Memory Mirroring</title>
    <para>
     <emphasis>Memory mirroring offers increased system reliability.
     However, full memory mirroring also dramatically decreases available
     memory size.</emphasis>
    </para>
    <para>
     Partial memory mirroring addresses this issue by setting up a smaller
     mirrored memory range and using this range for kernel code and data
     structures. The remaining memory operates in regular mode which leaves
     more room for applications. This feature requires support in hardware
     and EFI firmware and is currently supported on Fujitsu PRIMEQUEST 2000
     series systems and its successor models.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318015" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Paravirtualization Layer for Spinlocks</title>
    <para>
     <emphasis> To overcome issues like vCPU starvation (where a busy task
     waits on a scheduled out lock owner), paravirtualized spinlocks allow
     virtual environments, such as KVM and Xen, to replace the native
     spinlock implementation. This hypervisor replacement is tailored to be
     virtualization-friendly, for example, with it, after a period of
     busy-waiting, tasks yield the CPU. This could be enabled using the
     kernel parameter
     </emphasis><literal>CONFIG_PARAVIRT_SPINLOCK</literal><emphasis>.
     </emphasis>
    </para>
    <para>
     <emphasis>However, in the past, this incurred a considerable
     performance overhead on native systems due to the extra indirection
     layer. If enabled, virtual systems would perform better, but native
     systems would suffer. When the parameter was disabled, the opposite was
     true.</emphasis>
    </para>
    <para>
     With new features in the SLE 12 SP2 kernel, such as queued spinlocks,
     the overhead of the kernel parameter
     <literal>CONFIG_PARAVIRT_SPINLOCK</literal> is now negligible across
     systems and loads. Therefore, enabling this option by default allows
     the virtual environments to overcome the lock holder preemption
     challenges without impacting the native case. This is particularly
     useful in CPU overcommitment configurations, which are common, for
     example, in cloud-based solutions.
    </para>
   </section>
   <section role="notoc" xml:id="fate-313774" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Enhanced Accounting and Reporting of shmem Swap Usage</title>
    <para>
     <emphasis> There was a request to provide information about how much of
     Linux-kernel shared memory (</emphasis><literal>shmem</literal><emphasis>) is swapped out, for
     processes using such memory segments.
     </emphasis><literal>shmem</literal><emphasis> mappings are either
     System V shared memory segments, mappings created by
     </emphasis><literal>mmap()</literal><emphasis> with
     </emphasis><literal>MAP_ANONYMOUS</literal><emphasis> /
     </emphasis><literal>MAP_SHARED</literal><emphasis> flags, and shared
     </emphasis><literal>mmap()</literal><emphasis> mappings of files
     residing on the tmpfs RAM disk file system. Prior to the implemented
     changes, in </emphasis><literal>/proc/pid/smaps</literal><emphasis>,
     swap usage for these segments would have been shown as
     </emphasis><literal>0</literal><emphasis>. </emphasis>
    </para>
    <para>
     The kernel has been modified to show swap usage of
     <literal>shmem</literal> segments properly in
     <literal>/proc/pid/smaps</literal> files. Due to
     <literal>shmem</literal> implementation limitations, this value will
     also count swapped-out pages that the process has mapped, but never
     touched, which differs from anonymous memory accounting. Due to the
     same limitations and to prevent excessive CPU overhead, the
     <literal>VmSwap</literal> field in <literal>/proc/pid/status</literal>
     is unaffected and will not account for swapped-out portions of
     <literal>shmem</literal> mappings. In addition, the
     <literal>/proc/pid/status</literal> file has been enhanced to include
     three new <literal>Rss*</literal> fields as a breakdown of the
     <literal>VmRSS</literal> field to <literal>anonymous</literal>,
     <literal>file</literal> and <literal>shmem</literal> mappings. Example
     excerpt:
    </para>
<screen>VmRSS:              5108 kB
RssAnon:              92 kB
RssFile:            1324 kB
RssShmem:           3692 kB</screen>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Kernel-Modules" remap="InfraPackArch:ArchIndependent:Kernel_Modules">
   <title>Kernel Modules</title>


   <para>
    An important requirement for every enterprise operating system is the
    level of support customers receive for their environment. Kernel modules
    are the most relevant connector between hardware
    (<quote>controllers</quote>) and the operating system.
   </para>
   <para>

    For more information about the handling of kernel modules, see the SUSE
    Linux Enterprise Administration Guide.
   </para>

   <section role="notoc" xml:id="fate-319858" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>NVDIMM Kernel Subsystem</title>
    <para>
     <emphasis>Non-volatile DIMMs are byte-addressable memory chips that fit
     inside a computer's normal memory slot but are, in contrast to DRAM
     chips, persistent and thus can be used as an enhancement or replacement
     for a computer's hard disk drives. This imposes several challenges,
     namely:</emphasis>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Discovery of hardware</emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Mapping and addressing of this new memory type</emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Atomic semantics as with traditional storage
       media</emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Page frame addressing like with traditional
       memory</emphasis>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The Linux kernel shipped with SLE now includes several drivers to
     address these challenges:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Hardware discovery is initiated via the ACPI NFIT (Non-Volatile
       Memory Firmware Interface Table) mechanism and realized with the
       device driver <literal>nfit.ko</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Mapping and addressing of NVDIMMs is accomplished by the device
       driver <literal>nd_pmem.ko</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       The driver <literal>nd_btt.ko</literal> takes care of (optional)
       atomic read/write semantics to the underlying hardware.
      </para>
     </listitem>
     <listitem>
      <para>
       The pfn portion of <literal>nd_pmem.ko</literal> provides the ability
       to address NVDIMM memory just like any other DRAM type memory.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-319691" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>Direct Access to Files in Non-Volatile DIMMs</title>
    <para>
     <emphasis> The page cache is usually used to buffer reads and writes to
     files. It is also used to provide the pages which are mapped into
     userspace by a call to </emphasis><literal>mmap</literal><emphasis>.
     For block devices that are memory-like, the page cache pages would be
     unnecessary copies of the original storage. </emphasis>
    </para>
    <para>
     The Direct Access (DAX) kernel code avoids the extra copy by directly
     reading from and writing to the storage device. For file mappings, the
     storage device is mapped directly into userspace. This functionality is
     implemented in the XFS and Ext4 file systems.
    </para>
    <para>
     Non-volatile DIMMs can be "partitioned" into so-called namespaces which
     are then exposed as block devices by the Linux kernel. Each namespace
     can be configured in several modes. Although DAX functionality is
     available for file systems on top of namespaces in both
     <emphasis>raw</emphasis> or <emphasis>memory</emphasis> modes, SUSE
     does not support use of the DAX feature in file systems on top of
     <emphasis>raw</emphasis> mode namespaces as they have unexpected quirks
     and in future releases the feature is likely to go away completely.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318957" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>ZRAM Block Device</title>
    <para>
     The ZRAM module creates RAM-based block devices. Pages written to these
     disks are compressed and stored in memory itself. Such disks allow for
     very fast I/O. Additionally, compression provides memory savings.
    </para>
    <para>
     ZRAM devices can be managed and configured with the help of the tool
     <literal>zramctl</literal> (see the man page of
     <literal>zramctl(8)</literal>). Configuration persistence is ensured
     by <literal>zramcfg</literal> system service.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318829" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>Memory Compression with zswap</title>
    <para>
     <emphasis>Usually, when a system's physical memory is exceeded, the
     system moves some memory onto reserved space on a hard drive, called
     "swap" space. This frees physical memory space for additional use.
     However, this process of "swapping" memory onto (and off a hard drive
     is much slower than direct memory access, so it can slow down the
     entire system.</emphasis>
    </para>
    <para>
     The <literal>zswap</literal> driver inserts itself between the system
     and the swap hard drive, and instead of writing memory to a hard drive,
     it compresses memory. This speeds up both writing to swap and reading
     from swap, which results in better overall system performance while
     using swap.
    </para>
    <para>
     To enable the <literal>zswap</literal> driver, write
     <literal>1</literal> or <literal>Y</literal> to the file
     <literal>/sys/module/zswap/parameters/enabled</literal>.
    </para>
    <bridgehead renderas="sect5">Storage Back-ends</bridgehead>
    <para>
     There are two back-ends available for storing compressed pages,
     <literal>zbud</literal> (the default), and <literal>zsmalloc</literal>. The two back-ends each have their own advantages and disadvantages:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The effective compression ratio of <literal>zbud</literal> cannot
       exceed 50 percent. That is, it can at most store two uncompressed
       pages in one compressed page. If the workload's compression ratio
       exceeds 50% for all pages, <literal>zbud</literal> will not be able
       to save any memory.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>zsmalloc</literal> can achieve better compression ratios.
       However, it is more complex and its performance is less predictable.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>zsmalloc</literal> does not free pages when the limit set in
       <literal>/sys/module/zswap/parameters/max_pool_percent</literal> is
       reached. This is reflected by the counter
       <literal>/sys/kernel/debug/zswap/reject_reclaim_fail</literal>.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     It is not possible to give a general recommendation on which storage
     back-end should be used, as the decision is highly dependent on
     workload. To change the storage back-end, write either
     <literal>zbud</literal> or <literal>zsmalloc</literal> to the file
     <literal>/sys/module/zswap/parameters/zpool</literal>. Pick the
     back-end before enabling zswap. Changing it later is unsupported.
    </para>
    <bridgehead renderas="sect5">Setting
            zswap
            Memory</bridgehead>
    <para>
     Compressed memory still uses a certain amount of memory, so
     <literal>zswap</literal> has a limit to the amount of memory which will
     be stored compressed, which is controllable through the file
     <literal>/sys/module/zswap/parameters/max_pool_percent</literal>. By
     default, this is set to <literal>20</literal>, which indicates
     <literal>zswap</literal> will use 20 percent of the total system
     physical memory to store compressed memory.
    </para>
    <para>
     The <literal>zswap</literal> memory limit has to be carefully
     configured. Setting the limit too high can lead to premature
     out-of-memory situations that would not exist without
     <literal>zswap</literal>, if the memory is filled by non-swappable
     non-reclaimable pages. This includes mlocked memory and pages locked by
     drivers and other kernel users.
    </para>
    <para>
     For the same reason, performance can also be hurt by
     compression/decompression if the current workload's workset would, for
     example, fit into 90 percent of the available RAM, but 20 percent of
     RAM is already occupied by <literal>zswap</literal>. This means that
     the missing 10 percent of uncompressed RAM would constantly be swapped
     out of/in to the memory area compressed by <literal>zswap</literal>,
     while the rest of the memory compressed by <literal>zswap</literal>
     would hold pages that were swapped out earlier which are currently
     unused. There is no mechanism that would result in gradual writeback of
     those unused pages to let the uncompressed memory grow.
    </para>
    <bridgehead renderas="sect5">Freeing
            zswap
            Memory</bridgehead>
    <para>
     <literal>zswap</literal> will only free its pages in certain
     situations:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The processes using the pages free the pages or exit
      </para>
     </listitem>
     <listitem>
      <para>
       When the storage back-end <literal>zbud</literal> is in use,
       <literal>zswap</literal> will also free memory when its configured
       memory limit is exceeded. In this case, the oldest
       <literal>zswap</literal> pages are written back to disk-based swap.
      </para>
     </listitem>
    </itemizedlist>
    <bridgehead renderas="sect5">Memory Allocation Issues</bridgehead>
    <para>
     In theory, it can happen that <literal>zswap</literal> is not yet
     exceeding its memory limit, but already fails to allocate memory to
     store compressed pages. In that case, it will refuse to compress any
     new pages and they will be swapped to disk immediately. For
     confirmation whether this issue is occurring, check the value of
     <literal>/sys/kernel/debug/zswap/reject_alloc_fail</literal>.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Security" remap="InfraPackArch:ArchIndependent:Security">
   <title>Security</title>
   <para/>

   <section role="notoc" xml:id="fate-321984" remap="InfraPackArch:ArchIndependent:Security">


    <title>iSCSI with CHAP Is Not Supported in FIPS Mode</title>
    <para>
     <emphasis>iSCSI's use of the Challenge-Handshake Authentication
     Protocol (CHAP) is not supported in FIPS mode. The protocol uses a
     digest algorithm (MD5) that is not FIPS-compliant. If FIPS mode is
     enabled, iSCSI will not be able to use CHAP.</emphasis>
    </para>
    <para>
     If operation in FIPS mode is required, discontinue use of CHAP for
     iSCSI and secure the network by other means such as IPSec.
    </para>
   </section>
   <section role="notoc" xml:id="fate-317116" remap="InfraPackArch:ArchIndependent:Security">


    <title>SELinux Enablement</title>
    <para>
     SELinux capabilities have been added to SUSE Linux Enterprise Server
     (in addition to other frameworks, such as AppArmor). While SELinux is
     not enabled by default, customers can run SELinux with SUSE Linux
     Enterprise Server if they choose to.
    </para>
    <para>
     SELinux Enablement includes the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The kernel ships with SELinux support.
      </para>
     </listitem>
     <listitem>
      <para>
       We will apply SELinux patches to all “common” userland packages.
      </para>
     </listitem>
     <listitem>
      <para>
       The libraries required for SELinux (<literal>libselinux</literal>,
       <literal>libsepol</literal>, <literal>libsemanage</literal>, etc.)
       have been added SUSE Linux Enterprise.
      </para>
     </listitem>
     <listitem>
      <para>
       Quality Assurance is performed with SELinux disabled—to make sure
       that SELinux patches do not break the default delivery and the
       majority of packages.
      </para>
     </listitem>
     <listitem>
      <para>
       The SELinux-specific tools are shipped as part of the default
       distribution delivery.
      </para>
     </listitem>
     <listitem>
      <para>
       SELinux policies are not provided by SUSE. Supported policies may be
       available from the repositories in the future.
      </para>
     </listitem>
     <listitem>
      <para>
       Customers and Partners who have an interest in using SELinux in their
       solutions are encouraged to contact SUSE to evaluate their necessary
       level of support and how support and services for their specific
       SELinux policies will be granted.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     By enabling SELinux in our code base, we add community code to offer
     customers the option to use SELinux without replacing significant parts
     of the distribution.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Network" remap="InfraPackArch:ArchIndependent:Network">
   <title>Networking</title>
   <para/>

   <section role="notoc" xml:id="fate-322007" remap="InfraPackArch:ArchIndependent:Network">


    <title>Improved Bridge Handling in YaST</title>
    <para>
     <emphasis>The configuration UI for bridges in YaST did not always show
     all information and did not always convert parameters properly when
     editing older configurations.</emphasis>
    </para>
    <para>
     In SLE 12 SP2, this behavior has been improved upon:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The information about bridge ports and bridges is shown for each
       interface.
      </para>
     </listitem>
     <listitem>
      <para>
       In the case of old configuration, upon reading the configuration, the
       bootproto <literal>static</literal> will be converted to
       <literal>none</literal> and the parameter <literal>zero
       IPADDR</literal> will be removed.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Additionally, to improve the user experience, the management of bridges
     and bonding has been unified and the interface is now updated after any
     change.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320709" remap="InfraPackArch:ArchIndependent:Network">


    <title>No Support for Samba as Active Directory-Style Domain Controller</title>
    <para>
     The version of Samba shipped with SLE 12 GA and newer does not include
     support to operate as an Active Directory-style domain controller. This
     functionality is currently disabled, as it lacks integration with
     system-wide MIT Kerberos.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320270" remap="InfraPackArch:ArchIndependent:Network">


    <title>xrdp Supports More Concurrent Sessions</title>
    <para>
     <emphasis>xrdp assigns port numbers incrementally in sequence to each
     new Xorg session, and the port number starts counting from a hard coded
     number 5900. This causes port conflicts with local Xorg when the
     assigned number reaches 6000.</emphasis>
    </para>
    <para>
     In xrdp version 0.6.1, a new X11DisplayOffset configuration is
     introduced to <literal>xrdp/sesman.ini</literal>. It allows assigning
     ports in a customizable range starting from 5900+X11DisplayOffset for
     X.org and avoiding potential conflicts, as a result increasing the
     maximum number of concurrent remote X.org sessions connected the
     server.
    </para>
    <para>
     Note that this feature only removes the limit from the
     <literal>xrdp</literal> side. The maximum number of concurrent remote
     X.org sessions is still limited by hardware capabilities.
    </para>
   </section>
   <section role="notoc" xml:id="fate-315995" remap="InfraPackArch:ArchIndependent:Network">


    <title>Better Information About Physical Port IDs Used by Network Interfaces with NPAR/SR-IOV Capabilities</title>
    <para>
     <emphasis>Previously, YaST offered no way to know whether two
     interfaces with NPAR/SR-IOV capabilities were sharing the same physical
     port. As a result, users could bond them without realizing that they
     were not getting the desired effect in terms of redundancy.</emphasis>
    </para>
    <para>
     Information about the physical port ID has been added to
     <emphasis>Interface Overview</emphasis> and also for each entry of the
     <emphasis>Bond Slaves</emphasis> table, so you can now inspect the
     physical port ID when selecting an interface.
    </para>
    <para>
     Additionally, you will be alerted when trying to bond devices sharing
     the same physical port.
    </para>
   </section>


   <section role="notoc" xml:id="jsc-SLE-11183">
    <title>New GeoIP Database Sources</title>
    <para>
     The GeoIP databases allow approximately geo-locating users by their IP
     address.
     In the past, the company MaxMind made such data available for free in
     its GeoLite Legacy databases.
     On January 2, 2019, MaxMind discontinued the GeoLite Legacy databases,
     now offering only the newer GeoLite2 databases for download.
     To comply with new data protection regulation, since December 30, 2019,
     GeoLite2 database users are required to comply with an additional usage
     license.
     This change means users now need to register for a MaxMind account and
     obtain a license key to download GeoLite2 databases.
     For more information about these changes, see the
     <link xlink:href="https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases/">MaxMind blog</link>.
    </para>
    <para>
     SLES includes the <package>GeoIP</package> package of tools that are
     only compatible with GeoLite Legacy databases.
     As an update for SLES 12 SP2, we introduce the following new
     packages to deal with the changes to the GeoLite service:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <command>geoipupdate</command>:
       The official Maxmind tool for downloading GeoLite2 databases.
       To use this tool, set up the configuration file with your MaxMind
       account details.
       This configuration file can also be generated on the Maxmind web page.
       For more information, see
       <link xlink:href="https://dev.maxmind.com/geoip/geoip2/geolite2/"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>geolite2legacy</command>:
       A script for converting GeoLite2 CSV data to the GeoLite Legacy format.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>geoipupdate-legacy</command>:
       A convenience script that downloads GeoLite2 data, converts it to the
       GeoLite Legacy format, and stores it in <filename>/var/lib/GeoIP</filename>.
       With this script, applications developed for use with the legacy
       <command>geoip-fetch</command> tool will continue to work.
      </para>
     </listitem>
    </itemizedlist>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-SystemsManagement" remap="InfraPackArch:ArchIndependent:SystemsManagement">
   <title>Systems Management</title>
   <para/>
   <section role="notoc" xml:id="jsc-SLE-12830">
    <title>Salt Has Been Updated to Version 3000</title>
    <para>
     Salt has been upgraded to upstream version 3000, plus a number of
     patches, backports and enhancements by SUSE. In particular,
     CVE-2020-11651 and CVE-2020-11652 fixes are included in our release.
    </para>
    <para>
     As part of this upgrade, cryptography is now managed by the
     Python-M2Crypto library (which is itself based on the well-known OpenSSL
     library).
    </para>
    <para>
     We intend to regularly upgrade Salt to more recent versions.
    </para>
    <para>
     Salt 3000 is the last version of Salt which will support the old syntax
     of the <literal>cmd.run</literal> module.
    </para>
   </section>

   <section role="notoc" xml:id="fate-323175" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>The YaST Module for SSH Server Configuration Has Been Removed</title>
    <para>
     <emphasis>The YaST module for configuring an SSH server which was
     present in SLE 11, is not a part of SLE 12. It does not have any direct
     successor.</emphasis>
    </para>
    <para>
     The module <emphasis>SSH Server</emphasis> only supported configuring a
     small subset of all SSH server capabilities. Therefore, the
     functionality of the module can be replaced by using a combination of 2
     YaST modules: The <emphasis>/etc/sysconfig Editor</emphasis> and the
     <emphasis>Services Manager</emphasis>. This also applies to system
     configuration via AutoYaST.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321157" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>SASL Integration in sudo</title>
    <para>
     <emphasis> When SUSE Linux Enterprise 12 was first released, the
     </emphasis><literal>sudo</literal><emphasis> binary did not correctly
     support SASL authentication for LDAP because the package was built
     without a build dependency on the package
     </emphasis><literal>cyrus-sasl-devel</literal><emphasis>. </emphasis>
    </para>
    <para>
     To be able to use <literal>sudo</literal> with SASL, update to the
     latest version of the package <literal>sudo</literal>. For information
     about enabling SASL authentication for <literal>sudo</literal>, see
     <literal>man 5 sudoers.ldap</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320973" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>systemd: Support for System V and LSB Init Scripts Has Been Moved Out of Core Daemon</title>
    <para>
     <emphasis> To ease future maintenance, in SLE 12 SP2, systemd was
     updated to version 228. This version does not support using System V
     and LSB init scripts from the
     </emphasis><literal>systemd</literal><emphasis> daemon itself any more.
     </emphasis>
    </para>
    <para>
     This functionality is now implemented as a generator that creates
     systemd unit files from System V/LSB init scripts. These unit files are
     generated at boot or when systemd is reloaded. Therefore, to have
     changed System V init scripts recognized by systemd, run
     <literal>systemctl daemon-reload</literal> or reboot the machine.
    </para>
    <para>
     For more information, see the man page of
     <literal>systemd-sysv-generator</literal> (<literal>man
     systemd-sysv-generator</literal>).
    </para>
    <para>
     If you are packaging software that ships System V init scripts, use the
     RPM macros documented at
     <link xlink:href="https://en.opensuse.org/openSUSE:Systemd_packaging_guidelines">https://en.opensuse.org/openSUSE:Systemd_packaging_guidelines</link>
     (Section "Register Services in Install Scripts").
    </para>
   </section>
   <section role="notoc" xml:id="fate-320958" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>AutoYaST: Applying the First-Stage Network Configuration to the Installed System</title>
    <para>
     <emphasis> Due to a problem in the AutoYaST version shipped with SLE 12
     SP1, the network configuration used during the first stage was always
     copied to the installed system. This happened regardless of the value
     of </emphasis><literal>keep_install_network</literal><emphasis> in the
     AutoYaST profile. </emphasis>
    </para>
    <para>
     SLE 12 SP2 behaves as expected and
     <literal>keep_install_network</literal> will be set to
     <literal>true</literal> by default.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320616" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>New YaST VPN module</title>
    <para>
     The new YaST VPN module provides an intuitive and easy to use interface
     for setting up VPN gateways and clients. It simplifies the setup of
     typical IPSec VPN gateways and clients.
    </para>
    <para>
     IPSec is an open and standardized VPN protocol, natively supported by
     most operating systems and devices, including Linux, Unix, Windows,
     Android, Blackberry, Apple iOS and MacOS, without the need for
     third-party software solution.
    </para>
    <para>
     Using the YaST VPN module, you can create VPN gateways for the
     following scenarios:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Provide network access to Linux clients authenticated via a
       pre-shared key or certificate.
      </para>
     </listitem>
     <listitem>
      <para>
       Provide network access to Windows 7, 8, 10, and Blackberry clients
       authenticated via a combination of certificate and username/password.
      </para>
     </listitem>
     <listitem>
      <para>
       Provide network access to Android, iOS, and MacOS clients
       authenticated via a combination of a pre-shared key and
       username/password.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Additionally, you can set up connections to remote VPN gateways, for
     the following scenarios:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Prove client identity with a pre-shared key.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-320407" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Enrolling in a Microsoft Active Directory Domain via YaST</title>
    <para>
     <emphasis>You can configure a SLES computer to become a member in
     Microsoft Active Directory to leverage its user account and group
     management. In previous versions of SLES, enrolling a computer in a
     Microsoft Active Directory was a lengthy and error-prone
     procedure.</emphasis>
    </para>
    <para>
     In SLES 12 SP2, YaST ships with the new configuration tool
     <emphasis>User Logon Management</emphasis> (previously
     <emphasis>Authentication Client</emphasis>) which offers a powerful
     yet simple user interface for joining an Active Directory domain and
     allows authenticating users using those domain accounts. In addition to
     Active Directory, the editor can also set up authentication against a
     generic Kerberos or LDAP service.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320392" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>ntp 4.2.8</title>
    <para>
     <emphasis>ntp was updated to version 4.2.8.</emphasis>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis> The ntp server ntpd does not synchronize with its peers
       anymore and the peers are specified by their host name in
       </emphasis><literal>/etc/ntp.conf</literal><emphasis>. </emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis> The output of </emphasis><literal>ntpq
       --peers</literal><emphasis> lists IP numbers of the remote servers
       instead of their host names. </emphasis>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     <emphasis>Name resolution for the affected hosts works
     otherwise.</emphasis>
    </para>
    <bridgehead renderas="sect5">Parameter changes</bridgehead>
    <para>
     The meaning of some parameters for the sntp command-line tool have
     changed or have been dropped, for example <literal>sntp -s</literal> is
     now <literal>sntp -S</literal>. Please review any
     <literal>sntp</literal> usage in your own scripts for required changes.
    </para>
    <para>
     After having been deprecated for several years, ntpdc is now disabled
     by default for security reasons. It can be re-enabled by adding the
     line <literal>enable mode7</literal> to
     <literal>/etc/ntp.conf</literal>, but preferably
     <literal>ntpq</literal> should be used instead.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320268" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Installing kGraft Patches with Weak Package Dependency Resolution Disabled</title>
    <para>
     <emphasis> In environments with a clearly defined list of packages to
     be installed on the system and weak package dependency resolution
     disabled via
     </emphasis><literal>solver.onlyRequires=true</literal><emphasis> in
     </emphasis><literal>/etc/zypp/zypp.conf</literal><emphasis>, automatic
     installation of the initial kGraft patch is broken. </emphasis>
    </para>
    <para>
     As an aid in this situation, the package
     <literal>kernel-$FLAVOR-kgraft</literal> is provided. Installing this
     package pulls the associated kGraft patch into the system.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318850" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Sudo Now Respects Groups Added by the pam_group Module</title>
    <para>
     Sudo now respects groups added by the <literal>pam_group</literal>
     module and adds these groups to the target user.
    </para>
    <para>
     If there is a user <literal>tux</literal>, you can now use the
     following to add it to the group <literal>games</literal>:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Open <literal>/etc/security/group.conf</literal> and add:
       <literal>sudo;*;tux;Al0000-2400;games</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Open <literal>/etc/pam.d/sudo</literal> and add the following line at
       the beginning of the file: <literal>auth required
       pam_group.so</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Then run: <literal>sudo -iu tux id</literal>
      </para>
     </listitem>
    </orderedlist>
    <para>
     In SLE 12 SP1 and before, the user <literal>tux</literal> would not
     have been added to the group <literal>games</literal>:
    </para>
<screen>uid=1002(tux) gid=100(users) groups=100(users)</screen>
    <para>
     In SLE 12 SP2, the user <literal>tux</literal> is added to the group
     <literal>games</literal>:
    </para>
<screen>uid=1002(tux) gid=100(users) groups=100(users),40(games)</screen>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Performance" remap="InfraPackArch:ArchIndependent:Performance">
   <title>Performance Related Information</title>
   <para/>

   <section role="notoc" xml:id="fate-319935" remap="InfraPackArch:ArchIndependent:Performance">


    <title>perf Provides Guest Exit Statistics</title>
    <para>
     This feature enables <literal>perf</literal> to collect guest exit
     statistics based on the <literal>kvm_exits</literal> made by the
     threads of a guest-to-host context. The statistics report is grouped by
     exit reason. This can used as an indicator of the performance of a VM
     under a certain workload.
    </para>
    <para>
     Besides <literal>kvm_exits</literal>, hypervisor calls are also
     reported and grouped by <literal>hcall</literal> reason. The statistics
     can be shown for an individual guest or all guests running on a system.
    </para>
   </section>
   <section role="notoc" xml:id="fate-315818" remap="InfraPackArch:ArchIndependent:Performance">


    <title>Deferred and Parallelized Initialization of Page Structures in Memory Management</title>
    <para>
     <emphasis>Page initialization takes a very long time on large-memory
     systems. This is one of the reasons why large machines take a long time
     to boot.</emphasis>
    </para>
    <para>
     The kernel now provides deferred initialization of page structures on
     the x86_64 architecture. Only approximately 2 GB per memory node are
     initialized during boot, the rest is initialized in parallel with the
     boot process by kernel threads named <literal>pgdatinitX</literal>,
     where X indicates the node ID.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Storage" remap="InfraPackArch:ArchIndependent:Storage">
   <title>Storage</title>
   <para/>

   <section role="notoc" xml:id="fate-324448" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Compatibility of Newly Created XFS File Systems With SLE 11</title>
    <para>
     <emphasis>XFS file systems created with the default settings of SLES 12
     SP2 and later cannot be used SLE 11 installations.</emphasis>
    </para>
    <para>
     In SLE 12 SP2 and later, by default, XFS file systems are created with
     the option <literal>ftype=1</literal> that changes the superblock
     format. Among other things, this helps accommodate Docker. However,
     this option is incompatible with SLE 11.
    </para>
    <para>
     To create a SLE 11-compatible XFS file system, use the parameter
     <literal>ftype=0</literal>. For example, to format an empty device,
     run: :
    </para>
<screen>mkfs.xfs -m crc=0 -n ftype=0 [DEVICE]</screen>
   </section>
   <section role="notoc" xml:id="fate-322261" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Unloading device_handler Modules Not Possible Anymore</title>
    <para>
     <emphasis> With SLES 12 SP2,
     </emphasis><literal>device_handler</literal><emphasis> modules cannot
     be unloaded anymore. This functionality has been removed upstream
     because of the dangers associated with it. </emphasis>
    </para>
    <para>
     <emphasis> If the
     </emphasis><literal>device_handler</literal><emphasis> module is
     loaded, it is not possible to switch it to another. This was possible
     in earlier versions of SLES 12. </emphasis>
    </para>
    <para>
     There is no workaround that allows unloading
     <literal>device_handler</literal> modules. However, the SLES 12 SP2
     kernel has much improved algorithms for checking which device handler
     needs to be loaded for a given device. This will accurately reflect the
     capabilities of the device.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320870" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Root File System Conversion to Btrfs Not Supported</title>
    <para>
     <emphasis>If it is not the root file system and if the file system has
     at least 20 % free space available, in-place conversion of an existing
     Ext2/Ext3/Ext4 or ReiserFS file system is supported for data mount
     points.</emphasis>
    </para>
    <para>
     SUSE does not recommend or support in-place conversion of OS root file
     systems. In-place conversion to Btrfs of root file systems requires
     manual subvolume configuration and additional configuration changes
     that are not automatically applied for all use cases.
    </para>
    <para>
     To ensure data integrity and the highest level of customer
     satisfaction, when upgrading, maintain existing root file systems.
     Alternatively, reinstall the entire operating system.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320834" remap="InfraPackArch:ArchIndependent:Storage">


    <title>/var/cache on an Own Subvolume for Snapshots and Rollback</title>
    <para>
     <literal>/var/cache</literal><emphasis> contains very volatile data,
     like the Zypper cache with RPM packages in different versions for each
     update. As a result of storing data that is mostly redundant but highly
     volatile, the amount of disk space a snapshot occupies can increase
     very fast. </emphasis>
    </para>
    <para>
     To solve this, move <literal>/var/cache</literal> to a separate
     subvolume. On fresh installations of SLE 12 SP2 or newer, this is done
     automatically. To convert an existing root file system, perform the
     following steps:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Find out the device name (<literal>/dev/sda2</literal>,
       <literal>/dev/sda3</literal> etc.) of the root file system:
       <literal>df /</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Identify the parent subvolume of all the other subvolumes. For SLE 12
       installations, this is a subvolume named <literal>@</literal>. To
       check if you have a <literal>@</literal> subvolume, use:
       <literal>btrfs subvolume list / | grep '@'</literal>. If the output
       of this command is empty, you do not have a subvolume named
       <literal>@</literal>. In that case, you may be able to proceed with
       subvolume ID 5 which was used in older versions of SLE.
      </para>
     </listitem>
     <listitem>
      <para>
       Now mount the requisite subvolume.
      </para>
      <itemizedlist>
        <listitem>
         <para>
          If you have a <literal>@</literal> subvolume, mount that subvolume
          to a temporary mount point: <literal>mount &lt;root_device&gt; -o
          subvol=@ /mnt</literal>
         </para>
        </listitem>
        <listitem>
         <para>
          If you don't have a <literal>@</literal> subvolume, mount
          subvolume ID 5 instead: <literal>mount &lt;root_device&gt; -o
          subvolid=5 /mnt</literal>
         </para>
        </listitem>
       </itemizedlist>
     </listitem>
     <listitem>
      <para>
       <literal>/mnt/var/cache</literal> can already exist and could be the
       same directory as <literal>/var/cache</literal>. To avoid data loss,
       move it: <literal>mv /mnt/var/cache /mnt/var/cache.old</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       In either case, create a new subvolume: <literal>btrfs subvol create
       /mnt/var/cache</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       If there is now a directory <literal>/var/cache.old</literal>, move
       it to the new location: <literal>mv /var/cache.old/*
       /mnt/var/cache</literal>. If that is not the case, instead do:
       <literal>mv /var/cache/* /mnt/var/cache/</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Optionally, remove <literal>/mnt/var/cache.old</literal>:
       <literal>rm -rf /mnt/var/cache.old</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Unmount the subvolume from the temporary mount point: <literal>umount
       /mnt</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Add an entry to <literal>/etc/fstab</literal> for the new
       <literal>/var/cache</literal> subvolume. Use an existing subvolume as
       a template to copy from. Make sure to leave the UUID untouched (this
       is the root file system's UUID) and change the subvolume name and its
       mount point consistently to <literal>/var/cache</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Mount the new subvolume as specified in /etc/fstab: <literal>mount
       /var/cache</literal>
      </para>
     </listitem>
    </orderedlist>
   </section>
   <section role="notoc" xml:id="fate-319560" remap="InfraPackArch:ArchIndependent:Storage">


    <title>nvme-cli: A User-Space Tool to Manage NVMe Devices on Linux</title>
    <para>
     The tool <literal>nvme-cli</literal> provides management features to
     NVMe devices, such as adapter information retrieval, namespace
     creation/formatting and adapter firmware update.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319267" remap="InfraPackArch:ArchIndependent:Storage">


    <title>systemd: The NFS Mount Option bg Is Deprecated</title>
    <para>
     <emphasis> The upstream developers of systemd do not support the NFS
     mount option </emphasis><literal>bg</literal><emphasis> any more. While
     this mount option is still supported in SLE 12 SP2, it will be removed
     in the next version of SLE. </emphasis>
    </para>
    <para>
     It will be replaced by the systemd mount option
     <literal>nofail</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-312751" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Snapper: Cleanup Rules Based on Fill Level</title>
    <para>
     <emphasis>Some programs do not respect the special disk space
     characteristics of a Btrfs file system containing snapshots. This can
     result in unexpected situations where no free space is left on a Btrfs
     filesystem.</emphasis>
    </para>
    <para>
     Snapper can watch the disk space of snapshots that have automatic
     cleanup enabled and can try to keep the amount of disk space used below
     a threshold.
    </para>
    <para>
     If snapshots are enabled, the feature is enabled for the root file
     system by default on new installations.
    </para>
    <para>
     For existing installations, the system administrator must enable quota
     and set limits for the cleanup algorithm to use this new feature. This
     can be done using the following commands:
    </para>
    <orderedlist>
     <listitem>
      <para>
       <literal>snapper setup-quota</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>snapper set-config NUMBER_LIMIT=2-10
       NUMBER_LIMIT_IMPORTANT=4-10</literal>
      </para>
     </listitem>
    </orderedlist>
    <para>
     For more information, see the man pages of <literal>snapper</literal>
     and <literal>snapper-configs</literal>.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Virtualization" remap="InfraPackArch:ArchIndependent:Virtualization">
   <title>Virtualization</title>
   <para/>

   <section role="notoc" xml:id="fate-320535" remap="InfraPackArch:ArchIndependent:Virtualization">


    <title>Virtual Machine Driver Pack 2.4 (VMDP 2.4)</title>
    <para>
     <emphasis>SUSE Linux Enterprise Virtual Machine Driver Pack is a set of
     paravirtualized device drivers for Microsoft Windows operating systems.
     These drivers improve the performance of unmodified Windows guest
     operating systems that are run in virtual environments created using
     Xen or KVM hypervisors with SUSE Linux Enterprise Server 11 SP4 and
     SUSE Linux Enterprise Server 12 SP2. Paravirtualized device drivers are
     installed in virtual machine instances of operating systems and
     represent hardware and functionality similar to the underlying physical
     hardware used by the system virtualization software layer.</emphasis>
    </para>
    <para>
     The new features of SUSE Linux Enterprise Virtual Machine Driver Pack
     2.4 include:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Support for SUSE Linux Enterprise Server 12 SP2
      </para>
     </listitem>
     <listitem>
      <para>
       Drivers for Windows Server 2016
      </para>
     </listitem>
     <listitem>
      <para>
       Drivers are no longer dependent on <literal>pvvxbn.sys</literal>
       being loaded
      </para>
     </listitem>
     <listitem>
      <para>
       Support Windows Multipoint Server
      </para>
     </listitem>
    </itemizedlist>
    <para>
     New driver and utility features:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>pvvxbn.sys</literal>: Issues a Xen shutdown/reboot at the
       end of the power down sequence unless the PV control flag
       <literal>dfs</literal> ("disable forced shutdown") is enabled.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pvvxblk.sys</literal>: VirtIO: MSI vectors can now be used.
       Xen: support for indirect descriptors. Queuing, queue depth, and
       <literal>max_segs</literal> are tunable.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pvvxscsi.sys</literal>: VirtIO: MSI vectors can now be
       used.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>setup.exe</literal>: Has enhanced support for
       <literal>virt-v2v</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>pvctrl.exe</literal>: Can now modify NIC parameters.
       Enable/disable Xen <literal>pvvxblk</literal> queuing/queue depth (<literal>qdepth</literal>). Set Xen <literal>pvvxblk</literal>
       maximum number of segments (<literal>max_segs</literal>). Set debug
       print mask (dpm). Enable/disable Xen force shutdown after power-down
       sequence (dfs). Enable/disable <literal>virtio_serial</literal> MSI
       usage (<literal>vserial_msi</literal>).
      </para>
     </listitem>
    </itemizedlist>
   </section>

   <section xml:id="InfraPackArch-ArchIndependent-Virtualization-KVM" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">
    <title>KVM</title>
    <para/>

    <section role="notoc" xml:id="fate-320350" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>KVM Legacy Device Assignment Was Disabled</title>
     <para>
      <emphasis>The legacy device assignment feature of KVM was
      disabled.</emphasis>
     </para>
     <para>
      As a replacement, use VFIO. VFIO provides the same functionality and
      has the following advantages:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        It is actively maintained upstream while the legacy code is not.
       </para>
      </listitem>
      <listitem>
       <para>
        It is more secure.
       </para>
      </listitem>
      <listitem>
       <para>
        It supports new hardware features such as interrupt virtualization.
       </para>
      </listitem>
     </itemizedlist>
    </section>
    <section role="notoc" xml:id="fate-319621" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>virt-install: Parameter --sysinfo Allows Configuring sysinfo/SMBIOS Values</title>
     <para>
      <emphasis> libvirt and QEMU allow control over what SMBIOS information
      is presented to the guest. You can use tools such as
      </emphasis><literal>dmidecode</literal><emphasis> in the guest to
      inspect this information. However, previously, this control was not
      exposed in </emphasis><literal>virt-install</literal><emphasis>.
      </emphasis>
     </para>
     <para>
      In SLES 12 SP2, you can use <literal>virt-install</literal> to
      configure sysinfo/SMBIOS values exposted to guests using the parameter
      <literal>--sysinfo OPT=VAL,[...]</literal>. <literal>--sysinfo
      host</literal> can be used to expose the host's SMBIOS info to the VM,
      otherwise, values can be manually specified. To see a list of all
      available subparameters, use <literal>--sysinfo=?</literal>.
     </para>
     <para>
      For more information, see the libvirt documentation at
      <link xlink:href="https://libvirt.org/formatdomain.html"/>.
     </para>
    </section>
    <section role="notoc" xml:id="fate-319531" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>Support for UEFI in QEMU Virtual Machines</title>
     <para>
      libvirt and KVM/QEMU now support UEFI for virtual machines. UEFI
      firmware is provided through the <literal>qemu-ovmf-x86_64</literal>
      package.
     </para>
    </section>
    <section role="notoc" xml:id="fate-316628" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>Obtaining Addresses with libvirt-nss</title>
     <para>
      With <literal>libvirt-nss</literal>, you can obtain addresses of
      <literal>dnsmasq</literal> -backed KVM guests. For more information,
      see the <emphasis>Virtualization Guide, Chapter "Obtaining IP
      Addresses with nsswitch for NAT Networks"</emphasis>.
     </para>
    </section>
    <section role="notoc" xml:id="fate-316228" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>Post-Copy Live Migration Support in libvirt and QEMU/KVM</title>
     <para>
      <emphasis>Pre-copy live migration can take a lot of time depending on
      the workload and page dirtying rate of the virtual machine.</emphasis>
     </para>
     <para>
      libvirt and QEMU/KVM now support post-copy live migration. This means
      that the virtual machine starts running on the destination host as
      soon as possible and the RAM from the source host is pagefaulted into
      the destination over time. This ensures minimal downtime for the
      virtual machine.
     </para>
     <para>
      The guest will run on target host immediately, only CPU state and
      device state are transferred to target host. If the network is down
      before all missing memory pages are copied from the original guest,
      the new guest will crash.
     </para>
    </section>

   </section>
   <section xml:id="InfraPackArch-ArchIndependent-Virtualization-XEN" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">
    <title>Xen</title>
    <para/>

    <section role="notoc" xml:id="fate-320638" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">


     <title>qemu-xen Has Been Dropped From the Xen Package</title>
     <para>
      <emphasis> QEMU is a large software project that sees many bug and
      security fixes. Providing several different
      </emphasis><literal>qemu</literal><emphasis> binaries is challenging
      for maintenance, requiring bug and security fixes to be backported to
      all the different </emphasis><literal>qemu</literal><emphasis>
      sources. </emphasis>
     </para>
     <para>
      The Xen package now uses <literal>qemu-system-x86_64</literal> from
      the <literal>qemu</literal> package instead of providing its own
      <literal>qemu</literal> binary.
     </para>
    </section>
    <section role="notoc" xml:id="fate-320490" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">


     <title>Support UEFI in Xen HVM Virtual Machines</title>
     <para>
      libvirt and Xen now support UEFI for virtual machines. UEFI firmware
      is provided through the <literal>qemu-ovmf-x86_64</literal> package.
     </para>
    </section>
    <section role="notoc" xml:id="fate-320402" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">


     <title>GRUB Does Not Support vfb/vkbd Any More</title>
     <para>
      <emphasis>The version of GRUB shipped with SLES 12 SP1 and SP2 does
      not support vfb/vkbd any more. This means that in Xen paravirtualized
      machines, there is no graphical display available while GRUB is
      active.</emphasis>
     </para>
     <para>
      To be able to see and interact with GRUB, switch to the text-based
      xencon protocol: Modify the kernel parameter of the PV guest, add
      <literal>console=hvc0 xencons=tty</literal>, and connect with the
      command <literal>console DOMAINNAME</literal> of the
      <literal>libvirt</literal> toolstack.
     </para>
    </section>
    <section role="notoc" xml:id="fate-319810" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">


     <title>libvirt XML Now Supports the External Block Scripts of Xen</title>
     <para>
      <emphasis> The external block scripts of Xen, such as
      </emphasis><literal>block-drbd</literal><emphasis>,
      </emphasis><literal>block-dmmd</literal><emphasis> could formerly only
      be used with </emphasis><literal>xl</literal><emphasis> /
      </emphasis><literal>libxl</literal><emphasis> using the disk
      configuration syntax </emphasis><literal>script=</literal><emphasis>.
      libvirt did not support such external scripts and thus could not be
      used with disks configured with the block scripts. </emphasis>
     </para>
     <para>
      External block scripts of Xen can now be used with libvirt by
      specifying base name of the block script in the
      <literal>&lt;source&gt;</literal> element of the disk. For example:
     </para>
<screen>&lt;source dev='dmmd:md;/dev/md0;lvm;/dev/vgxen/lv-vm01'/&gt;</screen>
    </section>
    <section role="notoc" xml:id="fate-316612" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">


     <title>Support for the PVUSB Driver in Xen and the libvirt Xen Driver</title>
     <para>
      <literal>libxl</literal> now has a PVUSB API which supports passing a
      USB device from the host to the guest domain via PVUSB. This
      functionality is also supported by the command line tool
      <literal>xl</literal>.
     </para>
     <para>
      PVUSB support was also added to the libvirt <literal>libxl</literal>
      driver to use PVUSB functionality from the libvirt toolstack.
     </para>
    </section>
    <section role="notoc" xml:id="fate-315712" remap="InfraPackArch:ArchIndependent:Virtualization:XEN">


     <title>Xen: PV-OPS Kernel Supersedes kernel-xen</title>
     <para>
      The Xen hypervisor functions have been ported over to the standard
      PV-OPS mechanism and are now included in the default kernel. As
      everything necessary is now provided by the default kernel, the
      <literal>kernel-xen</literal> package was removed.
     </para>
    </section>

   </section>
   <section xml:id="InfraPackArch-ArchIndependent-Virtualization-Other" remap="InfraPackArch:ArchIndependent:Virtualization:Other">
    <title>Others</title>
    <para/>

    <section role="notoc" xml:id="fate-320353" remap="InfraPackArch:ArchIndependent:Virtualization:Other">


     <title>virt-convert: Support for Compressed Files in Within an OVA</title>
     <para>
      <emphasis> According to the OVF 1.1.0 specification, OVA files can
      contain files compressed using
      </emphasis><literal>gzip</literal><emphasis>, for example,
      </emphasis><literal>vmdk</literal><emphasis> files. This case was
      previously not handled correctly. </emphasis>
     </para>
     <para>
      In SLE 12 SP2, <literal>virt-convert</literal> will now correctly
      decompress <literal>gz</literal> files first and then convert them
      using <literal>qemu-img</literal>.
     </para>
    </section>
    <section role="notoc" xml:id="fate-320080" remap="InfraPackArch:ArchIndependent:Virtualization:Other">


     <title>libiscsi Integration with QEMU</title>
     <para>
      QEMU now integrates with <literal>libiscsi</literal>. This allows
      QEMU to access iSCSI resources directly and use them as virtual
      machine block devices. iSCSI-based disk devices can also be specified
      in the libvirt XML configuration. This feature is only available using
      the RAW image format, as the iSCSI protocol has some technical
      limitations.
     </para>
    </section>
    <section role="notoc" xml:id="fate-319685" remap="InfraPackArch:ArchIndependent:Virtualization:Other">


     <title>DPDK Support for vhost-user Live Migration</title>
     <para>
      <emphasis> Currently, the common back-end implementation to
      </emphasis><literal>vhost-user</literal><emphasis> is
      </emphasis><literal>dpdk</literal><emphasis>. To support
      </emphasis><literal>vhost-user</literal><emphasis> live migration, a
      feature bit called
      </emphasis><literal>VHOST_USER_PROTOCOL_F_LOG_SHMFD</literal><emphasis>
      is required on both the QEMU side and the
      </emphasis><literal>vhost-user</literal><emphasis> back-end side.
      </emphasis>
     </para>
     <para>
      <emphasis>On the QEMU side, upstream version 2.6 already provides the
      required functionality. But on the DPDK side, the upstream release of
      DPDK 2.2.0 does not provide it.</emphasis>
     </para>
     <para>
      The version of DPDK 2.2.0 shipped with SLE 12 SP2 is patched to
      provide the ability of <literal>vhost-user</literal> live migration.
     </para>
    </section>
    <section role="notoc" xml:id="fate-319145" remap="InfraPackArch:ArchIndependent:Virtualization:Other">


     <title>wbemcli Now Allows Configuring the SSL/TLS version</title>
     <para>
      <emphasis> Previously, it could be impossible to monitor certain
      servers that used very specific versions of the SSL/TLS protocols
      using </emphasis><literal>wbemcli</literal><emphasis>. </emphasis>
     </para>
     <para>
      <literal>wbemcli</literal> can now be configured to use a specific
      SSL/TLS protocol version. To do so, use the environment variable
      <literal>WBEMCLI_CURL_SSLVERSION</literal>. Possible values are:
      <literal>SSLv2</literal>, <literal>SSLv3</literal>,
      <literal>TLSv1</literal>, <literal>TLSv1_0</literal> (TLSv1.0),
      <literal>TLSv1_1</literal> (TLSv1.1), <literal>TLSv1_2</literal>
      (TLSv1.2).
     </para>
    </section>
    <section role="notoc" xml:id="fate-318990" remap="InfraPackArch:ArchIndependent:Virtualization:Other">


     <title>Support for 3D Graphics in VMware Guest</title>
     <para>
      The <literal>vmwgfx</literal> driver supports 3D with VMware hardware
      version 11.
     </para>
    </section>

   </section>
  </section>
 </section>
 <section xml:id="InfraPackArch-x86-64" remap="InfraPackArch:x86_64">
  <title>AMD64/Intel 64 (x86_64) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP2 for the AMD64/Intel 64 architectures.
  </para>

  <section role="notoc" xml:id="fate-323248" remap="InfraPackArch:x86_64">


   <title>Support for intel_idle and Hardware P States on Intel Skylake Processors Can Lead to Decreased Performance</title>
   <para>
    <emphasis>On Intel processors from the generation code-named Skylake,
    some workloads can run slower on SLE 12 SP2 than they run on SLE 12
    SP1.</emphasis>
   </para>
   <para>
    SLE 12 SP1 and before, when running on Intel processors from the
    generation code-named Skylake, did not leverage hardware P states (HWP)
    or the <literal>intel_idle</literal> driver to save power. Instead,
    these processors ran at the maximum CPU frequency even when idle.
   </para>
   <para>
    With SLE 12 SP2, hardware P states and the intel_idle driver are now
    supported on Skylake processors. This means that because the processor
    will not run at full speed at all times, some workloads can perform
    worse on SLE 12 SP2 than they do on SLE 12 SP1.
   </para>
  </section>
  <section role="notoc" xml:id="fate-318845" remap="InfraPackArch:x86_64">


   <title>Kernel NOHZ_FULL Process Scheduler Mode</title>
   <para>
    <emphasis>Under normal operation, the kernel interrupts process
    execution several hundred times per second for statistics collection and
    kernel internal maintenance tasks. Despite the interruptions being
    brief, they add up. This adds an unpredictable amount of time to process
    run time. Highly timing sensitive applications may be disturbed by this
    activity.</emphasis>
   </para>
   <para>
    The SLE kernel now ships with adaptive tick mode (<literal>NOHZ_FULL</literal>) enabled by default to reduce the number
    of kernel interrupts. With this option enabled and conditions for
    adaptive tick mode fulfilled, the number of interrupts goes down to ones
    per second.
   </para>
  </section>

  <section xml:id="InfraPackArch-x86-64-System" remap="InfraPackArch:x86_64:System">
   <title>System and Vendor Specific Information</title>
   <para/>

   <section role="notoc" xml:id="fate-317885" remap="InfraPackArch:x86_64:System">


    <title>Support for Run-Time Allocation of Huge Pages With 1 GB Size</title>
    <para>
     <emphasis>In previous versions of SLE, huge pages with a size of 1 GB
     could only be allocated via a kernel parameter at boot. This has the
     following drawbacks:</emphasis>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>You cannot specify the NUMA node for allocation.</emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>You cannot free these pages later without a
       reboot.</emphasis>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     On the x86-64 architecture, SLE can now allocate and free 1 GB huge
     pages at system run time, using the same methods that are also used for
     regular huge pages.
    </para>
    <para>
     However, you should still allocate 1 GB huge pages as early as possible
     during the run time. Otherwise, physical memory can become fragmented
     by other uses and the risk of allocation failure grows.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="InfraPackArch-Power" remap="InfraPackArch:Power">
  <title>POWER (ppc64le) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP2 for the POWER architecture.
  </para>

  <section role="notoc" xml:id="fate-321098" remap="InfraPackArch:Power,InfraPackArch:SystemZ:Storage">


   <title>Ceph Client Support on IBM Z and POWER</title>
   <para>
    On SLES 12 SP2 and SLES 12 SP3, IBM Z and POWER machines can now
    function as SUSE Enterprise Storage (Ceph) clients.
   </para>
   <para>
    This support is possible because the kernels for IBM Z and POWER now
    have the relevant modules for CephFS and RBD enabled. The Ceph client
    RPMs for IBM Z and POWER are included in SLE 12 SP3. Additionally, the
    QEMU packages for IBM Z and POWER are now built against librbd.
   </para>
  </section>
  <section role="notoc" xml:id="fate-320467" remap="InfraPackArch:Power">


   <title>Cluster Support and High Availability for POWER</title>
   <para>
    Packages to facilitate cluster setup and to enable HA have been added to
    SUSE Linux High Availability Extension for POWER (LE).
   </para>
  </section>
  <section role="notoc" xml:id="fate-320440" remap="InfraPackArch:Power">


   <title>The libcxl Userspace Library for CAPI Has Been Added</title>
   <para>
    SLES now ships with the package <literal>libcxl</literal>. It provides
    the library of the same name that can be used for userspace CAPI.
   </para>
   <para>
    The SLE SDK contains the corresponding development package,
    <literal>libcxl-devel</literal>.
   </para>
  </section>
  <section role="notoc" xml:id="fate-318471" remap="InfraPackArch:Power">


   <title>Enhanced Support for System Call Filtering on POWER</title>
   <para>
    Mode 2 of <literal>seccomp</literal> is now supported on POWER, allowing
    for fine-grained filtering of system calls. Support is available in both
    the kernel and in <literal>libseccomp</literal>.
   </para>
  </section>
  <section role="notoc" xml:id="fate-318236" remap="InfraPackArch:Power">


   <title>Hardware Transactional Memory (HTM) support in glibc for POWER</title>
   <para>
    Lock elision in the GNU C Library is available, but disabled by default.
    To enable it, set the environment variable
    <literal>GLIBC_ELISION_ENABLE</literal> to the value "yes".
   </para>
  </section>
  <section role="notoc" xml:id="fate-318001" remap="InfraPackArch:Power">


   <title>Support for CXL Flash Storage Device Driver</title>
   <para>
    The CXL flash storage device provides persistent, flash-based storage
    using CAPI technology.
   </para>
  </section>

  <section role="notoc" xml:id="bsc-1136157">
   <title>Speed of <literal>ibmveth</literal> Interface Not Reported Accurately</title>
   <para>
    The <literal>ibmveth</literal> interface is a paravirtualized interface.
    When communicating between LPARs within the same system, the interface's
    speed is limited only by the system's CPU and memory bandwidth.
    When the virtual Ethernet is bridged to a physical network, the
    interface's speed is limited by the speed of that physical network.
   </para>
   <para>
    Unfortunately, the <literal>ibmveth</literal> driver has no way of
    determining automatically whether it is bridged to a physical network
    and what the speed of that link is.
    <literal>ibmveth</literal> therefore reports its speed as a fixed value
    of 1 Gb/s which in many cases will be inaccurate.
    To determine the actual speed of the interface, use a benchmark.
   </para>
  </section>
 </section>
 <section xml:id="InfraPackArch-SystemZ">
  <title>IBM z Systems (s390x) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP2 for the IBM z Systems architecture.
  </para>
  <para>
   IBM zEnterprise 196 (z196) and IBM zEnterprise 114 (z114) further on
   referred to as z196 and z114.
  </para>
  <section xml:id="InfraPackArch-SystemZ-Hardware" remap="InfraPackArch:SystemZ:Hardware">
   <title>Hardware</title>
   <para/>

   <section role="notoc" xml:id="fate-319948" remap="InfraPackArch:SystemZ:Hardware">


    <title>Improved Auto LUN Scan</title>
    <para>
     Optimized tools and configuration enable improved WWPN and FCP LUN
     scanning. Manual intervention during installation or setup has been
     eliminated wherever possible.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319597" remap="InfraPackArch:SystemZ:Hardware">


    <title>Support for IPL Device in Any Subchannel Set</title>
    <para>
     IPL devices are no longer restricted to subchannel set 0. The
     limitation is removed as of IBM zEnterprise 196 GA2.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319584" remap="InfraPackArch:SystemZ:Hardware">


    <title>Bus Awareness for z Systems in systemd</title>
    <para>
     systemd now provides full and correct support for driver model buses
     specific to Linux on z Systems, such as <literal>ccw</literal>,
     <literal>ccwgroup</literal>, and <literal>zfcp</literal>.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Virtualization" remap="InfraPackArch:SystemZ:Virtualization">
   <title>Virtualization</title>
   <para/>

   <section role="notoc" xml:id="fate-320123" remap="InfraPackArch:SystemZ:Virtualization">


    <title>Executing Hypervisor-Specific Actions During Boot</title>
    <para>
     <emphasis>Depending on the hypervisor that a system runs on (such as
     z/VM, zKVM, or LPAR), during boot, differerent actions can be
     needed.</emphasis>
    </para>
    <para>
     The service <literal>virtsetup</literal> is preconfigured to do that.
     To activate it, execute the following command:
    </para>
<screen>systemctl enable virtsetup.service</screen>
    <para>
     To configure this service in more detail, see the file
     <literal>/etc/sysconfig/virtsetup</literal>. You can also edit the
     file through YaST:
    </para>
<screen>yast2 sysconfig</screen>
   </section>
   <section role="notoc" xml:id="fate-319606" remap="InfraPackArch:SystemZ:Virtualization">


    <title>VMUR Print Spool Options for Linux</title>
    <para>
     Linux guests are now better integrated into the z/VM print solution. It
     is now possible to specify the spool options <literal>CLASS</literal>
     and <literal>FORM</literal> together with the <literal>print</literal>
     command of the VMUR tool.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319575" remap="InfraPackArch:SystemZ:Virtualization">


    <title>zKVM: SIE Capability Exposed to User Space</title>
    <para>
     Userspace applications can now query whether the Linux instance can act
     as a hypervisor by checking for the SIE (Start Interpretive Execution)
     capability. This is useful, for example, in continuous integration (CI)
     environments.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Storage" remap="InfraPackArch:SystemZ:Storage">
   <title>Storage</title>
   <para/>

   <section role="notoc" xml:id="fate-321181" remap="InfraPackArch:SystemZ:Storage">


    <title>iSCSI Devices Not Enabled After Installation</title>
    <para>
     <emphasis>When installing SLES 12 SP2, iSCSI devices may not be enabled
     after installation.</emphasis>
    </para>
    <para>
     When configuring iSCSI volumes, make sure to set start mode to
     <literal>automatic</literal>. <literal>onboot</literal> is only valid
     for iSCSI devices which are supposed to be activated from the initrd,
     that is, when the system is booted from iSCSI. However, that is
     currently not supported on z Systems.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321098-1" remap="InfraPackArch:Power,InfraPackArch:SystemZ:Storage">


    <title>Ceph Client Support on IBM Z and POWER</title>
    <para>
     On SLES 12 SP2 and SLES 12 SP3, IBM Z and POWER machines can now
     function as SUSE Enterprise Storage (Ceph) clients.
    </para>
    <para>
     This support is possible because the kernels for IBM Z and POWER now
     have the relevant modules for CephFS and RBD enabled. The Ceph client
     RPMs for IBM Z and POWER are included in SLE 12 SP3. Additionally, the
     QEMU packages for IBM Z and POWER are now built against librbd.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319605" remap="InfraPackArch:SystemZ:Storage">


    <title>Query Host Access to Volume Support</title>
    <para>
     You can now concurrently access DASD volumes from different operating
     system instances. Applications can now query whether a DASD volume is
     online within another operating system instance by querying the storage
     server for the online status of all attached hosts. The command
     <literal>lsdasd</literal> can display this information, and the
     commands <literal>zdsfs</literal>, <literal>fdasd</literal>, and
     <literal>dasdfmt</literal> can evaluate it.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318242" remap="InfraPackArch:SystemZ:Storage">


    <title>Disk Mirroring with Real-Time Enhancement for z Systems</title>
    <para>
     This functionality is included in SLES 12 SP2 as a technology preview.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Network" remap="InfraPackArch:SystemZ:Network">
   <title>Network</title>
   <para/>

   <section role="notoc" xml:id="fate-320112" remap="InfraPackArch:SystemZ:Network">


    <title>Port Name for Open Systems Adapter (OSA) Is No Longer Needed</title>
    <para>
     <emphasis>For some time, systems sharing an OSA with the z/OS operating
     system needed to specify a port name that matched what was used on
     z/OS. Not providing a port name, or providing a non-matching port name
     would result in the network interface not activating.</emphasis>
    </para>
    <para>
     IBM has modified the microcode on all the OSAs currently available so
     that a port name is not needed. As a result, SUSE has removed the
     prompt for it from the installer. If a port name is provided via the
     installation parameter file, an informational message is displayed:
    </para>
<screen>**
** The Portname parameter is no longer needed. Please do not specify it. **
**</screen>
   </section>
   <section role="notoc" xml:id="fate-319949" remap="InfraPackArch:SystemZ:Network">


    <title>10GbE RoCE Express Feature for RDMA</title>
    <para>
     SLES 12 SP2 supports the 10GbE RoCE Express feature on zEC12, zBC12 and
     IBM z13 via the Ethernet device using TCP/IP traffic without
     restrictions. Before using this feature on an IBM z13, make sure that
     the minimum required service is applied: z/VM APAR UM34525 and HW ycode
     N98778.057 (bundle 14). Use the default MTU size (1500).
    </para>
    <para>
     SLES 12 SP2 now includes support for RDMA enablement and DAPL/OFED for
     z Systems. With the Mellanox virtualization support (SR-IOV) the
     limitation for LPAR use only on an IBM zEC12 or zBC12 is removed and
     RDMA can be used on an IBM z13.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319602" remap="InfraPackArch:SystemZ:Network">


    <title>Bridging HiperSockets to Ethernet</title>
    <para>
     A HiperSocket port can now be configured to accept Ethernet frames to
     unknown MAC addresses. This enables it to be used as a member of a
     software bridge. Control and report of the bridge port status of the
     HiperSocket port and the udev events are performed via new sysfs
     attributes.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319600" remap="InfraPackArch:SystemZ:Network">


    <title>IPv6 Priority Queuing Added to qeth Device Driver</title>
    <para>
     Priority queuing is now supported for IPv6, similarly to IPv4. This
     especially improves Linux Live Guest Migration by using IPv6 to
     minimize impact on workload traffic and enables priority queuing for
     all applications that use IPv6 QoS traffic operations.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319586" remap="InfraPackArch:SystemZ:Network">


    <title>Layer 2 Offloads Enabled</title>
    <para>
     Classic OSA operation in layer 3 mode provides numerous offload
     operations, exchanging larger amounts of data between the operating
     system and the OSA adapter. The <literal>qeth</literal> device driver
     now also provides large send/receive and checksum offload operations
     for layer 2 mode.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319578" remap="InfraPackArch:SystemZ:Network">


    <title>IPv6 Support in snIPL</title>
    <para>
     The tool for remote systems management for Linux, snIPL, now includes
     IPv6 support. This broadens the set of environments that snIPL supports
     and simplifies moving from IPv4 to IPv6.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318722" remap="InfraPackArch:SystemZ:Network">


    <title>Enhanced OSA Network to Receive All Frames Through a Network Interface</title>
    <para>
     Enhancements in the OSA device driver enable setting network interfaces
     into promiscuous mode. The mode can provide outside connectivity for
     virtual servers by receiving all frames through a network interface.
    </para>
    <para>
     In OpenStack environments, Open vSwitch is one of the connectivity
     options that use this feature.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Security" remap="InfraPackArch:SystemZ:Security">
   <title>Security</title>
   <para/>

   <section role="notoc" xml:id="fate-319610" remap="InfraPackArch:SystemZ:Security">


    <title>Support for DBRG in libica</title>
    <para>
     The libica support for the generation of pseudo-random numbers for the
     "Deterministic Random Bit Generator" (DRBG) was enhanced to comply with
     updated security specifications (NIST SP 800-90A).
    </para>
   </section>
   <section role="notoc" xml:id="fate-319598" remap="InfraPackArch:SystemZ:Security">


    <title>Monitoring CPACF Crypto Activity</title>
    <para>
     This feature enables the monitoring of CPACF crypto activity in the
     Linux image, in the kernel, and in userspace. A configurable
     crypto-activity counter allows switching monitoring of CPACF crypto
     activity on or off for selected areas to verify and monitor specific
     needs in the crypto stack.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319592" remap="InfraPackArch:SystemZ:Security">


    <title>Support for Dynamic Traces in openCryptoki</title>
    <para>
     Dynamic tracing in openCryptoki now allows starting and stopping
     tracing of all openCryptoki API calls and the related tokens while the
     application is running. This also allows using cryptography in the Java
     Security Architecture (JCA/JCE) which transparently falls back to
     software cryptography. Enhanced tracing can now identify whether
     cryptographic hardware is actually used.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319585" remap="InfraPackArch:SystemZ:Security">


    <title>CPACF MSA 4: Support for the GCM mechanism in openCryptoki</title>
    <para>
     The openCryptoki ICA includes support for a new mechanism supported by
     CPACF MSA 4. GCM is a highly recommended mechanism for use with TLS
     1.2.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319576" remap="InfraPackArch:SystemZ:Security">


    <title>Support for CCA Master Key Change for openCryptoki CCA Token</title>
    <para>
     We now provide a tool to change master keys on the CCA co-processor
     without losing the encrypted data. This helps to stay compliant with
     enhanced industry regulations and company policies.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-RAS" remap="InfraPackArch:SystemZ:RAS">
   <title>Reliability, Availability, Serviceability (RAS)</title>
   <para/>

   <section role="notoc" xml:id="fate-319588" remap="InfraPackArch:SystemZ:RAS">


    <title>CUIR: Enhanced Scope Detection</title>
    <para>
     The Linux support for CUIR (Control Unit Initiated Reconfiguration),
     which enables concurrent storage service with no or minimized down
     time, has been extended to include Linux running as a z/VM guest.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Performance" remap="InfraPackArch:SystemZ:Performance">
   <title>Performance</title>
   <para/>

   <section role="notoc" xml:id="fate-319952" remap="InfraPackArch:SystemZ:Performance">


    <title>Extended CPU Performance Metrics in HYPFS for Linux z/VM guests</title>
    <para>
     The HYPFS has been extended to provide the "diag 0C data" also for
     Linux z/VM guests that distinguish "management time" spent as part of
     CPU load.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319945" remap="InfraPackArch:SystemZ:Performance">


    <title>GCC SIMD Performance Tuning</title>
    <para>
     Enhanced instruction support in GCC improves application performance.
     Optimized applications can now also use SIMD instructions.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319591" remap="InfraPackArch:SystemZ:Performance">


    <title>IBM z13 Hardware Instructions in glibc</title>
    <para>
     Support of the IBM z13 hardware instructions in glibc provides improved
     application performance.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319589" remap="InfraPackArch:SystemZ:Performance">


    <title>Fake NUMA Support</title>
    <para>
     Splitting the system memory into multiple NUMA nodes and distributing
     memory without using real topology information about the physical
     memory can improve performance. This is especially true for large
     systems. This feature is turned off by default but can be enabled for a
     system from the command line.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Misc">

   <title>Miscellaneous</title>
   <para/>

   <section role="notoc" xml:id="fate-321367" remap="InfraPackArch:SystemZ:Misc">


    <title>Enable Boot Parameter quiet for Better Visibility of Password Prompts</title>
    <para>
     <emphasis> In the default configuration of SLES 12 SP2 for z Systems,
     the boot parameter </emphasis><literal>quiet</literal><emphasis> is
     disabled, so the system console shows more useful log messages. This
     has the drawback that the increased amount of log messages can hide a
     password prompt, such as the prompt for decrypting devices at boot.
     </emphasis>
    </para>
    <para>
     To make the password prompt more visible among the system messages, add
     the boot parameter <literal>quiet</literal> when there are encrypted
     devices that need to be activated at system boot.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318899" remap="InfraPackArch:SystemZ:Misc">


    <title>Installing From DVD/USB Drive of the HMC</title>
    <para>
     You can now install from media in the DVD/USB drive of the Hardware
     Management Console (HMC).
    </para>
    <para>
     To do so:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Add <literal>install=hmc:/</literal> to the parm file or kernel
       options.
      </para>
     </listitem>
     <listitem>
      <para>
       Alternatively, in manual mode, in <literal>linuxrc</literal>, choose
       <emphasis>Start Installation</emphasis> &gt;
       <emphasis>Installation</emphasis> &gt; <emphasis>Hardware Management
       Console</emphasis>. The installation medium must be inserted in the
       HMC.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     There are two <literal>.ins</literal> files available which you can
     install with:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>suse.ins</literal> to install with network access. When
       using this option, do not forget to configure the network in
       <literal>linuxrc</literal> before starting the installation. There is
       no way to pass boot parameters later and it is very likely that you
       will need network access. In <literal>linuxrc</literal>, go to
       <emphasis>Start Installation</emphasis> &gt; <emphasis>Network
       Setup</emphasis>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>susehmc.ins</literal> which allows installing without
       network access.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     <emphasis role="bold">Important:</emphasis> Wait until the Linux system
     is booting before granting access to the DVD in the HMC. IPLing seems
     to disrupt the connection between the HMC and the LPAR in some way. If
     the first attempt to use it fails, you can grant the access and retry
     the option <emphasis>HMC</emphasis>.
    </para>
    <para>
     <emphasis role="bold">Note</emphasis>: Because of the transitory
     nature of the assignment, the DVD that was used during installation
     will not be kept as a repository. If you need an installation
     repository there, register and use the online repository.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="InfraPackArch-AArch64" remap="InfraPackArch:AArch64">
  <title>ARM 64-Bit (AArch64) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP2 for the AArch64 architecture.
  </para>

  <section role="notoc" xml:id="fate-321497" remap="InfraPackArch:AArch64">


   <title>KVM on AArch64</title>
   <para>
    KVM virtualization has been enabled and is supported on some
    system-on-chip platforms for mutually agreed-upon partner-specific use
    cases. It is only supported on partner certified hardware and firmware.
    Not all QEMU options and backends are available on AArch64. The same
    statement is applicable for other virtualization tools shipped on
    AArch64.
   </para>
  </section>
  <section role="notoc" xml:id="fate-320679" remap="InfraPackArch:AArch64">


   <title>Toolchain Module Enabled in Default Installation</title>
   <para>
    <emphasis> The system compiler (</emphasis><literal>gcc4.8</literal><emphasis>) is not supported on the
    AArch64 architecture. To work around this issue, you previously had to
    enable the Toolchain module manually and use the GCC version from that
    module. </emphasis>
   </para>
   <para>
    On AArch64, the Toolchain Module is now automatically pre-selected after
    registering SLES during installation. This makes the latest SLE
    compilers available on all installations. You now only need to make sure
    to also use that compiler.
   </para>
   <important>
    <title>When Using AutoYaST, Make Sure to Enable Toolchain Module</title>
    <para>
     Be aware that when using AutoYaST to install, you have to explicitly
     add the Toolchain module into the XML installation profile.
    </para>
   </important>
  </section>
  <section role="notoc" xml:id="fate-319898" remap="InfraPackArch:AArch64">


   <title>GICv2 and GICv3 Interrupt Controller Support in QEMU</title>
   <para>
    KVM/QEMU now works with GICv2 and GICv3 interrupt controllers that
    implement virtualization capabilities.
   </para>
  </section>
  <section role="notoc" xml:id="fate-319482" remap="InfraPackArch:AArch64">


   <title>Boot Requirements for AppliedMicro X-Gene 1</title>
   <para>
    The AppliedMicro X-C1 Server Development Platform (Mustang) ships with
    U-Boot based firmware. To install SUSE Linux Enterprise Server 12 SP2,
    the firmware needs to be updated to the UEFI based firmware version
    3.06.15 or newer.
   </para>
   <para>
    Other server systems, such as Gigabyte MP30, may also require a firmware
    update for an optimal experience. For details, contact your vendor.
   </para>
  </section>
  <section role="notoc" xml:id="fate-318444" remap="InfraPackArch:AArch64">


   <title>ARM AArch64 System-on-Chip Platform Driver Enablement</title>
   <para>
    <emphasis>For ARM based systems to boot SUSE Linux Enterprise Server,
    some chipset-specific drivers are needed.</emphasis>
   </para>
   <para>
    The following System-on-Chip (SoC) platforms have been enabled for SP2:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      AMD Opteron A1100
     </para>
    </listitem>
    <listitem>
     <para>
      AppliedMicro X-Gene 1
     </para>
    </listitem>
    <listitem>
     <para>
      AppliedMicro X-Gene 2
     </para>
    </listitem>
    <listitem>
     <para>
      Cavium ThunderX
     </para>
    </listitem>
    <listitem>
     <para>
      NXP QorIQ LS2085A / LS2045A, LS2080A / LS2040A
     </para>
    </listitem>
    <listitem>
     <para>
      Xilinx UltraScale+ MPSoC
     </para>
    </listitem>
   </itemizedlist>
  </section>

 </section>
 <section xml:id="Driver-Updates">
  <title>Driver Updates</title>
  <remark>** list of update by category ** version comparison table from
  previous SP/Product </remark>
  <para/>
  <section xml:id="Driver-Updates-Network" remap="Driver_Updates:Network">
   <title>Network Drivers</title>
   <para/>

   <section role="notoc" xml:id="fate-321027" remap="Driver_Updates:Network">


    <title>Support Status of Ethernet Drivers</title>
    <para>
     <emphasis>Ethernet drivers have been added between kernel versions 3.12
     (SLES 12 GA) and 4.4 (SLES 12 SP2).</emphasis>
    </para>
    <para>
     The support status of Ethernet drivers has been updated for SLE 12 SP2
     and below is the list of newly supported drivers.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Agere Systems ET1310 (et131x)
      </para>
     </listitem>
     <listitem>
      <para>
       Qualcomm Atheros AR816x/AR817x PCI-E (alx)
      </para>
     </listitem>
     <listitem>
      <para>
       Broadcom BCM573xx (bnxt_en)
      </para>
     </listitem>
     <listitem>
      <para>
       JMicron JMC2x0 PCI-E (jme)
      </para>
     </listitem>
     <listitem>
      <para>
       QLogic FastLinQ 4xxxx (qede)
      </para>
     </listitem>
     <listitem>
      <para>
       SMC 83c170 EPIC series (epic100)
      </para>
     </listitem>
     <listitem>
      <para>
       SMSC LAN911x/LAN921x (smsc911x)
      </para>
     </listitem>
     <listitem>
      <para>
       SMSC LAN9420 PCI (smsc9420)
      </para>
     </listitem>
     <listitem>
      <para>
       STMMAC 10/100/1000 PCI (stmmac-pci)
      </para>
     </listitem>
     <listitem>
      <para>
       WIZnet W5100 (w5100)
      </para>
     </listitem>
     <listitem>
      <para>
       WIZnet W5300 (w5300)
      </para>
     </listitem>
     <listitem>
      <para>
       FUJITSU Extended Socket Network (fjes)
      </para>
     </listitem>
     <listitem>
      <para>
       SMSC95XX USB (smsc95xx)
      </para>
     </listitem>
     <listitem>
      <para>
       Xilinx LL TEMAC (ll_temac)
      </para>
     </listitem>
     <listitem>
      <para>
       APM X-Gene (xgene-enet)
      </para>
     </listitem>
     <listitem>
      <para>
       Cavium Thunder (nicpf, nicvf, thunder_bgx)
      </para>
     </listitem>
    </itemizedlist>
   </section>

  </section>
  <section xml:id="Driver-Updates-Other" remap="Driver_Updates:Other">
   <title>Other Drivers</title>
   <para/>

   <section role="notoc" xml:id="fate-320122" remap="Driver_Updates:Other">


    <title>Support for New Intel Processors</title>
    <para>
     This Service Pack adds support for the following Intel processors:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Intel® Xeon® Processor E3-1200/1500 v5 Product Family
      </para>
     </listitem>
     <listitem>
      <para>
       Intel® Xeon Phi™ Product Family x200
      </para>
     </listitem>
    </itemizedlist>
   </section>

  </section>
 </section>
 <section xml:id="Packages">
  <title>Packages and Functionality Changes</title>
  <para>
   This section comprises changes to packages, such as additions, updates,
   removals and changes to the package layout of software. It also contains
   information about modules available for SUSE Linux Enterprise Server. For
   information about changes to package management tools, such as Zypper or
   RPM, see
   <xref linkend="InfraPackArch-ArchIndependent-SystemsManagement"/>.
  </para>
  <section xml:id="Packages-New" remap="Packages:New">
   <title>New Packages</title>
   <remark> *** Description of the new package with direct link to the official website (need the
    URL tag) *** Version and release of the package *** Why this new package: enhancements request
    *** link to the official documentation *** link to the SUSE's documentation *** Dependencies and
    Build requires </remark>
   <para/>

   <section role="notoc" xml:id="fate-324454" remap="Packages:New">


    <title>Icinga Monitoring Server Shipped as Part of SUSE Manager</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     Fully supported packages of the Icinga monitoring server for SUSE Linux
     Enterprise Server 12 are available with a SUSE Manager subscription.
     Icinga is compatible with a previously included monitoring server.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320751" remap="Packages:New">


    <title>Mutt Has Been Updated to 1.6.0</title>
    <para>
     Mutt has been updated to version 1.6.0. This version has the following
     new features:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Better internationalization support: UTF-8 mailbox support for IMAP
       and improved support for internationalized email and SMTPUTF8
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>$use_idn</literal> has been renamed to
       <literal>$idn_decode</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Expandos for comma-separated lists of To (<literal>%r</literal>)
       and CC recipients (<literal>%R</literal>).
      </para>
     </listitem>
     <listitem>
      <para>
       Improved handling of drafts: <literal>-E</literal> command-line
       argument for edit draft or include files,
       <literal>$resume_draft_files</literal> and
       <literal>$resume_edited_draft_files</literal> to control processing
       of draft files, and support for multipart draft files
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>$reflow_space_quotes</literal> allows
       <literal>format=flowed</literal> email quotes to be displayed with
       spacing between them.
      </para>
     </listitem>
     <listitem>
      <para>
       The S/MIME message digest algorithm is now specified using the option
       <literal>$smime_sign_digest_alg</literal>.
       <literal>$smime_sign_command</literal> should be modified to include
       <literal>-md %d</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       For classic GPG mode, set <literal>$pgp_decryption_okay</literal> to
       verify that multipart/encrypted mails are actually encrypted.
      </para>
     </listitem>
     <listitem>
      <para>
       By default, mailto URL header parameters are restricted to
       <literal>body</literal> and <literal>subject</literal>. To add or
       remove allowed mailto URL header parameters, use
       <literal>mailto_allow</literal> and <literal>unmailto_allow</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>$hostname</literal> is set differently: the domain will now
       be determined using DNS calls
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-320434" remap="Packages:New">


    <title>targetcli-fb Has Been Added</title>
    <para>
     In addition to the established tool <literal>targetcli</literal>,
     there is now also its enhanced version <literal>targetcli-fb</literal>
     available. New users are encouraged to deploy
     <literal>targetcli-fb</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320018" remap="Packages:New">


    <title>Devilspie 2 Has Been Added</title>
    <para>
     <emphasis>Desktop users often want the size and position of windows to
     remain the same, even across application restarts. Such functionality
     usually has to be implemented at the application level but not all
     applications do so.</emphasis>
    </para>
    <para>
     In SUSE Linux Enterprise 12 SP2, Devilspie 2 (package
     <literal>devilspie2</literal>) has been added. Desvilspie 2 is a
     window matching utility that allow you to script actions on windows as
     they are created, such as maximizing windows or setting their size and
     position.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319461" remap="Packages:New">


    <title>openldap2-ppolicy-check-password Has Been Added: OpenLDAP Password Strength Policy Enforcer</title>
    <para>
     To allow evaluating and enforcing password strength in an OpenLDAP
     deployment, the package
     <literal>openldap2-ppolicy-check-password</literal> has been added. It
     is an OpenLDAP password policy plugin which evaluates and enforces
     strength in new user passwords, and denies weak passwords in password
     change operations. Configuration options of the plugin allow system
     administrators to adjust password strength requirements.
    </para>
   </section>

  </section>
  <section xml:id="Packages-Update" remap="Packages:Update">
   <title>Updated Packages</title>
   <remark> *** Description of the new package with direct link to the official website *** version
    comparison table from previous SP/product *** Bugs fixed since previous SP/Product (link to
    Bugzilla) *** Enhancements request (link to Fate number) *** link to the official documentation
    *** link to the SUSE's documentation *** Security fixes (link to CVE or SUSE's security web
    site), link to PIF *** Changelog diff (all other information non relative to security/fate/bugs) </remark>
   <para/>

   <section role="notoc" xml:id="fate-321010" remap="Packages:Update">


    <title>Ceph Client Enablement Has Been Upgraded to Ceph Jewel</title>
    <para>
     <emphasis>SUSE Enterprise Storage 3 and later versions expose
     additional functionality and performance to upgraded clients, such as
     the use of advanced RBD features and improved CephFS integration. While
     SUSE Enterprise Storage 3 is backwards-compatible with older clients,
     the full benefits are only available to newer clients.</emphasis>
    </para>
    <para>
     As part of SUSE Linux Enterprise Server 12 Service Pack 2, the Ceph
     client code, as provided by <literal>ceph-common</literal> and the
     related library packages, has been upgraded to match the latest SUSE
     Enterprise Storage release.
    </para>
    <para>
     This update also includes rebuilt versions of the KVM integration to
     take advantage of these.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320889" remap="Packages:Update">


    <title>Upgrade of libStorageMgmt to Version 1.3.2</title>
    <para>
     <emphasis>libStorageMgmt allows programmatically managing storage
     hardware in a vendor-neutral way.</emphasis>
    </para>
    <para>
     In SLES 12 SP2, libStorageMgmt was upgraded to version 1.3.2. This
     version fixes several bugs and adds the ability to more retrieve disk
     information, such as information on batteries and the list of local
     disks.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320594" remap="Packages:Update">


    <title>Glibc Has Been Upgraded to Version 2.22</title>
    <para>
     glibc has been upgraded to meet demands in transactional memory
     handling and memory protection and to gain performance optimizations
     for modern platforms.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320541" remap="Packages:Update">


    <title>lsof Has Been Updated to Version 4.89</title>
    <para>
     lsof has been updated from version 4.84 to 4.89. The changelog can be
     found in the file <literal>/usr/share/doc/packages/lsof/DIST</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320330" remap="Packages:Update">


    <title>Qt 5 Has Been Updated to 5.6.1</title>
    <para>
     The Qt 5 libraries were updated to 5.6.1, a Qt 5.6 LTS based release.
     Qt 5.6.1 includes new features and security fixes for known
     vulnerabilities over Qt 5.5.1 (the version shipped in an upgrade to
     SP1).
    </para>
    <para>
     This release includes many bug fixes and changes that improve
     performance and reduce memory consumption.
    </para>
    <para>
     For security reasons, the MNG and JPEG2000 image format plugins are not
     shipped anymore, because the underlying MNG and JPEG2000 libraries have
     known security issues.
    </para>
    <para>
     New features include:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Better support for high-DPI screens
      </para>
     </listitem>
     <listitem>
      <para>
       Update of QtWebEngine which updates the included Chromium snapshot to
       version 45 and now uses many of the system libraries instead of
       bundled ones
      </para>
     </listitem>
     <listitem>
      <para>
       New Qt WebEngineCore module for new low-level APIs
      </para>
     </listitem>
     <listitem>
      <para>
       The Qt Location module is not fully supported.
      </para>
     </listitem>
     <listitem>
      <para>
       Improved compatibility with C++11 and the STL
      </para>
     </listitem>
     <listitem>
      <para>
       New QVersionNumber class
      </para>
     </listitem>
     <listitem>
      <para>
       Added support for HTTP redirection in QNetworkAccessManager
      </para>
     </listitem>
     <listitem>
      <para>
       Improved support for OpenGL ES 3
      </para>
     </listitem>
     <listitem>
      <para>
       Qt Multimedia got a new PlayList QML type and an audio role API for
       the media player
      </para>
     </listitem>
     <listitem>
      <para>
       Qt Canvas 3D now supports Qt Quick Items as textures and can directly
       render to the QML scenes foreground or background
      </para>
     </listitem>
     <listitem>
      <para>
       Qt 3D has received many improvements and new functionality
      </para>
     </listitem>
     <listitem>
      <para>
       Many other features and bugfixes
      </para>
     </listitem>
    </itemizedlist>
    <para>
     As part of this update, Qt Creator has been updated to 4.0.1 (from Qt
     Creator 3.5.1 shipped as an update to SP1).
    </para>
    <para>
     New features of Qt Creator include:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Clang static analyzer integration, extended QML profiler features,
       path editor of Qt Quick Designer and auto test integration
       (experimental) are now available
      </para>
     </listitem>
     <listitem>
      <para>
       The Clang code model is now automatically used if the (experimental)
       plugin is turned on
      </para>
     </listitem>
     <listitem>
      <para>
       Improved workflow for CMake-based projects
      </para>
     </listitem>
     <listitem>
      <para>
       The Analyze mode was merged with Debug mode, so that the new unified
       Debug mode includes the Debugger, Clang Static Analyzer, Memcheck,
       Callgrind and QML Profiler tools
      </para>
     </listitem>
     <listitem>
      <para>
       Many other features and bugfixes
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-320245" remap="Packages:Update">


    <title>RPM Ignores the BuildRoot Directive in Spec Files</title>
    <para>
     <emphasis> In versions of RPM greater than 4.6.0, the behavior of the
     </emphasis><literal>BuildRoot</literal><emphasis> directive was changed
     compared to prior versions. RPM now enforces using a build root for all
     packages and ignores the
     </emphasis><literal>BuildRoot</literal><emphasis> directive in spec
     files. By default, </emphasis><literal>rpmbuild</literal><emphasis>
     places the build root inside
     </emphasis><literal>%{_topdir}</literal><emphasis>. However, this can
     be changed through macro configuration. </emphasis>
    </para>
    <para>
     In the version of RPM shipped with SUSE Linux Enterprise 12 (and
     later), the <literal>BuildRoot</literal> directive of spec files is
     silently ignored. However, it is recommended to keep the
     <literal>BuildRoot</literal> directive in spec files for backward
     compatibility with earlier versions of SUSE Linux Enterprise (and RPM).
    </para>
    <para>
     For more information, see the RPM 4.6.0 release notes at
     <link xlink:href="https://rpm.org/wiki/Releases/4.6.0">https://rpm.org/wiki/Releases/4.6.0</link>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319675" remap="Packages:Update">


    <title>OpenSSH Has Been Updated to Version 7.2</title>
    <para>
     <emphasis>To bring more features and bugfixes we synced the openSSH
     version to a newer release.</emphasis>
    </para>
    <para>
     OpenSSH received numerous changes and improvements in the last years.
     To ease further maintenance, OpenSSH was upgraded to a more current
     release.
    </para>
    <para>
     Note that the SSHv1 protocol is no longer supported.
    </para>
    <para>
     Further changes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The "UseDNS" option now defaults to 'no'. Configurations that match
       against the client host name (via sshd_config or authorized_keys) may
       need to re-enable it or convert to matching against addresses.
      </para>
     </listitem>
     <listitem>
      <para>
       The default set of ciphers and MACs has been altered to remove unsafe
       algorithms. In particular, CBC ciphers and arcfour* are disabled by
       default. The full set of algorithms remains available if configured
       explicitly via the Ciphers and MACs sshd_config options.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-319493" remap="Packages:Update">


    <title>Puppet Has Been Updated from 3.6.2 to 3.8.5</title>
    <para>
     Puppet has been updated from 3.6.2 to 3.8.5. All releases between these
     two versions should only bring Puppet 3 backward-compatible features
     and bug and security fixes.
    </para>
    <para>
     In particular, you should pay attention to the following upgrade notes
     and warnings:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The new default value of the <literal>environment_timeout</literal>
       option is <literal>0</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       You can now set the parser setting per-environment in
       <literal>environment.conf</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Make sure the keepalive timeout is configured to be five or more
       seconds.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-319365" remap="Packages:Update">


    <title>Changes in Behavior Between coreutils 8.22 and 8.25</title>
    <para>
     SLE 12 SP1 shipped with coreutils 8.22. SLE 12 SP2 ships with coreutils
     8.25. This new release brings a number of changes in behavior:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>base64</literal>: <literal>base64</literal> no longer
       supports <literal>--wrap</literal> parameters in hexal or octal
       format. This improves support for decimals with leading zeros.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>chroot</literal>: Using <literal>/</literal> as the
       argument no longer implicitly changes the current directory to
       <literal>/</literal>. This allows changing user credentials for a
       single command only.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>chroot</literal>: <literal>--userspec</literal> will now
       unset supplemental groups associated with root and instead use the
       supplemental groups of the specified user.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>cut</literal>: Using <literal>-d$'\n'</literal> will again
       output lines identified in the <literal>--fields</literal> list (this
       behavior had been changed in version 8.21 and 8.22). Note that this
       functionality is non-portable and will result in the delayed output
       of lines.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>date</literal>: The option <literal>--iso-8601</literal>
       now uses the timezone format <literal>+00:00</literal> rather than
       <literal>+0000</literal>. This "extended" format is preferred by the
       ISO 8601 standard.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>df</literal>: <literal>df</literal> now prefers sources
       towards the root of a device when eliding duplicate
       <literal>bind</literal> -mounted entries.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>df</literal>: <literal>df</literal> no longer suppresses
       separate exports of the same remote device, as these are generally
       explicitly mounted. The <literal>--total</literal> option does still
       suppress duplicate remote file systems.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>join</literal>, <literal>sort</literal>,
       <literal>uniq</literal>: When called with
       <literal>--zero-terminated</literal>, these commands now treat
       <literal>\n</literal> as a field delimiter.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>ls</literal>: If neither of the environment variables
       <literal>LS_COLORS</literal> and <literal>COLORTERM</literal> is set
       and the environment variable <literal>TERM</literal> is empty or
       unknown, <literal>ls</literal> will now not output colors even with
       <literal>--colors=always</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>ls</literal>: When outputting to a terminal,
       <literal>ls</literal> now quotes file names unambiguously and
       appropriate for use in a shell.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mv</literal>: <literal>mv</literal> no longer supports
       moving a file to a hard link. If you try, it issues an error. The
       prior implementation was susceptible to races in the presence of
       multiple <literal>mv</literal> instances which could result in both
       hard links being deleted. Also, on case-insensitive file systems like
       HFS, <literal>mv</literal> would remove a hardlinked file if called
       like <literal>mv file File</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>numfmt</literal>: The options
       <literal>--from-unit</literal> and <literal>--to-unit</literal> now
       interpret suffixes as SI units, and IEC (power of 2) units are now
       specified by appending <literal>i</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>tee</literal>: If there are no more writable outputs,
       <literal>tee</literal> will exit early.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>tee</literal>: <literal>tee</literal> does not treat the
       file operand <literal>-</literal> as meaning standard output any
       longer. This allows for better POSIX conformance.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>timeout</literal>: The option
       <literal>--foreground</literal> no longer sends
       <literal>SIGCONT</literal> to the monitored process, as this was seen
       to cause intermittent issues with GDB for example.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-317043" remap="Packages:Update">


    <title>openSSL Has Been Updated to Version 1.0.2</title>
    <para>
     <emphasis>openSSL has been updated from version 1.0.1 to 1.0.2 which is
     a compatible minor version update. This will help future maintenance,
     and also brings many bug fixes.</emphasis>
    </para>
    <para>
     The update to openSSL 1.0.2 should be transparent to existing programs.
    </para>
    <para>
     However, there were some functional changes were done: SSL 2 support is
     now fully disabled and certain weak ciphers are no longer built in.
    </para>
   </section>

  </section>
  <section xml:id="Packages-Deprecated" remap="Packages:Deprecated">
   <title>Removed and Deprecated Functionality</title>
   <remark>
    ** Why this choice?
    ** How to handle this deprecation in your system
    ** specific case (LTSS etc.)
   </remark>

   <section role="notoc" xml:id="fate-320983" remap="Packages:Deprecated">


    <title>Perl Bindings for Cyrus Have Been Removed</title>
    <para>
     With SLE 12 SP2, the packages <literal>perl-Cyrus-IMAP</literal> and
     <literal>perl-Cyrus-SIEVE-managesieve</literal> have been removed from
     the media.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320900" remap="Packages:Deprecated">


    <title>librpcsecgss3 Has Been Removed</title>
    <para>
     <literal>librpcsecgss</literal> (packages:
     <literal>librpcsecgss3</literal>,
     <literal>librpcsecgss-devel</literal>) has been removed. With the
     release of <literal>libtirpc</literal>, the development of libsecgss
     stopped and it fell out of use. We recommend using
     <literal>libtirpc</literal> instead.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320740" remap="Packages:Deprecated">


    <title>Docker Compose Has Been Removed from the Containers Module</title>
    <para>
     <emphasis>Docker Compose is not supported as a part of SUSE Linux
     Enterprise Server 12. While it was temporarily included as a Technology
     Preview, testing showed that the technology was not ready for
     enterprise use.</emphasis>
    </para>
    <para>
     SUSE's focus is on Kubernetes which provides better value in terms of
     features, extensibility, stability and performance.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319923" remap="Packages:Deprecated">


    <title>libusnic_verbs-rdmav2 and libusnic_verbs-rdmav2-pingpong Are Now Obsolete</title>
    <para>
     Functionality previously shipped in the packages
     <literal>libusnic_verbs-rdmav2</literal> and
     <literal>libusnic_verbs-rdmav2-pingpong</literal> has been integrated
     into <literal>libibverbs</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-316136" remap="Packages:Deprecated">


    <title>Nagios Monitoring Server Has Been Removed</title>
    <para>
     The Nagios monitoring server has been removed from SLES 12.
    </para>
    <para>
     When upgrading to SLES 12 or later, installed Nagios configuration may
     be removed. Therefore, we recommend creating backups of the Nagios
     configuration before the upgrade.
    </para>
   </section>

   <section xml:id="Packages-Deprecated-rm-with-12sp1" remap="Packages:Deprecated:rm_with_12sp1">
    <title>Packages Removed with SUSE Linux Enterprise Server 12 SP1</title>
    <para>
     The packages listed below were removed with the release of SUSE Linux
     Enterprise Server 12 SP1.
    </para>

    <section role="notoc" xml:id="fate-319666" remap="Packages:Deprecated:rm_with_12sp1">


     <title>wpa_supplicant Replaces xsupplicant</title>
     <para>
      In SUSE Linux Enterprise 12 SP1 and 12 SP2,
      <literal>xsupplicant</literal> was removed entirely.
     </para>
     <para>
      For pre-authentication of systems via network (including RADIUS) and
      specifically wireless connections, install the
      <literal>wpa_supplicant</literal> package.
      <literal>wpa_supplicant</literal> now replaces
      <literal>xsupplicant</literal>. <literal>wpa_supplicant</literal>
      provides better stability, security and a broader range of
      authentication options.
     </para>
    </section>

   </section>
   <section xml:id="Packages-Deprecated-Future" remap="Packages:Deprecated:Future">
    <title>Packages and Features to Be Removed in the Future</title>
    <para/>

    <section role="notoc" xml:id="fate-321117" remap="Packages:Deprecated:Future">


     <title>Server Component of Puppet Is Deprecated</title>
     <para>
      <emphasis>Puppet is shipped as part of the Advanced Systems Management
      module for SLES. Currently, this module contains both the Puppet
      client and the Puppet server.</emphasis>
     </para>
     <para>
      Starting with the packages for Puppet 4 which will be released in
      early 2017, SUSE will only ship the Puppet client (currently packaged
      as <literal>puppet</literal>, in the future packaged as
      <literal>rubygem-puppet</literal>) but not the Puppet server (package
      <literal>puppet-server</literal>).
     </para>
     <para>
      For a period of 6 months after the release of the Puppet 4 package,
      SUSE will continue to provide and support packages for Puppet 3. SUSE
      will not support the migration of the server package.
     </para>
    </section>

   </section>
  </section>
  <section xml:id="Packages-Packaging" remap="Packages:Packaging">
   <title>Changes in Packaging and Delivery</title>
   <para/>

   <section role="notoc" xml:id="fate-321179" remap="Packages:Packaging">


    <title>GNOME Desktop: Clicking "Open in Terminal" on the Desktop Now Opens the Home Directory</title>
    <para>
     <emphasis> When right-clicking the GNOME desktop and selecting
     </emphasis>Open in Terminal<emphasis>, GNOME Terminal will now open
     with the working directory set to the home directory (</emphasis><literal>~</literal><emphasis>) and not set to the Desktop
     directory (</emphasis>~/Desktop<emphasis>). This happens because the
     package
     </emphasis><literal>nautilus-extension-terminal</literal><emphasis> is
     now installed by default. </emphasis>
    </para>
    <para>
     To switch to the former behavior, first uninstall
     <literal>nautilus-extension-terminal</literal> and then install
     <literal>nautilus-open-terminal</literal>. However, note that the
     package <literal>nautilus-open-terminal</literal> may not be provided
     in future service packs.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321021" remap="Packages:Packaging">


    <title>Change of OpenMPI Behavior for Plugin Developers</title>
    <para>
     <emphasis> To be compliant with the upstream version of OpenMPI, the
     source configuration option
     </emphasis><literal>--with-devel-header</literal><emphasis> has been
     removed. This only affects developers of OpenMPI plugins outside of the
     source tree. </emphasis>
    </para>
    <para>
     Developers of plugins outside of the source tree need to recompile the
     source with the option <literal>--with-devel-header</literal> added.
     All other users are not affected.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319240" remap="Packages:Packaging">


    <title>Support for Intel OPA Fabrics Moved to mvapich2-psm2 Package</title>
    <para>
     <emphasis> The version of the package
     </emphasis><literal>mvapich2-psm</literal><emphasis> originally shipped
     with SLES 12 SP2 and SLES 12 SP3 exclusively supported Intel Omni-Path
     Architecture (OPA) fabrics. In SLES 12 SP1 and earlier, this package
     supported the use of Intel True Scale fabrics instead. </emphasis>
    </para>
    <para>
     This issue is fixed by a maintenance update providing an additional
     package named <literal>mvapich2-psm2</literal> which only supports
     Intel OPA, whereas the original package <literal>mvapich2-psm</literal>
     only supports Intel True Scale fabrics again.
    </para>
    <para>
     If you are currently using <literal>mvapich2-psm</literal> together
     with Intel OPA fabrics, make sure to switch to the new package
     <literal>mvapich2-psm2</literal> after this maintenance update.
    </para>
   </section>

  </section>
  <section xml:id="Packages-Modules" remap="Packages:Modules">
   <title>Modules</title>
   <para>
    This section contains information about important changes to modules.
    For more information about available modules, see
    <xref linkend="Intro-Module"/>.
   </para>

   <section role="notoc" xml:id="fate-320852" remap="Packages:Modules">


    <title>libgcrypt11 Available from the Legacy Module</title>
    <para>
     The Legacy module now provides a package for
     <literal>libgcrypt11</literal>. This enables running applications
     built on SLES 11 against <literal>libgcrypt11</literal> on SLES 12.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320127" remap="Packages:Modules">


    <title>PHP 7 Packages Have Been Added to the Web and Scripting Module</title>
    <para>
     <emphasis>So far, the Web and Scripting module for SLES contained
     packages for PHP 5 only.</emphasis>
    </para>
    <para>
     The Web and Scripting module for SLES now additionally contains
     packages for PHP 7. For a detailed overview of changes over PHP 5, see
     <link xlink:href="https://secure.php.net/releases/7_0_0.php">https://secure.php.net/releases/7_0_0.php</link>.
    </para>
   </section>

  </section>
  <section xml:id="SDK" remap="SDK">
   <title>SDK</title>
   <para/>

   <section role="notoc" xml:id="fate-318421" remap="SDK">


    <title>Byebug Has Been Added</title>
    <para>
     Byebug is a simple-to-use, feature-rich Ruby 2 debugger that is also
     used to debug YaST. It uses the TracePoint API and the Debug Inspector
     API. For speed, it is implemented as a C extension.
    </para>
    <para>
     It allows you to see what is going on inside a Ruby program while it
     executes and offers traditional debugging features such as stepping,
     breaking, evaluating, and tracking.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="TechInfo" remap="TechInfo">
  <title>Technical Information</title>
  <para>
   This section contains information about system limits, a number of
   technical changes and enhancements for the experienced user.
  </para>

  <para>
   When talking about CPUs, we use the following terminology:
  </para>
  <variablelist>
   <varlistentry>
    <term>CPU Socket</term>
    <listitem>
     <para>
      The visible physical entity, as it is typically mounted to a
      motherboard or an equivalent.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CPU Core</term>
    <listitem>
     <para>
      The (usually not visible) physical entity as reported by the CPU
      vendor.
     </para>
     <para>
      On IBM z Systems, this is equivalent to an IFL.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logical CPU</term>
    <listitem>
     <para>
      This is what the Linux Kernel recognizes as a "CPU".
     </para>
     <para>
      We avoid the word "thread" (which is sometimes used), as the word
      "thread" would also become ambiguous subsequently.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual CPU</term>
    <listitem>
     <para>
      A logical CPU as seen from within a Virtual Machine.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <section role="notoc" xml:id="fate-317893" remap="TechInfo">


   <title>Virtualization: Network Devices Supported</title>
   <para>
    SLES 12 supports the following virtualized network drivers:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Full virtualization: Intel e1000
     </para>
    </listitem>
    <listitem>
     <para>
      Full virtualization: Realtek 8139
     </para>
    </listitem>
    <listitem>
     <para>
      Paravirtualized: QEMU Virtualized NIC Card (virtio, KVM only)
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section role="notoc" xml:id="fate-317892" remap="TechInfo">


   <title>Virtualization: Devices Supported for Booting</title>
   <para>
    SLE12 support VM guest to boot from:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Parallel ATA (PATA/IDE)
     </para>
    </listitem>
    <listitem>
     <para>
      Advanced Host Controller Interface (AHCI)
     </para>
    </listitem>
    <listitem>
     <para>
      Floppy Disk Drive (FDD)
     </para>
    </listitem>
    <listitem>
     <para>
      virtio-blk
     </para>
    </listitem>
    <listitem>
     <para>
      virtio-scsi
     </para>
    </listitem>
    <listitem>
     <para>
      Preboot eXecution Environment (PXE) ROMs (for supported Network
      Interface Cards)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Boot from <literal>USB</literal> and <literal>PCI pass-through</literal>
    devices are not supported.
   </para>
  </section>
  <section role="notoc" xml:id="fate-317891" remap="TechInfo">


   <title>Virtualization: Supported Disks Formats and Protocols</title>
   <para>
    The following disk formats support read-write access (RW):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>raw</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>qed</literal> (KVM only)
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>qcow2</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The following disk formats support read-only access (RO):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>vmdk</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>vpc</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>vhd</literal> / <literal>vhdx</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The following protocols can be used for read-only access (RO) to images:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>http</literal>, <literal>https</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>ftp</literal>, <literal>ftps</literal>,
      <literal>tftp</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    When using Xen, the <literal>qed</literal> format will not be displayed
    as a selectable storage in <literal>virt-manager</literal>.
   </para>
   <note>
    <title>Parameter Unprivileged SG_IO (unpriv_sgio) Is Not Supported</title>
    <para>
     The parameter for unprivileged SG_IO (<literal>unpriv_sgio</literal>)
     depends on non-standard kernel patches that are not included in the
     SLES 12 kernel. Trying to attach a disk using this parameter will
     result in an error.
    </para>
   </note>
  </section>

  <section xml:id="TechInfo-Kernel">
   <title>Kernel Limits</title>
   <para>
    This table summarizes the various limits which exist in our recent
    kernels and utilities (if related) for SUSE Linux Enterprise Server 12
    SP2.
   </para>
   <informaltable frame="all">
    <tgroup cols="5">
     <colspec colnum="1" colname="feature"/>
     <colspec colnum="2" colname="x86-64"/>
     <colspec colnum="3" colname="z"/>
     <colspec colnum="4" colname="power"/>
     <colspec colnum="5" colname="aarch64"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP2 (Linux 4.4)</emphasis>
       </entry>
       <entry>AMD64/Intel 64 (x86_64)</entry>
       <entry>IBM z Systems (s390x)</entry>
       <entry>POWER (ppc64le)</entry>

       <entry>AArch64 (ARMv8)</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         CPU bits
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of logical CPUs
        </para>
       </entry>

       <entry>
        <para>
         8192
        </para>
       </entry>
       <entry>
        <para>
         256
        </para>
       </entry>

       <entry>
        <para>
         2048
        </para>
       </entry>
       <entry>
        <para>
         128
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of RAM (theoretical/certified)
        </para>
       </entry>
       <entry>
        <para>
         &gt; 1 PiB/64 TiB
        </para>
       </entry>
       <entry>
        <para>
         10 TiB/256 GiB
        </para>
       </entry>

       <entry>
        <para>
         1 PiB/64 TiB
        </para>
       </entry>
       <entry>
        <para>
         256 TiB/n.a.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of user space/kernel space
        </para>
       </entry>
       <entry>
        <para>
         128 TiB/128 TiB
        </para>
       </entry>
       <entry>
        <para>
         n.a.
        </para>
       </entry>
       <entry>
        <para>
         64 TiB/2 EiB
        </para>
       </entry>
       <entry>
        <para>
         256 TiB/128 TiB
        </para>
       </entry>
      </row>
      <row>
       <entry>

        <para>
         Maximum amount of swap space
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         Up to 29 * 64 GB (x86_64) or 30 * 64 GB (other architectures)
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of processes
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         1048576
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of threads per process
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         Upper limit depends on memory and other parameters (tested with
         more than 120,000).
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum size per block device
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         Up to 8 EiB on all 64-bit architectures
        </para>
       </entry>
      </row>
      <row>

       <entry>
        <para>
         FD_SETSIZE
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         1024
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
  <section xml:id="TechInfo-KVM">
   <title>KVM Limits</title>
   <informaltable frame="all">
    <tgroup cols="2">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP2 Virtual Machine (VM)</emphasis>
       </entry>
       <entry>Limits</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Maximum VMs per host
        </para>
       </entry>
       <entry>
        <para>
         Unlimited (total number of virtual CPUs in all guests being no
         greater than 8 times the number of CPU cores in the host)
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum Virtual CPUs per VM
        </para>
       </entry>
       <entry>
        <para>
         240
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum Memory per VM
        </para>
       </entry>
       <entry>
        <para>
         4 TiB
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <para>
    Virtual Host Server (VHS) limits are identical to those of SUSE Linux
    Enterprise Server.
   </para>
  </section>
  <section xml:id="TechInfo-XEN">
   <title>Xen Limits</title>
   <para>
    Since SUSE Linux Enterprise Server 11 SP2, we removed the 32-bit
    hypervisor as a virtualization host. 32-bit virtual guests are not
    affected and are fully supported with the provided 64-bit hypervisor.
   </para>

   <informaltable frame="all">
    <tgroup cols="2">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP2 Virtual Machine (VM)</emphasis>
       </entry>
       <entry>Limits</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Maximum number of virtual CPUs per VM
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of memory per VM
        </para>
       </entry>
       <entry>
        <para>
         16 GiB x86_32, 511 GiB x86_64
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <informaltable frame="all">
    <tgroup cols="2">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP2 Virtual Host Server (VHS)</emphasis>
       </entry>
       <entry>Limits</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Maximum number of physical CPUs
        </para>
       </entry>
       <entry>
        <para>
         256
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of virtual CPUs
        </para>
       </entry>
       <entry>
        <para>
         256
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of physical memory
        </para>
       </entry>
       <entry>
        <para>
         5 TiB
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of Dom0 physical memory
        </para>
       </entry>
       <entry>
        <para>
         500 GiB
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of block devices
        </para>
       </entry>
       <entry>
        <para>
         12,000 SCSI logical units
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <itemizedlist>
    <listitem>
     <formalpara>
      <title>PV:</title>
      <para>
       Paravirtualization
      </para>
     </formalpara>
    </listitem>
    <listitem>
     <formalpara>
      <title>FV:</title>
      <para>
       Full virtualization
      </para>
     </formalpara>
    </listitem>
   </itemizedlist>
   <para>
    For more information about acronyms, see the virtualization
    documentation provided at
    <link xlink:href="https://documentation.suse.com/sles/12-SP2/"/>.
   </para>
  </section>
  <section xml:id="TechInfo-Filesystems" remap="TechInfo:Filesystems">
   <title>File Systems</title>

   <section role="notoc" xml:id="fate-322002" remap="TechInfo:Filesystems">


    <title>Btrfs File System Going Read-only When Executing Balance Operation</title>
    <para>
     <emphasis>When executing a balance operation on a Btrfs file system on
     which there is almost no free space available, the file system may go
     into a forced read-only mode.</emphasis>
    </para>
    <para>
     <emphasis>Balancing a Btrfs file system involves relocating extents.
     When there is not enough free space available, this fails. The error is
     unconditionally overwritten and success is returned, but the extent has
     not actually been relocated. There is no data loss at this point, but
     balancing will fail on every invocation.</emphasis>
    </para>
    <para>
     This has been fixed via a maintenance update. If you are seeing this
     issue, make sure your system is up-to-date.
    </para>
    <para>
     For more information, see
     <link xlink:href="https://support.scc.suse.com/s/kb/BTRFS-filesystem-going-readonly-on-balance-operation-1583239427606">https://support.scc.suse.com/s/kb/BTRFS-filesystem-going-readonly-on-balance-operation-1583239427606</link>.
    </para>
   </section>

   <section xml:id="TechInfo-Filesystems-Comparison">
    <title>Comparison of Supported File Systems</title>
    <para>
     SUSE Linux Enterprise was the first enterprise Linux distribution to
     support journaling file systems and logical volume managers back in
     2000. Later, we introduced XFS to Linux, which today is seen as the
     primary work horse for large-scale file systems, systems with heavy
     load and multiple parallel reading and writing operations. With SUSE
     Linux Enterprise 12, we went the next step of innovation and started
     using the copy-on-write file system Btrfs as the default for the
     operating system, to support system snapshots and rollback.
    </para>
    <simplelist type="vert">
      <member><emphasis role="bold">+</emphasis> supported</member>
      <member><emphasis role="bold">–</emphasis> unsupported</member>
    </simplelist>
    <informaltable frame="all">
     <tgroup cols="6">
      <colspec colnum="1" colname="c1" colwidth="28*"/>
      <colspec colnum="2" colname="c2" colwidth="18*"/>
      <colspec colnum="3" colname="c3" colwidth="18*"/>
      <colspec colnum="4" colname="c4" colwidth="18*"/>
      <colspec colnum="5" colname="c5" colwidth="18*"/>
      <colspec colnum="6" colname="c6" colwidth="18*"/>
      <thead>
       <row>
        <entry>Feature</entry>
        <entry>Btrfs</entry>
        <entry>XFS</entry>
        <entry>Ext4</entry>
        <entry>ReiserFS **</entry>
        <entry>OCFS 2 ***</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          Data/metadata journaling
         </para>
        </entry>
        <entry>
         <para>
          N/A *
         </para>
        </entry>
        <entry>
         <para>
          – / +
         </para>
        </entry>
        <entry/>
        <entry>
         <para>
          – / +
         </para>
        </entry>
        <entry>
         <para>
          – / +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Journal internal/external
         </para>
        </entry>
        <entry>
         <para>
          N/A *
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry/>
	<entry/>
       </row>
       <row>
        <entry>
         <para>
          Offline extend/shrink
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          – / –
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry/>
       </row>
       <row>
        <entry>
         <para>
          Online extend/shrink
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Inode allocation map
         </para>
        </entry>
        <entry>
         <para>
          B-tree
         </para>
        </entry>
        <entry>
         <para>
          B+-tree
         </para>
        </entry>
        <entry>
         <para>
          table
         </para>
        </entry>
        <entry>
         <para>
          u. B*-tree
         </para>
        </entry>
        <entry>
         <para>
          table
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Sparse files
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry/>
	<entry/>
	<entry/>
	<entry/>
       </row>
       <row>
        <entry>
         <para>
          Tail packing
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry/>
       </row>
       <row>
        <entry>
         <para>
          Defrag
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry/>
	<entry/>
	<entry/>
       </row>
       <row>
        <entry>
         <para>
          ExtAttr/ACLs
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry/>
	<entry/>
	<entry/>
	<entry/>
       </row>
       <row>
        <entry>
         <para>
          Quotas
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry/>
	<entry/>
	<entry/>
	<entry/>
       </row>
       <row>
        <entry>
         <para>
          Dump/restore
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry/>
	<entry/>
       </row>
       <row>
        <entry>
         <para>
          Block size default
         </para>
        </entry>
        <entry namest="c2" nameend="c6">
         <para>
          4 KiB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Maximum file system size
         </para>
        </entry>
        <entry>
         <para>
          16 EiB
         </para>
        </entry>
        <entry>
         <para>
          8 EiB
         </para>
        </entry>
        <entry>
         <para>
          1 EiB
         </para>
        </entry>
        <entry>
         <para>
          16 TiB
         </para>
        </entry>
        <entry>
         <para>
          4 PiB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Maximum file size
         </para>
        </entry>
        <entry>
         <para>
          16 EiB
         </para>
        </entry>
        <entry>
         <para>
          8 EiB
         </para>
        </entry>
        <entry>
         <para>
          1 EiB
         </para>
        </entry>
        <entry>
         <para>
          1 EiB
         </para>
        </entry>
        <entry>
         <para>
          4 PiB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Support in products
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE HA
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis role="bold">*</emphasis> Btrfs is a copy-on-write file
       system. Rather than journaling changes before writing them in-place,
       it writes them to a new location and then links the new location in.
       Until the last write, the new changes are not
       <quote>committed</quote>. Due to the nature of the file system,
       quotas are implemented based on subvolumes
       (<literal>qgroups</literal>).
      </para>
      <para>
       The block size default varies with different host architectures. 64
       KiB is used on POWER, 4 KiB on most other systems. The actual size
       used can be checked with the command <command>getconf</command> <replaceable>PAGE_SIZE</replaceable>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">**</emphasis> ReiserFS is supported for
       existing file systems. The creation of new ReiserFS file systems is
       discouraged.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">***</emphasis> OCFS2 is fully supported as part
       of the SUSE Linux Enterprise High Availability Extension.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The maximum file size above can be larger than the file system's actual
     size due to usage of sparse blocks. Note that unless a file system
     comes with large file support (LFS), the maximum file size on a 32-bit
     system is 2 GB (2^31 bytes). Currently all of our standard file systems
     (including Ext3 and ReiserFS) have LFS, which gives a maximum file size
     of 2^63 bytes in theory. The numbers in the above tables assume that
     the file systems are using 4 KiB block size. When using different block
     sizes, the results are different, but 4 KiB reflects the most common
     standard.
    </para>
    <para>
     In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB = 1
     GiB; 1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB. See also
     <link xlink:href="https://physics.nist.gov/cuu/Units/binary.html"/>.
    </para>
    <para>

     NFSv4 with IPv6 is only supported for the client side. An NFSv4 server
     with IPv6 is not supported.
    </para>
    <para>

     The version of Samba shipped with SUSE Linux Enterprise Server 12 SP2
     delivers integration with Windows 7 Active Directory domains. In
     addition, we provide the clustered version of Samba as part of SUSE
     Linux Enterprise High Availability Extension 12 SP2.
    </para>
   </section>
   <section xml:id="TechInfo-Filesystems-Btrfs">
    <title>Supported Btrfs Features</title>
    <para>
     The following table lists supported and unsupported Btrfs features
     across multiple SLES versions.
    </para>
    <simplelist type="vert">
      <member><emphasis role="bold">+</emphasis> supported</member>
      <member><emphasis role="bold">–</emphasis> unsupported</member>
    </simplelist>
    <informaltable>
     <tgroup cols="5">
      <colspec colnum="1" colname="feature" colwidth="32*"/>
      <colspec colnum="2" colname="11SP4" colwidth="17*"/>
      <colspec colnum="3" colname="12GA" colwidth="17*"/>
      <colspec colnum="4" colname="12SP1" colwidth="17*"/>
      <colspec colnum="5" colname="12SP2" colwidth="17*"/>
      <thead>
       <row>

        <entry>Feature</entry>
        <entry>SLES 11 SP4</entry>
        <entry>SLES 12 GA</entry>
        <entry>SLES 12 SP1</entry>
        <entry>SLES 12 SP2</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Copy on Write</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Snapshots/Subvolumes</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Metadata Integrity</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Data Integrity</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Online Metadata Scrubbing</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Automatic Defragmentation</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Manual Defragmentation</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>In-band Deduplication</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Out-of-band Deduplication</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Quota Groups</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Metadata Duplication</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Multiple Devices</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 0</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 1</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 10</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 5</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>RAID 6</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Hot Add/Remove</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Device Replace</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Seeding Devices</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Compression</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Big Metadata Blocks</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Skinny Metadata</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Send Without File Data</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Send/Receive</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Inode Cache</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Fallocate with Hole Punch</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>+</entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </section>
  </section>
  <section xml:id="TechInfo-Java">
   <title>Supported Java Versions</title>
   <para>
    The following table lists Java implementations available in SUSE Linux
    Enterprise Server 12 SP2:
   </para>
   <informaltable>
    <tgroup cols="4">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <colspec colnum="3" colname="c3"/>
     <colspec colnum="4" colname="c4"/>
     <thead>
      <row>
       <entry>Name (Package Name)</entry>
       <entry>Version</entry>
       <entry>Part of SUSE Linux Enterprise Server</entry>
       <entry>Support</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>OpenJDK (<package>java-1_8_0-openjdk</package>)</entry>
       <entry>1.8.0</entry>
       <entry>SLES</entry>
       <entry>SUSE, L3</entry>
      </row>
      <row>
       <entry>OpenJDK (<package>java-1_7_0-openjdk</package>)</entry>
       <entry>1.7.0</entry>
       <entry>SLES</entry>
       <entry>SUSE, L3</entry>
      </row>
      <row>
       <entry>IBM Java (<package>java-1_8_0-ibm</package>)</entry>
       <entry>1.8.0</entry>
       <entry>SLES</entry>
       <entry>External only</entry>
      </row>
      <row>
       <entry>IBM Java (<package>java-1_7_1-ibm</package>)</entry>
       <entry>1.7.1</entry>
       <entry>SLES</entry>
       <entry>External only</entry>
      </row>
      <row>
       <entry>IBM Java (<package>java-1_6_0-ibm</package>)</entry>
       <entry>1.6.0</entry>
       <entry>Legacy Module</entry>
       <entry>External only</entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
 </section>


 <section xml:id="Legal">
  <title>Legal Notices</title>
  <para>
   SUSE makes no representations or warranties with respect to the contents
   or use of this documentation, and specifically disclaims any express or
   implied warranties of merchantability or fitness for any particular
   purpose. Further, SUSE reserves the right to revise this publication and
   to make changes to its content, at any time, without the obligation to
   notify any person or entity of such revisions or changes.
  </para>
  <para>
   Further, SUSE makes no representations or warranties with respect to any
   software, and specifically disclaims any express or implied warranties of
   merchantability or fitness for any particular purpose. Further, SUSE
   reserves the right to make changes to any and all parts of SUSE software,
   at any time, without any obligation to notify any person or entity of
   such changes.
  </para>
  <para>
   Any products or technical information provided under this Agreement may
   be subject to U.S. export controls and the trade laws of other countries.
   You agree to comply with all export control regulations and to obtain any
   required licenses or classifications to export, re-export, or import
   deliverables. You agree not to export or re-export to entities on the
   current U.S. export exclusion lists or to any embargoed or terrorist
   countries as specified in U.S. export laws. You agree to not use
   deliverables for prohibited nuclear, missile, or chemical/biological
   weaponry end uses. Refer to
   <link xlink:href="https://www.suse.com/company/legal/"/> for more information on
   exporting SUSE software. SUSE assumes no responsibility for your failure
   to obtain any necessary export approvals.
  </para>
  <para>
   Copyright © 2010-
<?dbtimestamp format="Y" ?>
   SUSE LLC. This release notes document is licensed under a Creative
   Commons Attribution-NoDerivs 3.0 United States License (CC-BY-ND-3.0 US,
   <link xlink:href="https://creativecommons.org/licenses/by-nd/3.0/us/"/>).
  </para>
  <para>
   SUSE has intellectual property rights relating to technology embodied in
   the product that is described in this document. In particular, and
   without limitation, these intellectual property rights may include one or
   more of the U.S. patents listed at
   <link xlink:href="https://www.suse.com/company/legal/"/> and one or more
   additional patents or pending patent applications in the U.S. and other
   countries.
  </para>
  <para>
   For SUSE trademarks, see SUSE Trademark and Service Mark list
   (<link xlink:href="https://www.suse.com/company/legal/"/>). All third-party
   trademarks are the property of their respective owners.
  </para>
 </section>
 <section>
  <title>Colophon</title>
  <para>
   Thanks for using SUSE Linux Enterprise Server in your business.
  </para>
  <para>
   The SUSE Linux Enterprise Server Team.
  </para>
 </section>
</article>
