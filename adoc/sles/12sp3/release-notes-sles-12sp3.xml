<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
    type="text/xml"
    title="Profiling step"
?>
<!DOCTYPE article
[

]>


<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en" xml:id="rnotes">
  <title>Release Notes</title>
  <info>
    <releaseinfo>12.3.20170706</releaseinfo>
    <productname><phrase os="sles">SUSE Linux Enterprise Server</phrase></productname>
    <productnumber>12 SP3</productnumber>
    <date>2017-03-12</date>
    <revhistory>
      <revision>
        <date>2017-03-12</date>
      </revision>
    </revhistory>
    <meta name="series">Release Notes</meta>
    <meta name="maintainer" content="lukas.kucharczyk@suse.com"/>
    <meta name="productname">
      <productname version="12 SP3">SUSE Linux Enterprise Server</productname>
    </meta>
    <meta name="description">Latest information about new and deprecated features, improvements, and known-issues</meta>
    <meta name="social-descr">New and deprecated features, improvements, known-issues</meta>

    <abstract>
      <para>
        This document provides guidance and an overview to high level general
        features and updates for SUSE Linux Enterprise Server 12 SP3. Besides
        architecture or product-specific information, it also describes the
        capabilities and limitations of SUSE Linux Enterprise Server 12 SP3.
      </para>

      <para>
        General documentation can be found at:
        <link xlink:href="https://documentation.suse.com/sles/12-SP3/"/>.
      </para>
    </abstract>
  </info>


 <section xml:id="rn-about">
  <title>About the Release Notes</title>
  <para>
   These Release Notes are identical across all architectures, and the most
   recent version is always available online at
   <link xlink:href="https://documentation.suse.com/releasenotes/"/>.
  </para>


  <para>
   Some entries may be listed twice, if they are important and belong to
   more than one section.
  </para>
  <para>
   Release notes usually only list changes that happened between two
   subsequent releases. Certain important entries from the release notes
   documents of previous product versions are repeated. To make these
   entries easier to identify, they contain a note to that effect.
  </para>
  <para>
   However, repeated entries are provided as a courtesy only. Therefore, if
   you are skipping one or more service packs, check the release notes of
   the skipped service packs as well. If you are only reading the release
   notes of the current release, you could miss important changes.
  </para>
 </section>
 <section xml:id="Intro">
  <title>SUSE Linux Enterprise Server</title>
  <remark> SLE definition: general information </remark>



  <para>
   SUSE Linux Enterprise Server is a highly reliable, scalable, and secure
   server operating system, built to power mission-critical workloads in
   both physical and virtual environments. It is an affordable,
   interoperable, and manageable open source foundation. With it,
   enterprises can cost-effectively deliver core business services, enable
   secure networks, and simplify the management of their heterogeneous IT
   infrastructure, maximizing efficiency and value.
  </para>
  <para>
   The only enterprise Linux recommended by Microsoft and SAP, SUSE Linux
   Enterprise Server is optimized to deliver high-performance
   mission-critical services, as well as edge of network, and web
   infrastructure workloads.
  </para>
  <section xml:id="Intro-Interoperability">
   <title>Interoperability and Hardware Support</title>
   <para>
    Designed for interoperability, SUSE Linux Enterprise Server integrates
    into classical Unix as well as Windows environments, supports open
    standard interfaces for systems management, and has been certified for
    IPv6 compatibility.
   </para>
   <para>
    This modular, general purpose operating system runs on four processor
    architectures and is available with optional extensions that provide
    advanced capabilities for tasks such as real time computing and high
    availability clustering.
   </para>
   <para>
    SUSE Linux Enterprise Server is optimized to run as a high performing
    guest on leading hypervisors and supports an unlimited number of virtual
    machines per physical system with a single subscription, making it the
    perfect guest operating system for virtual computing.
   </para>
  </section>
  <section xml:id="Intro-Lifecycle">
   <title>Support and Life Cycle</title>
   <para>
    SUSE Linux Enterprise Server is backed by award-winning support from
    SUSE, an established technology leader with a proven history of
    delivering enterprise-quality support services.
   </para>

   <para>
    SUSE Linux Enterprise Server 12 has a 13-year life cycle, with 10 years
    of General Support and 3 years of Extended Support. The current version
    (SP3) will be fully maintained and supported until 6 months after the
    release of SUSE Linux Enterprise Server 12 SP4.
   </para>
   <para>
    If you need additional time to design, validate and test your upgrade
    plans, Long Term Service Pack Support can extend the support you get an
    additional 12 to 36 months in twelve month increments, providing a total
    of 3 to 5 years of support on any given service pack.
   </para>
   <para>
    For more information, check our Support Policy page
    <link xlink:href="https://www.suse.com/support/policy-products/"/> or the Long Term
    Service Pack Support Page
    <link xlink:href="https://www.suse.com/products/long-term-service-pack-support/"/>.
   </para>
  </section>
  <section xml:id="Intro-New">
   <title>What Is New?</title>
   <remark> Marketing POV. </remark>
   <remark> ** Update information : link to severity ratings documentation ** Installation : change,
    new kernel, compatibility ** features : features description (not technical) ** System update :
    link to the documentation (not just the tittle) ** Update related notes : specific change to
    focus on.
    BNC#898123
   </remark>
   <para>
    SUSE Linux Enterprise Server 12 introduces many innovative changes
    compared to SUSE Linux Enterprise Server 11. Here are some of the
    highlights:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Robustness on administrative errors and improved management
      capabilities with full system rollback based on Btrfs as the default
      file system for the operating system partition and the Snapper
      technology of SUSE.
     </para>
    </listitem>
    <listitem>
     <para>
      An overhaul of the installer introduces a new workflow that allows you
      to register your system and receive all available maintenance updates
      as part of the installation.
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Linux Enterprise Server Modules offer a choice of supplemental
      packages, ranging from tools for Web Development and Scripting,
      through a Cloud Management module, all the way to a sneak preview of
      upcoming management tooling called Advanced Systems Management.
      Modules are part of your SUSE Linux Enterprise Server subscription,
      are technically delivered as online repositories, and differ from the
      base of SUSE Linux Enterprise Server only by their life cycle. For
      more information about modules, see <xref linkend="Intro-Module"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      New core technologies like systemd (replacing the time-honored System
      V-based init process) and Wicked (introducing a modern, dynamic
      network configuration infrastructure).
     </para>
    </listitem>
    <listitem>
     <para>
      The open-source database system MariaDB is fully supported now.
     </para>
    </listitem>
    <listitem>
     <para>
      Support for open-vm-tools together with VMware for better integration
      into VMware-based hypervisor environments.
     </para>
    </listitem>
    <listitem>
     <para>
      Linux Containers are integrated into the virtualization management
      infrastructure (libvirt). Docker is provided as a fully supported
      technology.
     </para>
    </listitem>
    <listitem>
     <para>
      Support for the AArch64 architecture (64-bit ARMv8) and the 64-bit
      Little-Endian variant of the IBM POWER architecture. Additionally, we
      continue to support the Intel 64/AMD64 and IBM Z architectures.
     </para>
    </listitem>
    <listitem>
     <para>
      GNOME 3.20 gives users a modern desktop environment with a choice of
      several different look and feel options, including a special
      <emphasis>SUSE Linux Enterprise Classic</emphasis> mode for easier
      migration from earlier SUSE Linux Enterprise Desktop environments.
     </para>
    </listitem>
    <listitem>
     <para>
      For users wishing to use the full range of productivity applications
      of a Desktop with their SUSE Linux Enterprise Server, we are now
      offering SUSE Linux Enterprise Workstation Extension (requires a SUSE
      Linux Enterprise Desktop subscription).
     </para>
    </listitem>
    <listitem>
     <para>
      Integration with the new SUSE Customer Center, the new central web
      portal from SUSE to manage Subscriptions, Entitlements, and provide
      access to Support.
     </para>
    </listitem>
   </itemizedlist>

   <remark> This is Read Me First information. </remark>
   <para>
    If you are upgrading from a previous SUSE Linux Enterprise Server
    release, you should review at least the following sections:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="Intro-Support"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="InstUpgrade-Upgrade"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="TechInfo"/>
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section xml:id="Intro-Documentation">
   <title>Documentation and Other Information</title>
   <para/>
   <section>
    <title>Available on the Product Media</title>
    <itemizedlist>
     <listitem>
      <para>
       Read the READMEs on the media.
      </para>
     </listitem>
     <listitem>
      <para>
       Get the detailed change log information about a particular package
       from the RPM (where <filename>&lt;FILENAME&gt;.rpm</filename> is the
       name of the RPM):
      </para>
<screen>rpm --changelog -qp &lt;FILENAME&gt;.rpm</screen>
     </listitem>
     <listitem>
      <para>
       Check the <filename>ChangeLog</filename> file in the top level of the
       media for a chronological log of all changes made to the updated
       packages.
      </para>
     </listitem>
     <listitem>
      <para>
       Find more information in the <filename>docu</filename> directory of
       the media of SUSE Linux Enterprise Server 12 SP3. This directory
       includes PDF versions of the SUSE Linux Enterprise Server 12 SP3
       Installation Quick Start and Deployment Guides. Documentation (if
       installed) is available below the
       <filename>/usr/share/doc/</filename> directory of an installed
       system.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section>
    <title>Externally Provided Documentation</title>
    <itemizedlist>
     <listitem>
      <para>
       <link xlink:href="https://documentation.suse.com/sles/12-SP3/"/> contains
       additional or updated documentation for SUSE Linux Enterprise
       Server 12 SP3.
      </para>
     </listitem>
    </itemizedlist>
   </section>
  </section>
  <section xml:id="Intro-Sourcecode">
   <title>How to Obtain Source Code</title>


   <para>
    This SUSE product includes materials licensed to SUSE under the GNU
    General Public License (GPL). The GPL requires SUSE to provide the
    source code that corresponds to the GPL-licensed material. The source
    code is available for download at
    <link xlink:href="https://www.suse.com/source-code/"/>.
    Also, for up to three years after distribution of the SUSE product, upon
    request, SUSE will mail a copy of the source code. Requests should be
    sent by e-mail to <link xlink:href="mailto:sle_source_request@suse.com"/> or
    as otherwise instructed at
    <link xlink:href="https://www.suse.com/source-code/"/>. SUSE
    may charge a reasonable fee to recover distribution costs.
   </para>
  </section>
  <section xml:id="Intro-Support">
   <title>Support Statement for SUSE Linux Enterprise Server</title>
   <para>
    To receive support, you need an appropriate subscription with SUSE. For
    more information, see
    <link xlink:href="https://www.suse.com/support/"/>.
   </para>
   <para>
    The following definitions apply:
   </para>
   <variablelist>
    <varlistentry>
     <term>L1</term>
     <listitem>
      <para>
       Problem determination, which means technical support designed to
       provide compatibility information, usage support, ongoing
       maintenance, information gathering and basic troubleshooting using
       available documentation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>L2</term>
     <listitem>
      <para>
       Problem isolation, which means technical support designed to analyze
       data, reproduce customer problems, isolate problem area and provide a
       resolution for problems not resolved by Level 1 or alternatively
       prepare for Level 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>L3</term>
     <listitem>
      <para>
       Problem resolution, which means technical support designed to resolve
       problems by engaging engineering to resolve product defects which
       have been identified by Level 2 Support.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For contracted customers and partners, SUSE Linux Enterprise Server 12
    SP3 and its Modules are delivered with L3 support for all packages,
    except the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Technology Previews, see <xref linkend="Intro-Support-Techpreviews"/>
     </para>
    </listitem>
    <listitem>
     <para>
      sound, graphics, fonts and artwork
     </para>
    </listitem>
    <listitem>
     <para>
      packages that require an additional customer contract, see
      <xref linkend="Intro-Support-External"/>
     </para>
    </listitem>
    <listitem>
     <para>
      packages provided as part of the Software Development Kit (SDK)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    SUSE will only support the usage of original (that is, unchanged and
    un-recompiled) packages.
   </para>
  </section>
  <section xml:id="Intro-Support-General">
   <title>General Support</title>
   <para>
    To learn about supported kernel, virtualization, and file system
    features, as well as supported Java versions, see
    <xref linkend="TechInfo"/>.
   </para>
  </section>

  <section xml:id="Intro-Support-External" remap="Intro:Support:External">
   <title>Software Requiring Specific Contracts</title>




   <para>
    The following packages require additional support contracts to be
    obtained by the customer in order to receive full support:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      PostgreSQL Database
     </para>
    </listitem>
    <listitem>
     <para>
      LibreOffice
     </para>
    </listitem>
   </itemizedlist>

  </section>
  <section xml:id="Intro-Support-Techpreviews-Overview">
   <title>Technology Previews</title>
   <remark> ** clear statement of TP (no support) ** Title of the TP ** full description of the TP
    ** link to official documentation (external) ** link to SUSE's documentation </remark>

   <para>
    Technology previews are packages, stacks, or features delivered by SUSE
    which are not supported. They may be functionally incomplete, unstable
    or in other ways not suitable for production use. They are included for
    your convenience and give you a chance to test new technologies within
    an enterprise environment.
   </para>
   <para>
    Whether a technology preview becomes a fully supported technology later
    depends on customer and market feedback. Technology previews can be
    dropped at any time and SUSE does not commit to providing a supported
    version of such technologies in the future.
   </para>
   <para>
    Give your SUSE representative feedback, including your experience and
    use case.
   </para>
   <section xml:id="Intro-Support-Techpreviews" remap="Intro:Support:Techpreviews">
    <title>Technology Previews for All Architectures</title>
    <para/>

    <section role="notoc" xml:id="fate-321778" remap="Intro:Support:Techpreviews">


     <title>Support for KVM Guests Using NVDIMM Devices</title>
     <para>
      As a technology preview, KVM guests can now use NVDIMM devices.
     </para>
    </section>
    <section role="notoc" xml:id="fate-319684" remap="Intro:Support:Techpreviews">


     <title>QEMU: NVDIMM and Persistent Memory</title>
     <para>
      As a technical preview, QEMU now supports NVDIMM. To use NVDIMM,
      create a memory device with <literal>model=nvdimm</literal>. This
      functionality can be used directly with the <literal>qemu</literal>
      command line tool or using <literal>libvirt</literal>. However, this
      functionality is not yet exposed through
      <literal>virt-manager</literal>.
     </para>
     <para>
      NVDIMM supports two access modes:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        PMEM: NVDIMM is mapped into the CPU's address space, so that the CPU
        can directly access it like normal memory
       </para>
      </listitem>
      <listitem>
       <para>
        BLK: NVDIMM is used as a block device, this avoids occupying the CPU
        address space.
       </para>
      </listitem>
     </itemizedlist>
    </section>
    <section role="notoc" xml:id="fate-316354" remap="Intro:Support:Techpreviews">


     <title>KVM Nested Virtualization</title>
     <para>
      KVM Nested Virtualization is available in SLE 12 as a technology
      preview.
     </para>
    </section>

   </section>
   <section xml:id="Intro-Support-Techpreviews-SystemZ" remap="Intro:Support:Techpreviews:SystemZ">
    <title>Technology Previews for IBM Z (s390x)</title>
    <para/>

    <section role="notoc" xml:id="fate-322375" remap="Intro:Support:Techpreviews:SystemZ">


     <title>Exploitation of Shared Memory Communications</title>
     <para>
      As a technology preview, SLES 12 SP3 enables communication through
      shared memory segments with the 10 GB Ethernet RoCE card:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Support for the networking card itself is included in the kernel.
       </para>
      </listitem>
      <listitem>
       <para>
        The package <literal>smc-tools</literal> contains additional
        user-space tools.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      This technology should only be used in a trusted network
      infrastructure.
     </para>
    </section>

   </section>
   <section xml:id="Intro-Support-Techpreviews-Power" remap="Intro:Support:Techpreviews:Power">
    <title>Technology Previews for POWER (ppc64le)</title>
    <para/>

    <section role="notoc" xml:id="fate-321734" remap="Intro:Support:Techpreviews:Power">


     <title>Support for KVM</title>
     <para>
      With SLES 12 SP3, KVM is now available as a technology preview on
      OpenPower S822LC systems running OPAL firmware.
     </para>
    </section>
    <section role="notoc" xml:id="fate-321601" remap="Intro:Support:Techpreviews:Power">


     <title>Inclusion of IBM TPM 2.0 Stack</title>
     <para>
      <emphasis>IBM has developed a TPM 2.0 TSS stack that can exist and be
      used in parallel to the Intel TPM 2.0 stack.</emphasis>
     </para>
     <para>
      <emphasis>It is not clear at this time which of them will be the
      preferable solution on all TPM supporting platforms.</emphasis>
     </para>
     <para>
      <emphasis>The general guideline of SUSE Linux Enterprise is having one
      preferred tool to do the job.</emphasis>
     </para>
     <para>
      The IBM TPM 2.0 stack is shipped as a Technology Preview in addition
      to the supported Intel TPM 2.0 stack.
     </para>
    </section>

   </section>
  </section>
  <section xml:id="Intro-ModuleExtensionRelated">
   <title>Modules, Extensions, and Related Products</title>
   <para>
    This section comprises information about modules and extensions for SUSE
    Linux Enterprise Server 12 SP3. Modules and extensions add parts or
    functionality to the system.
   </para>
   <section xml:id="Intro-Module">
    <title>Available Modules</title>
    <para>
     Modules are fully supported parts of SUSE Linux Enterprise Server with
     a different life cycle and update timeline. They are a set of packages,
     have a clearly defined scope and are delivered via an online channel
     only. Release notes for modules are contained in this document, see
     <xref linkend="Packages-Modules"/>.
    </para>
    <para>
     The following modules are available for SUSE Linux Enterprise Server 12
     SP3:
    </para>
    <informaltable>
     <tgroup cols="3">
      <colspec colnum="1" colname="name" colwidth="40*"/>
      <colspec colnum="2" colname="content" colwidth="50*"/>
      <colspec colnum="3" colname="cycle" colwidth="25*"/>
      <thead>
       <row>
        <entry>Name</entry>
        <entry>Content</entry>
        <entry>Life Cycle</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Advanced Systems Management Module</entry>
        <entry>CFEngine, Puppet, Salt and the Machinery tool</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>Containers Module</entry>
        <entry>Docker, tools, prepackaged images</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>HPC Module</entry>
        <entry>Tools and libraries related to High Performance Computing (HPC)</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>Legacy Module<emphasis role="bold">*</emphasis>
        </entry>
        <entry>Sendmail, old IMAP stack, old Java, …</entry>
        <entry>Until September/October 2017 (except for <literal>ksh</literal>)</entry>
       </row>
       <row>
        <entry>Public Cloud Module</entry>
        <entry>Public cloud initialization code and tools</entry>
        <entry>Frequent releases</entry>
       </row>
       <row>
        <entry>Toolchain Module</entry>
        <entry>GNU Compiler Collection (GCC)</entry>
        <entry>Yearly delivery</entry>
       </row>
       <row>
        <entry>Web and Scripting Module</entry>
        <entry>PHP, Python, Ruby on Rails</entry>
        <entry>3 years, ~18 months overlap</entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
    <para>
     <emphasis role="bold">*</emphasis> Module is not available for the
     AArch64 architecture.
    </para>
    <para>
     For more information about the life cycle of packages contained in
     modules, see
     <link xlink:href="https://scc.suse.com/docs/lifecycle/sle/12/modules"/>.
    </para>
   </section>
   <section xml:id="Intro-Extension">
    <title>Available Extensions</title>
    <para>
     Extensions add extra functionality to the system and require their own
     registration key, usually at additional cost. Extensions are delivered
     via an online channel or physical media. In many cases, extensions have
     their own release notes documents that are available from
     <link xlink:href="https://documentation.suse.com/releasenotes/"/>.
    </para>
    <para>
     The following extensions are available for SUSE Linux Enterprise Server
     12 SP3:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SUSE Linux Enterprise Live Patching:
       <link xlink:href="https://www.suse.com/products/live-patching/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise High Availability Extension:
       <link xlink:href="https://www.suse.com/products/highavailability/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       Geo Clustering for SUSE Linux Enterprise High Availability Extension:
       <link xlink:href="https://www.suse.com/products/highavailability/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Real Time:
       <link xlink:href="https://www.suse.com/products/realtime/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Workstation Extension
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Additionally, there are the following extensions which are not covered
     by SUSE support agreements, available at no additional cost and without
     an extra registration key:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SUSE Package Hub: <link xlink:href="https://packagehub.suse.com/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Software Development Kit
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section xml:id="Intro-Derived-Products">
    <title>Derived and Related Products</title>
    <remark> Marketing POV. </remark>
    <para>
     This sections lists derived and related products. In many cases, these
     products have their own release notes documents that are available from
     <link xlink:href="https://documentation.suse.com/releasenotes/"/>.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SUSE Enterprise Storage
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Desktop:
       <link xlink:href="https://www.suse.com/products/desktop/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Linux Enterprise Server for SAP Applications:
       <link xlink:href="https://www.suse.com/products/sles-for-sap/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE Manager:
       <link xlink:href="https://www.suse.com/products/multi-linux-manager/"/>
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE OpenStack Cloud
      </para>
     </listitem>
    </itemizedlist>
   </section>
  </section>
  <section xml:id="Intro-Certification">
   <title>Security, Standards, and Certification</title>
   <para>
    SUSE Linux Enterprise Server 12 SP3 has been submitted to the
    certification bodies for:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.commoncriteriaportal.org/">Common Criteria
      Certification</link>
     </para>
    </listitem>
    <listitem>
     <para>
      FIPS 140-2 validation, see:
      <link xlink:href="https://csrc.nist.gov/projects/cryptographic-module-validation-program/modules-in-process/modules-in-process-list"/>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information about certification, see
    <link xlink:href="https://www.suse.com/support/security/certifications/"/>.
   </para>
  </section>
 </section>



 <section xml:id="InstUpgrade">
  <title>Installation and Upgrade</title>
  <para>
   SUSE Linux Enterprise Server can be deployed in several ways:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Physical machine
    </para>
   </listitem>
   <listitem>
    <para>
     Virtual host
    </para>
   </listitem>
   <listitem>
    <para>
     Virtual machine
    </para>
   </listitem>
   <listitem>
    <para>
     System containers
    </para>
   </listitem>
   <listitem>
    <para>
     Application containers
    </para>
   </listitem>
  </itemizedlist>
  <section xml:id="InstUpgrade-Installation" remap="InstUpgrade:Installation">
   <title>Installation</title>
   <para>
    This section includes information related to the initial installation of
    SUSE Linux Enterprise Server 12 SP3. For information about installing,
    see <citetitle>Deployment Guide</citetitle> at
    <link xlink:href="https://documentation.suse.com/sles/12-SP3/html/SLES-all/book-sle-deployment.html"/>.
   </para>

   <section role="notoc" xml:id="fate-323796" remap="InstUpgrade:Installation">


    <title>FCoE Storage Does Not Work with Cavium or QLogic Storage Controllers with FCoE Offload</title>
    <para>
     <emphasis>On a default installation of SLES 12 SP3, there is no support
     for FCoE storage on systems that use Cavium or QLogic storage
     controllers with support for FCoE offload.</emphasis>
    </para>
    <para>
     SUSE has created a kISO (Kernel Update ISO) which can be downloaded
     from
     <link xlink:href="https://drivers.suse.com/suse/installer-update/sle-12-sp3-x86_64/1.0/install-readme.html"/>.
    </para>
    <para>
     For more information about kISOs in general, see
     <link xlink:href="https://www.suse.com/communities/blog/kiso-kernel-update-iso/"/>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-322275" remap="InstUpgrade:Installation">


    <title>Installing Systems from Online Repositories</title>
    <para>
     <emphasis>To install SLES, you need the installation media. If you also
     mirror the repositories, for example with SMT, this means that
     effectively you need to download all packages twice: once as a part of
     the media and additionally from the online repository.</emphasis>
    </para>
    <para>
     For such scenarios, we provide packages named
     <literal>tftpboot-installation-*</literal> in the product repositories.
     These packages include an installer prepared for a network boot
     environment (PXE).
    </para>
    <para>
     To use them, configure the PXE environment (DHCP, TFTP servers) and
     install the package for the respective product and architecture. Make
     sure to adjust the included configuration, so that the correct local
     URLs are passed to the installer.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321358" remap="InstUpgrade:Installation">


    <title>Network Interfaces Configured via linuxrc Take Precendence</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis> For some configurations with many network interfaces, it can
     take several hours until all network interfaces are initialized (see
     </emphasis><link xlink:href="https://bugzilla.suse.com/show_bug.cgi?id=988157"><emphasis>https://bugzilla.suse.com/show_bug.cgi?id=988157</emphasis></link><emphasis>). In such cases, the installation is blocked. SLE 12 SP1 and earlier
     did not offer a workaround for this behavior. </emphasis>
    </para>
    <para>
     Starting with SLE 12 SP2, you can speed up interactive installations on
     systems with many network interfaces by configuring them via linuxrc.
     When a network interface is configured via linuxrc, YaST will not
     perform automatic DHCP configuration for any interface. Instead, YaST
     will continue to use the configuration from linuxrc.
    </para>
    <para>
     To configure a particular interface via linuxrc, add the following to
     the boot command line before starting the installation:
    </para>
<screen>ifcfg=eth0=dhcp</screen>
    <para>
     In the parameter, replace <literal>eth0</literal> with the name of the
     appropriate network interface. The <literal>ifcfg</literal> option can
     be used multiple times.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320416" remap="InstUpgrade:Installation">


    <title>Warning When Enabling Snapshots on Small Root File Systems</title>
    <para>
     <emphasis>Btrfs file system snapshots take up extra disk space.
     Previous versions of SLE did not check during installation whether a
     custom root file system size was appropriate for enabling
     snapshots.</emphasis>
    </para>
    <para>
     For Btrfs root file systems with snapshotting, the SLE installer now
     verifies that the size of the file system at least matches the value of
     <literal>root_base</literal> from the product's
     <literal>control.xml</literal>. For example, for a default SLES
     installation, the root file system size is 12 GB. If the file system is
     smaller, the installer will display a warning - which can be ignored.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319135" remap="InstUpgrade:Installation">


    <title>SMT: Upgrading Database Schema and Engine</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>SMT 12 comes with a new database schema and is standardized
     on the InnoDB database back-end.</emphasis>
    </para>
    <para>
     <emphasis> In order to upgrade SMT 11 SPx to SMT 12, it is necessary
     that SMT 11 is configured against SCC (SUSE Customer Center)
     </emphasis>before<emphasis> initializing the upgrade of SLES and SMT to
     version 12 SP1 or newer. If the host is upgraded to SLES 12 SP1 or
     newer without switching to SCC first, the installed SMT instance will
     no longer work. </emphasis>
    </para>
    <para>
     <emphasis>Only SMT 11 SP3 can be configured against SCC. Older versions
     need to be upgraded to version 11 SP3 first.</emphasis>
    </para>
    <para>
     Whether the schema or database engine must be upgraded is checked
     during package upgrade and displayed as an update notification. Back up
     your database before doing the database upgrade. Both the schema and
     database engine upgrade are done by the utility
     <literal>/usr/bin/smt-schema-upgrade</literal> (can be called directly
     or via <literal>systemctl start smt-schema-upgrade</literal>) or are
     done automatically after <literal>smt.target restart</literal>
     (computer reboot or via <literal>systemctl restart smt.target</literal>). However, manual database tuning is required for optimal performance.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319132" remap="InstUpgrade:Installation">


    <title>SMT Supports SCC Exclusively</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     Support for NCC (Novell Customer Center) was removed from SMT. SMT can
     still serve SLE 11 clients, but must be configured to receive updates
     from SCC.
    </para>
    <para>
     Before migrating from SMT 11 SP3, SMT must be reconfigured against SCC.
     Migration from older versions of SMT is not possible.
    </para>
   </section>
   <section role="notoc" xml:id="fate-317838" remap="InstUpgrade:Installation">


    <title>Installing with LVM2, Without a Separate /boot Partition</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     SUSE Linux Enterprise 12 and newer generally supports the installation
     with a linear LVM2 without a separate <literal>/boot</literal>
     partition, for example to use it with Btrfs as the root file system, to
     achieve full system snapshot and rollback.
    </para>
    <para>
     However, this setup is only supported under the following conditions:
    </para>
    <itemizedlist>
      <listitem>
       <para>
        Only linear LVM2 setups are supported.
       </para>
      </listitem>
      <listitem>
       <para>
        There must be enough space in the partitioning "label" (the
        partition table) for the grub2 bootloader first stage files. If the
        installation of the grub2 bootloader fails, you will have to create
        a new partition table. <emphasis role="bold">CAVEAT: Creating a new
        partition table destroys all data on the given disk!</emphasis>
       </para>
      </listitem>
    </itemizedlist>
    <para>
     For a migration from an existing SUSE Linux Enterprise 11 system with
     LVM2 to SUSE Linux Enterprise 12 or newer, the <literal>/boot</literal>
     partition must be preserved.
    </para>
   </section>

  </section>
  <section xml:id="InstUpgrade-Upgrade" remap="InstUpgrade:Upgrade">
   <title>Upgrade-Related Notes</title>
   <para>
    This section includes upgrade-related information for SUSE Linux
    Enterprise Server 12 SP3. For information about general preparations and
    supported upgrade methods and paths, see the documentation at
    <link xlink:href="https://documentation.suse.com/sles/12-SP3/html/SLES-all/cha-update-sle.html"/>.
   </para>

   <section role="notoc" xml:id="fate-326567" remap="InstUpgrade:Upgrade">


    <title>Product Registration Changes for HPC Customers</title>
    <para>
     <emphasis>For SUSE Linux Enterprise 12, there was a High Performance
     Computing subscription named "SUSE Linux Enterprise Server for HPC"
     (SLES for HPC). With SLE 15, this subscription does not exist anymore
     and has been replaced. The equivalent subscription is named "SUSE Linux
     Enterprise High Performance Computing" (SLE-HPC) and requires a
     different license key. Because of this requirement, a SLES for HPC 12
     system will by default upgrade to a regular "SUSE Linux Enterprise
     Server".</emphasis>
    </para>
    <para>
     To properly upgrade a SLES for HPC system to a SLE-HPC, the system
     needs to be converted to SLE-HPC first. SUSE provides a tool to
     simplify this conversion by performing the product conversion and
     switch to the SLE-HPC subscription. However, the tool does not perform
     the upgrade itself.
    </para>
    <para>
     When run without extra parameters, the script assumes that the SLES for
     HPC subscription is valid and not expired. If the subscription has
     expired, you need to provide a valid registration key for SLE-HPC.
    </para>
    <para>
     The script reads the current set of registered modules and extensions
     and after the system has been converted to SLE-HPC, it tries to add
     them again.
    </para>
    <important>
     <title>Providing a Registration Key to the Conversion Script</title>
     <para>
      The script cannot restore the previous registration state if the
      supplied registration key is incorrect or invalid.
     </para>
    </important>
    <orderedlist>
     <listitem>
      <para>
       To install the script, run <literal>zypper in
       switch_sles_sle-hpc</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Execute the script from the command line as <literal>root</literal>:
      </para>
<screen>switch_sles_sle-hpc -e &lt;REGISTRATION_EMAIL&gt; -r &lt;NEW_REGISTRATION_KEY&gt;</screen>
      <para>
       The parameters <literal>-e</literal> and <literal>-r</literal> are
       only required if the previous registration has expired, otherwise
       they are optional. To run the script in batch mode, add the option
       <literal>-y</literal>. It answers all questions with
       <emphasis>yes</emphasis>.
      </para>
     </listitem>
    </orderedlist>
    <para>
     For more information, see the man page
     <literal>switch_sles_sle-hpc(8)</literal> and
     <literal>README.SUSE</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-324497" remap="InstUpgrade:Upgrade">


    <title>FreeRADIUS Configuration Needs to Be Merged Manually</title>
    <para>
     When upgrading a SLES installation that includes
     <literal>freeradius-server</literal> and a non-standard
     <literal>/etc/raddb/radiusd.conf</literal> configuration file to SLES
     12 SP3, make sure to manually merge the new
     <literal>radiusd.conf</literal> configuration section into the custom
     configuration before running the FreeRADIUS server.
    </para>
    <para>
     In particular, pay attention to the parameter
     <literal>correct_escapes</literal>: The default behavior did not
     change, but the new default
     <literal>/etc/raddb/policy.d/filter</literal> only functions with the
     setting <literal>correct_escapes=true</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-323911" remap="InstUpgrade:Upgrade">


    <title>Error on Migration From SP2 to SP3 When HPC Module Is Selected</title>
    <para>
     <emphasis>When the High Performance Computing module is selected, the
     following error message may be encountered during Migration from SLES
     12 SP2 to SLES 12 SP3:</emphasis>
    </para>
<screen>Can't get available migrations from server: SUSE::Connect::ApiError: The requested products '' are not activated on the system.
'/usr/lib/zypper/commands/zypper-migration' exited with status 1</screen>
    <para>
     The problem can be resolved by re-registering the HPC module using the
     following two commands:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>rpm -e sle-module-hpc-release-POOL
       sle-module-hpc-release</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>SUSEConnect -p sle-module-hpc/12/x86_64</literal>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     These commands can also be performed before migration as a preventive
     measure.
    </para>
   </section>
   <section role="notoc" xml:id="fate-322037" remap="InstUpgrade:Upgrade">


    <title>Automatic Log Rotation Will Be Disabled After Upgrade</title>
    <para>
     <emphasis> If the package
     </emphasis><literal>logrotate</literal><emphasis> was installed or
     updated before
     </emphasis><literal>systemd-presets-branding-SLE</literal><emphasis>,
     automatic log rotation will be disabled after the upgrade to SLES 12
     SP3. </emphasis>
    </para>
    <para>
     Enable the <literal>logrotate</literal> systemd timer manually. To do
     so, run the following commands as root:
    </para>
    <orderedlist>
     <listitem>
      <para>
       <literal>systemctl enable logrotate.timer</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>systemctl restart logrotate.timer</literal>
      </para>
     </listitem>
    </orderedlist>
   </section>
   <section role="notoc" xml:id="fate-321493" remap="InstUpgrade:Upgrade">


    <title>Online Migration with Live Patching Enabled</title>
    <para>
     <emphasis>The SLES online migration process reports package conflicts
     when Live Patching is enabled and the kernel is being upgraded. This
     applies when crossing the boundary between two Service
     Packs.</emphasis>
    </para>
    <para>
     To prevent the conflicts, before starting the migration, execute the
     following as a super user:
    </para>
<screen>zypper rm $(rpm -qa kgraft-patch-*)</screen>
   </section>
   <section role="notoc" xml:id="fate-320534" remap="InstUpgrade:Upgrade">


    <title>Online Migration: Checking the Status of Registered Products</title>
    <para>
     <emphasis>It is common that during the lifecycle of a system
     installation, registered extensions and modules are removed from the
     system without also deactivating them on the registration
     server.</emphasis>
    </para>
    <para>
     <emphasis>To prevent errors and unexpected behavior during an online
     migration, the status of installed products needs to be checked before
     the migration to allow reinstalling or deactivating
     products.</emphasis>
    </para>
    <para>
     A new step has been added to the current online migration workflow, it
     will check the registered products that are not currently installed in
     the system and allows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Trying to install the products from the available repositories (<emphasis>Install</emphasis>).
      </para>
     </listitem>
     <listitem>
      <para>
       Deactivating the products in SCC (<emphasis>Deactivate</emphasis>).
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-319118" remap="InstUpgrade:Upgrade">


    <title>Updating Registration Status After Rollback</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>When performing a service pack migration, it is necessary to
     change the configuration on the registration server to provide access
     to the new repositories. If the migration process is interrupted or
     reverted (via restoring from a backup or snapshot), the information on
     the registration server is inconsistent with the status of the system.
     This may lead to you being prevented from accessing update repositories
     or to wrong repositories being used on the client.</emphasis>
    </para>
    <para>
     When a rollback is done via Snapper, the system will notify the
     registration server to ensure access to the correct repositories is set
     up during the boot process. If the system was restored any other way or
     the communication with the registration server failed for any reason
     (for example, because the server was not accessible due to network
     issues), trigger the rollback on the client manually by calling
     <literal>snapper rollback</literal>.
    </para>
    <para>
     We suggest always checking that the correct repositories are set up on
     the system, especially after refreshing the service using
     <literal>zypper ref -s</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-314974" remap="InstUpgrade:Upgrade">


    <title>/tmp Cleanup from sysconfig Automatically Migrated into systemd Configuration</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis> By default, systemd cleans temporary directories daily, and
     systemd does not honor sysconfig settings in
     </emphasis><literal>/etc/sysconfig/cron</literal><emphasis> such as
     </emphasis><literal>TMP_DIRS_TO_CLEAR</literal><emphasis>. Thus, it is
     needed to change sysconfig settings to avoid data loss or unwanted
     behavior. </emphasis>
    </para>
    <para>
     When updating to SLE 12 or newer, the variables in
     <literal>/etc/sysconfig/cron</literal> will be automatically migrated
     into an appropriate systemd configuration (see
     <literal>/etc/tmpfiles.d/tmp.conf</literal>). The following variables
     are affected:
    </para>
<screen>MAX_DAYS_IN_TMP
MAX_DAYS_IN_LONG_TMP
TMP_DIRS_TO_CLEAR
LONG_TMP_DIRS_TO_CLEAR
CLEAR_TMP_DIRS_AT_BOOTUP
OWNER_TO_KEEP_IN_TMP</screen>
   </section>

  </section>
  <section>
   <title>For More Information</title>
   <para>
    For more information, see
    <xref linkend="InfraPackArch-ArchIndependent"/> and the sections
    relating to your respective hardware architecture.
   </para>
  </section>
 </section>

 <section xml:id="InfraPackArch-ArchIndependent">
  <title>Architecture Independent Information</title>
  <remark> This is highly technical * Know issues categorized ** full description of the problem **
   how to reproduce the issue ** link to Bugzilla number ** command line change/configuration change
   ** work around if available
  </remark>
  <para>
   Information in this section pertains to all architectures supported by
   SUSE Linux Enterprise Server 12 SP3.
  </para>
  <section xml:id="InfraPackArch-ArchIndependent-Kernel" remap="InfraPackArch:ArchIndependent:Kernel">
   <title>Kernel</title>
   <para/>
   <section role="notoc" xml:id="jsc-SLE-22593">
    <title>Unprivileged eBPF usage has been disabled</title>
    <para>
      A large amount of security issues was found and fixed in the Extended
      Berkeley Packet Filter (eBPF) code. To reduce the attack surface, its
      usage has been restricted to privileged users only.
    </para>
    <para>
     Privileged users include <literal>root</literal>. Programs with the
     <literal>CAP_BPF</literal> capability in the newer versions of the Linux
     kernel can still use eBPF as-is.
    </para>
    <para>
     To check the privileged state, you can check the value of the
     <literal>/proc/sys/kernel/unprivileged_bpf_disabled</literal>
     parameter. Value of 0 means "unprivileged enable", and value of 2 means
     "only privileged users enabled".
    </para>
    <para>
     This setting can be changed by the <literal>root</literal> user:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       to enable it temporarily for all users by running the command
       <literal>sysctl kernel.unprivileged_bpf_disabled=0</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       to enable it permanently by adding
       <literal>kernel.unprivileged_bpf_disabled=0</literal>
       to the
       <literal>/etc/sysctl.conf</literal>
       file.
      </para>
     </listitem>
    </itemizedlist>
   </section>

   <section role="notoc" xml:id="fate-322164" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Support for Scalable MCA (SMCA)</title>
    <para>
     <emphasis>As more functionality is added to hardware beginning with
     family 0x17, being able to track it requires an enhanced approach to
     MCA.</emphasis>
    </para>
    <para>
     SLE 12 SP3 now supports AMD's Scalable MCA (SMCA). SMCA is a
     specification which enriches the error information logged by the
     hardware to allow for improved error handling, better diagnosability,
     and future scalability.
    </para>
   </section>
   <section role="notoc" xml:id="fate-322004" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Update Repositories for kGraft Live Patching Are Now Specific to Service Packs</title>
    <para>
     Starting with SLE 12 SP3, the update repositories supplying kernel
     patches that can be applied using kGraft are split up by Service Pack
     version. This allows for easier maintenance and reduces the chance of
     complications during Service Pack upgrades.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321402" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Support for Intel Kaby Lake Processors</title>
    <para>
     SLE 12 SP3 now contains support for Intel processors from the
     generation code-named Kaby Lake.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321398" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>Support for Intel Xeon Phi Knights Landing Coprocessors</title>
    <para>
     SLE 12 SP3 now supports Intel Xeon Phi coprocessors from the product
     line code-named Kights Landing.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321135" remap="InfraPackArch:ArchIndependent:Kernel">


    <title>NVDIMM: Support for Device DAX (Direct Access)</title>
    <para>
     SLE 12 SP3 now supports Device DAX. Device DAX is the device-centric
     analogue of File System DAX: It allows memory ranges to be allocated
     and mapped without need of an intervening file system. This feature can
     improve the performance of both KVM guests and databases like MSSQL
     using raw I/O access to NVDIMM.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Kernel-Modules" remap="InfraPackArch:ArchIndependent:Kernel_Modules">
   <title>Kernel Modules</title>


   <para>
    An important requirement for every enterprise operating system is the
    level of support available for specific environments. Kernel modules are
    the most relevant connector between hardware
    (<quote>controllers</quote>) and the operating system.
   </para>
   <para>

    For more information about the handling of kernel modules, see the SUSE
    Linux Enterprise Administration Guide.
   </para>

   <section role="notoc" xml:id="fate-322780" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>Support for Matrox G200eH3 Graphics Chips</title>
    <para>
     SLE 12 SP3 includes a driver to enable Matrox G200eH3 graphics chips
     that will be used in HPE Gen10 servers.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321401" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>hpwdt Driver (HPE Watchdog) Has Been Updated</title>
    <para>
     SLE 12 SP3 includes an updated version of the HPE watchdog driver
     <literal>hpwdt</literal> to enable support for the upcoming HPE Gen10
     Servers.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319691" remap="InfraPackArch:ArchIndependent:Kernel_Modules">


    <title>Direct Access to Files in Non-Volatile DIMMs</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis> The page cache is usually used to buffer reads and writes to
     files. It is also used to provide the pages which are mapped into
     userspace by a call to </emphasis><literal>mmap</literal><emphasis>.
     For block devices that are memory-like, the page cache pages would be
     unnecessary copies of the original storage. </emphasis>
    </para>
    <para>
     The Direct Access (DAX) kernel code avoids the extra copy by directly
     reading from and writing to the storage device. For file mappings, the
     storage device is mapped directly into userspace. This functionality is
     implemented in the XFS and Ext4 file systems.
    </para>
    <para>
     Non-volatile DIMMs can be "partitioned" into so-called namespaces which
     are then exposed as block devices by the Linux kernel. Each namespace
     can be configured in several modes. Although DAX functionality is
     available for file systems on top of namespaces in both
     <emphasis>raw</emphasis> or <emphasis>memory</emphasis> modes, SUSE
     does not support use of the DAX feature in file systems on top of
     <emphasis>raw</emphasis> mode namespaces as they have unexpected quirks
     and in future releases the feature is likely to go away completely.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Security" remap="InfraPackArch:ArchIndependent:Security">
   <title>Security</title>
   <para/>

   <section role="notoc" xml:id="fate-317116" remap="InfraPackArch:ArchIndependent:Security">


    <title>SELinux Enablement</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     SELinux capabilities have been added to SUSE Linux Enterprise Server
     (in addition to other frameworks, such as AppArmor). While SELinux is
     not enabled by default, customers can run SELinux with SUSE Linux
     Enterprise Server if they choose to.
    </para>
    <para>
     SELinux Enablement includes the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The kernel ships with SELinux support.
      </para>
     </listitem>
     <listitem>
      <para>
       We will apply SELinux patches to all “common” userland packages.
      </para>
     </listitem>
     <listitem>
      <para>
       The libraries required for SELinux (<literal>libselinux</literal>,
       <literal>libsepol</literal>, <literal>libsemanage</literal>, etc.)
       have been added SUSE Linux Enterprise.
      </para>
     </listitem>
     <listitem>
      <para>
       Quality Assurance is performed with SELinux disabled—to make sure
       that SELinux patches do not break the default delivery and the
       majority of packages.
      </para>
     </listitem>
     <listitem>
      <para>
       The SELinux-specific tools are shipped as part of the default
       distribution delivery.
      </para>
     </listitem>
     <listitem>
      <para>
       SELinux policies are not provided by SUSE. Supported policies may be
       available from the repositories in the future.
      </para>
     </listitem>
     <listitem>
      <para>
       Customers and Partners who have an interest in using SELinux in their
       solutions are encouraged to contact SUSE to evaluate their necessary
       level of support and how support and services for their specific
       SELinux policies will be granted.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     By enabling SELinux in our code base, we add community code to offer
     customers the option to use SELinux without replacing significant parts
     of the distribution.
    </para>
   </section>
   <section role="notoc" xml:id="fate-315831" remap="InfraPackArch:ArchIndependent:Security">


    <title>TPM-Capable UEFI Bootloader</title>
    <para>
     SLES 12 SP3 has TPM support in the bootloader used on UEFI systems.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Network" remap="InfraPackArch:ArchIndependent:Network">
   <title>Networking</title>
   <para/>

   <section role="notoc" xml:id="fate-321897" remap="InfraPackArch:ArchIndependent:Network">


    <title>Support for the IDNA2008 Standard for Internationalized Domain Names</title>
    <para>
     <emphasis>The original method for implementing Internationalized Domain
     Names was IDNA2003. This has been replaced by the IDNA2008 standard,
     the use of which is mandatory for some top-level domains.</emphasis>
    </para>
    <para>
     The network utilities <literal>wget</literal> and
     <literal>curl</literal> have been updated to support IDNA2008 through
     the use of <literal>libidn2</literal>. This update also affects
     consumers of the <literal>libcurl</literal> library.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320709" remap="InfraPackArch:ArchIndependent:Network">


    <title>No Support for Samba as Active Directory-Style Domain Controller</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     The version of Samba shipped with SLE 12 GA and newer does not include
     support to operate as an Active Directory-style domain controller. This
     functionality is currently disabled, as it lacks integration with
     system-wide MIT Kerberos.
    </para>
   </section>


   <section role="notoc" xml:id="jsc-SLE-11183">
    <title>New GeoIP Database Sources</title>
    <para>
     The GeoIP databases allow approximately geo-locating users by their IP
     address.
     In the past, the company MaxMind made such data available for free in
     its GeoLite Legacy databases.
     On January 2, 2019, MaxMind discontinued the GeoLite Legacy databases,
     now offering only the newer GeoLite2 databases for download.
     To comply with new data protection regulation, since December 30, 2019,
     GeoLite2 database users are required to comply with an additional usage
     license.
     This change means users now need to register for a MaxMind account and
     obtain a license key to download GeoLite2 databases.
     For more information about these changes, see the
     <link xlink:href="https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases/">MaxMind blog</link>.
    </para>
    <para>
     SLES includes the <package>GeoIP</package> package of tools that are
     only compatible with GeoLite Legacy databases.
     As an update for SLES 12 SP3, we introduce the following new
     packages to deal with the changes to the GeoLite service:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <command>geoipupdate</command>:
       The official Maxmind tool for downloading GeoLite2 databases.
       To use this tool, set up the configuration file with your MaxMind
       account details.
       This configuration file can also be generated on the Maxmind web page.
       For more information, see
       <link xlink:href="https://dev.maxmind.com/geoip/geoip2/geolite2/"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>geolite2legacy</command>:
       A script for converting GeoLite2 CSV data to the GeoLite Legacy format.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>geoipupdate-legacy</command>:
       A convenience script that downloads GeoLite2 data, converts it to the
       GeoLite Legacy format, and stores it in <filename>/var/lib/GeoIP</filename>.
       With this script, applications developed for use with the legacy
       <command>geoip-fetch</command> tool will continue to work.
      </para>
     </listitem>
    </itemizedlist>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-SystemsManagement" remap="InfraPackArch:ArchIndependent:SystemsManagement">
   <title>Systems Management</title>
   <para/>
   <section role="notoc" xml:id="jsc-SLE-12830">
    <title>Salt Has Been Updated to Version 3000</title>
    <para>
     Salt has been upgraded to upstream version 3000, plus a number of
     patches, backports and enhancements by SUSE. In particular,
     CVE-2020-11651 and CVE-2020-11652 fixes are included in our release.
    </para>
    <para>
     As part of this upgrade, cryptography is now managed by the
     Python-M2Crypto library (which is itself based on the well-known OpenSSL
     library).
    </para>
    <para>
     We intend to regularly upgrade Salt to more recent versions.
    </para>
    <para>
     Salt 3000 is the last version of Salt which will support the old syntax
     of the <literal>cmd.run</literal> module.
    </para>
   </section>

   <section role="notoc" xml:id="fate-323556" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>System Clone AutoYaST XML Reflects Btrfs Snapshot State</title>
    <para>
     <emphasis> In previous versions of SLE 12, when using
     </emphasis><literal>yast clone_system</literal><emphasis>, AutoYaST
     would always enable snapshots for Btrfs Volumes, regardless of whether
     they were enabled on the original system. </emphasis>
    </para>
    <para>
     Starting with SLE 12 SP3, <literal>yast clone_system</literal> will now
     create an AutoYaST XML file that accurately reflects snapshot state of
     Btrfs volumes.
    </para>
   </section>
   <section role="notoc" xml:id="fate-323343" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>"Register Extensions or Modules Again" Has Been Removed from YaST</title>
    <para>
     <emphasis> The button </emphasis>Register Extensions or Modules
     Again<emphasis> has been removed from the YaST registration module.
     </emphasis>
    </para>
    <para>
     This option was redundant: It is still possible to register modules or
     extensions again with a different SCC account or using a different
     registration server (SCC or SMT).
    </para>
    <para>
     Additionally, the option to filter out beta versions is now only
     visible if the server provides beta versions, otherwise the check box
     is hidden.
    </para>
   </section>
   <section role="notoc" xml:id="fate-323175" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>The YaST Module for SSH Server Configuration Has Been Removed</title>
    <para>
     <emphasis>The YaST module for configuring an SSH server which was
     present in SLE 11, is not a part of SLE 12. It does not have any direct
     successor.</emphasis>
    </para>
    <para>
     The module <emphasis>SSH Server</emphasis> only supported configuring a
     small subset of all SSH server capabilities. Therefore, the
     functionality of the module can be replaced by using a combination of 2
     YaST modules: The <emphasis>/etc/sysconfig Editor</emphasis> and the
     <emphasis>Services Manager</emphasis>. This also applies to system
     configuration via AutoYaST.
    </para>
   </section>
   <section role="notoc" xml:id="fate-322095" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Sudo Has Been Updated from 1.8.10p3 to 1.8.19p2</title>
    <para>
     Sudo has been updated from version 1.8.10p3 to 1.8.19p2. This update
     fixes many bugs and security vulnerabilities and also brings several
     enhancements. For more information, read the changelog file in
     <literal>/usr/share/doc/packages/sudo/NEWS</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321767" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>YaST: Default Auto-Refresh Status for Local Repositories Is "Off"</title>
    <para>
     <emphasis>In previous versions of SLE 12, when installing from a USB
     drive or external disk, the repository linking to the installation
     media was set to auto-refresh. This means that when the USB drive or
     the external disk had been removed and you are trying to work with YaST
     or Zypper, you were asked to insert the external medium
     again.</emphasis>
    </para>
    <para>
     In the YaST version shipped with SLE 12 SP3, we have changed the
     default auto-refresh status for local repositories (USB drives, hard
     disks or <literal>dir://</literal>) to <literal>off</literal> which
     avoids checking the now usually unnecessary repository.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321049" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>All Snapper Commands Support the Option --no-dbus</title>
    <para>
     <emphasis> Normally, the
     </emphasis><literal>snapper</literal><emphasis> command line tool uses
     DBus to connect to </emphasis><literal>snapperd</literal><emphasis>
     which does most of the actual work. This allows non-root users to work
     with Snapper. </emphasis>
    </para>
    <para>
     <emphasis> However, there are situations when using DBus is not
     possible, for example, when chrooted on the rescue system or when DBus
     itself is broken after an update. This can limit the usefulness of
     Snapper as a disaster recovery tool. Therefore, some Snapper commands
     already supported the </emphasis><literal>--no-dbus</literal><emphasis>
     option, bypassing DBus and
     </emphasis><literal>snapperd</literal><emphasis>. </emphasis>
    </para>
    <para>
     In the version of Snapper shipped with SLE 12 SP3, all Snapper commands
     support the <literal>--no-dbus</literal> option.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320593" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>blogd Boot Log Daemon Available as an Alternative to Plymouth</title>
    <para>
     The <literal>blogd</literal> boot log daemon (package
     <literal>blog</literal> and <literal>blog-plymouth</literal>) can be
     used as a replacement for Plymouth in situations where a splash screen
     or usage of a frame buffer is unwanted. <literal>blogd</literal> is
     also a Plymouth agent. That means, it can handle requests for a
     password prompt by the system password service of systemd.
    </para>
    <para>
     The <literal>blogd</literal> daemon writes out boot log messages to
     every terminal device used for <literal>/dev/console</literal> and to
     the log file <literal>/var/log/boot.log</literal>. When halting or
     rebooting the system, it moves the log file to
     <literal>/var/log/boot.old</literal> and appends all log messages up to
     the point at which the file systems becomes unavailable.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320392" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>ntp 4.2.8</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>ntp was updated to version 4.2.8.</emphasis>
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis> The ntp server ntpd does not synchronize with its peers
       anymore and the peers are specified by their host name in
       </emphasis><literal>/etc/ntp.conf</literal><emphasis>. </emphasis>
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis> The output of </emphasis><literal>ntpq
       --peers</literal><emphasis> lists IP numbers of the remote servers
       instead of their host names. </emphasis>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     <emphasis>Name resolution for the affected hosts works
     otherwise.</emphasis>
    </para>
    <bridgehead renderas="sect5">Parameter changes</bridgehead>
    <para>
     The meaning of some parameters for the sntp command-line tool have
     changed or have been dropped, for example <literal>sntp -s</literal> is
     now <literal>sntp -S</literal>. Please review any
     <literal>sntp</literal> usage in your own scripts for required changes.
    </para>
    <para>
     After having been deprecated for several years, ntpdc is now disabled
     by default for security reasons. It can be re-enabled by adding the
     line <literal>enable mode7</literal> to
     <literal>/etc/ntp.conf</literal>, but preferably
     <literal>ntpq</literal> should be used instead.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320016" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Support for Setting Kdump Low-Memory and High-Memory Allocation on the YaST Command Line</title>
    <para>
     <emphasis> In the past, YaST supported setting a high-memory and
     low-memory amounts for the kernel parameter
     </emphasis><literal>crashkernel</literal><emphasis> only from the
     ncurses or Qt interfaces. </emphasis>
    </para>
    <para>
     You can now set these memory amounts on the command line too. To do so,
     use, for example, <literal>yast kdump startup enable
     alloc_mem=256,768</literal>. The first number represents the
     low-memory amount, the second number represents the high-memory amount.
     Therefore, the example is equivalent to setting
     <literal>crashkernel=256,low crashkernel=768,high</literal> on the
     kernel command line.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319830" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Salt Configuration with AutoYaST</title>
    <para>
     With SLE 12 SP3, it is possible to configure Salt clients using
     AutoYaST. To use this feature, you need the package
     <literal>salt-minion</literal> which is not available in the standard
     SLES product. However, you can install this dependency from the SLE
     Module <emphasis>Advanced Systems Management</emphasis>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319486" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>Zypper Option --plus-content Has Been Enhanced</title>
    <para>
     The <literal>zypper</literal> option <literal>--plus-content</literal>
     was enhanced to allow specifying disabled repositories by name or alias
     also. Additionally, it can now be used with the <literal>zypper
     refresh</literal> command, to refresh either specified or all disabled
     repositories without the need to enable them.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318832" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>YaST: iSCSI Authentication Has Been Redesigned</title>
    <para>
     <emphasis>In the past, the user interface for iSCSI authentication
     offered by YaST was not optimal. Additionally, not every option was
     explained in the help.</emphasis>
    </para>
    <para>
     In SLE 12 SP3, the YaST module <emphasis>iSCSI Initiator and
     Target</emphasis> has with the following enhancements:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Clearer terminology:
      </para>
      <itemizedlist>
        <listitem>
         <para>
          For discovery sessions, <emphasis>No Authentication</emphasis> is
          now called <emphasis>No Discovery Authentication</emphasis>.
         </para>
         <para>
          For login sessions, <emphasis>Use Authentication</emphasis> is now
          called <emphasis>Use Login Authentication</emphasis>, whereas
          <emphasis>No Authentication</emphasis> is now called <emphasis>No
          Login Authentication</emphasis>.
         </para>
        </listitem>
        <listitem>
         <para>
          <emphasis>Incoming Authentication</emphasis> is now called
          <emphasis>Authentication by Initiators</emphasis> on the initiator
          side, whereas it is called <emphasis>Authentication by
          Targets</emphasis> on the target side.
         </para>
        </listitem>
        <listitem>
         <para>
          <emphasis>Outgoing Authentication</emphasis> is now called
          <emphasis>Authentication by Targets</emphasis> on the initiator
          side, whereas it is called <emphasis>Authentication by
          Initiators</emphasis> on the target side.
         </para>
        </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       <emphasis>No Login Authentication</emphasis> can now be used to log
       in to targets without authentication.
      </para>
     </listitem>
     <listitem>
      <para>
       The help now explains password options.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-316631" remap="InfraPackArch:ArchIndependent:SystemsManagement">


    <title>systemd Daemon</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     SLE 12 has moved to systemd, a new way of managing services. For more
     information, see the <emphasis>SUSE Linux Enterprise Admin
     Guide</emphasis>, Section <emphasis>The systemd Daemon</emphasis> (<link xlink:href="https://documentation.suse.com/sles/12-SP3/"/>.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Storage" remap="InfraPackArch:ArchIndependent:Storage">
   <title>Storage</title>
   <para/>

   <section role="notoc" xml:id="fate-324448" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Compatibility of Newly Created XFS File Systems With SLE 11</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>XFS file systems created with the default settings of SLES 12
     SP2 and later cannot be used SLE 11 installations.</emphasis>
    </para>
    <para>
     In SLE 12 SP2 and later, by default, XFS file systems are created with
     the option <literal>ftype=1</literal> that changes the superblock
     format. Among other things, this helps accommodate Docker. However,
     this option is incompatible with SLE 11.
    </para>
    <para>
     To create a SLE 11-compatible XFS file system, use the parameter
     <literal>ftype=0</literal>. For example, to format an empty device,
     run: :
    </para>
<screen>mkfs.xfs -m crc=0 -n ftype=0 [DEVICE]</screen>
   </section>
   <section role="notoc" xml:id="fate-321773" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Automatic Cleanup of Snapshots Created by Rollbacks</title>
    <para>
     <emphasis>In SLES 12 SP2 and before, you had to manually delete
     snapshots created by rollbacks at an appropriate time to avoid filling
     up the storage.</emphasis>
    </para>
    <para>
     Starting with SLE 12 SP3, this process has been automated. During a
     rollback, Snapper sets the cleanup algorithm <literal>number</literal>
     for the snapshot corresponding to the previous default subvolume and
     for the backup snapshot of the previous default subvolume.
    </para>
    <para>
     For more information, see
     <link xlink:href="http://snapper.io/2017/05/10/automatic-cleanup-after-rollback.html">http://snapper.io/2017/05/10/automatic-cleanup-after-rollback.html</link>
    </para>
   </section>
   <section role="notoc" xml:id="fate-321536" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Establishing an NVMe-over-Fabrics Connection</title>
    <para>
     To be able to establish an NVMe-over-Fabrics connection with the Linux
     kernel provided with the SLE 12 SP3 media, you need to delete or rename
     the file <literal>/etc/nvme/hostid</literal>.
    </para>
    <para>
     To restore this file when the kernel update that fixes this issue is
     released, generate a new host ID by running:
    </para>
<screen>uuidgen &gt; /etc/nvme/hostid</screen>
   </section>
   <section role="notoc" xml:id="fate-320870" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Root File System Conversion to Btrfs Not Supported</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>If it is not the root file system and if the file system has
     at least 20 % free space available, in-place conversion of an existing
     Ext2/Ext3/Ext4 or ReiserFS file system is supported for data mount
     points.</emphasis>
    </para>
    <para>
     SUSE does not recommend or support in-place conversion of OS root file
     systems. In-place conversion to Btrfs of root file systems requires
     manual subvolume configuration and additional configuration changes
     that are not automatically applied for all use cases.
    </para>
    <para>
     To ensure data integrity and the highest level of customer
     satisfaction, when upgrading, maintain existing root file systems.
     Alternatively, reinstall the entire operating system.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320834" remap="InfraPackArch:ArchIndependent:Storage">


    <title>/var/cache on an Own Subvolume for Snapshots and Rollback</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <literal>/var/cache</literal><emphasis> contains very volatile data,
     like the Zypper cache with RPM packages in different versions for each
     update. As a result of storing data that is mostly redundant but highly
     volatile, the amount of disk space a snapshot occupies can increase
     very fast. </emphasis>
    </para>
    <para>
     To solve this, move <literal>/var/cache</literal> to a separate
     subvolume. On fresh installations of SLE 12 SP2 or newer, this is done
     automatically. To convert an existing root file system, perform the
     following steps:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Find out the device name (<literal>/dev/sda2</literal>,
       <literal>/dev/sda3</literal> etc.) of the root file system:
       <literal>df /</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Identify the parent subvolume of all the other subvolumes. For SLE 12
       installations, this is a subvolume named <literal>@</literal>. To
       check if you have a <literal>@</literal> subvolume, use:
       <literal>btrfs subvolume list / | grep '@'</literal>. If the output
       of this command is empty, you do not have a subvolume named
       <literal>@</literal>. In that case, you may be able to proceed with
       subvolume ID 5 which was used in older versions of SLE.
      </para>
     </listitem>
     <listitem>
      <para>
       Now mount the requisite subvolume.
      </para>
      <itemizedlist>
        <listitem>
         <para>
          If you have a <literal>@</literal> subvolume, mount that subvolume
          to a temporary mount point: <literal>mount &lt;root_device&gt; -o
          subvol=@ /mnt</literal>
         </para>
        </listitem>
        <listitem>
         <para>
          If you don't have a <literal>@</literal> subvolume, mount
          subvolume ID 5 instead: <literal>mount &lt;root_device&gt; -o
          subvolid=5 /mnt</literal>
         </para>
        </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       <literal>/mnt/var/cache</literal> can already exist and could be the
       same directory as <literal>/var/cache</literal>. To avoid data loss,
       move it: <literal>mv /mnt/var/cache /mnt/var/cache.old</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       In either case, create a new subvolume: <literal>btrfs subvol create
       /mnt/var/cache</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       If there is now a directory <literal>/var/cache.old</literal>, move
       it to the new location: <literal>mv /var/cache.old/*
       /mnt/var/cache</literal>. If that is not the case, instead do:
       <literal>mv /var/cache/* /mnt/var/cache/</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Optionally, remove <literal>/mnt/var/cache.old</literal>:
       <literal>rm -rf /mnt/var/cache.old</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Unmount the subvolume from the temporary mount point: <literal>umount
       /mnt</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       Add an entry to <literal>/etc/fstab</literal> for the new
       <literal>/var/cache</literal> subvolume. Use an existing subvolume as
       a template to copy from. Make sure to leave the UUID untouched (this
       is the root file system's UUID) and change the subvolume name and its
       mount point consistently to <literal>/var/cache</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Mount the new subvolume as specified in /etc/fstab: <literal>mount
       /var/cache</literal>
      </para>
     </listitem>
    </orderedlist>
   </section>
   <section role="notoc" xml:id="fate-317775" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Support for Arbitrary Btrfs Subvolume Structure in AutoYaST</title>
    <para>
     To set up a system with a non-default Btrfs subvolume structure with
     AutoYaST, you can now specify an arbitrary Btrfs subvolume structure in
     <literal>autoinst.xml</literal>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-312751" remap="InfraPackArch:ArchIndependent:Storage">


    <title>Snapper: Cleanup Rules Based on Fill Level</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>Some programs do not respect the special disk space
     characteristics of a Btrfs file system containing snapshots. This can
     result in unexpected situations where no free space is left on a Btrfs
     filesystem.</emphasis>
    </para>
    <para>
     Snapper can watch the disk space of snapshots that have automatic
     cleanup enabled and can try to keep the amount of disk space used below
     a threshold.
    </para>
    <para>
     If snapshots are enabled, the feature is enabled for the root file
     system by default on new installations.
    </para>
    <para>
     For existing installations, the system administrator must enable quota
     and set limits for the cleanup algorithm to use this new feature. This
     can be done using the following commands:
    </para>
    <orderedlist>
     <listitem>
      <para>
       <literal>snapper setup-quota</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>snapper set-config NUMBER_LIMIT=2-10
       NUMBER_LIMIT_IMPORTANT=4-10</literal>
      </para>
     </listitem>
    </orderedlist>
    <para>
     For more information, see the man pages of <literal>snapper</literal>
     and <literal>snapper-configs</literal>.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Virtualization" remap="InfraPackArch:ArchIndependent:Virtualization">
   <title>Virtualization</title>
   <para/>

   <section role="notoc" xml:id="fate-322404" remap="InfraPackArch:ArchIndependent:Virtualization">


    <title>Supported Offline Migration Scenarios</title>
    <para>
     The following host operating system combinations will be fully
     supported (L3) for migrating guests from one host to another for SLES
     12 SP3:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLES 12 GA to SLES 12 SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLES 12 SP1 to SLES 12 SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLES 12 SP2 to SLES 12 SP3
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-322145" remap="InfraPackArch:ArchIndependent:Virtualization">


    <title>SUSE Virtual Machine Driver Pack 2.5</title>
    <para>
     <emphasis>SUSE Linux Enterprise Virtual Machine Driver Pack is a set of
     paravirtualized device drivers for Microsoft Windows operating systems.
     These drivers improve the performance of unmodified Windows guest
     operating systems that are run in virtual environments created using
     Xen or KVM hypervisors with SUSE Linux Enterprise Server 11 SP4 and
     SUSE Linux Enterprise Server 12 SP3. Paravirtualized device drivers are
     installed in virtual machine instances of operating systems and
     represent hardware and functionality similar to the underlying physical
     hardware used by the system virtualization software layer.</emphasis>
    </para>
    <para>
     SLE now comes with SUSE Linux Enterprise Virtual Machine Driver Pack 2.5.
    </para>
   </section>

   <section xml:id="InfraPackArch-ArchIndependent-Virtualization-KVM" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">
    <title>KVM</title>
    <para/>

    <section role="notoc" xml:id="fate-321335" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>KVM Now Supports up to 288 vCPUs</title>
     <para>
      KVM now supports up to 288 vCPUs in a virtual machine.
     </para>
    </section>
    <section role="notoc" xml:id="fate-319478" remap="InfraPackArch:ArchIndependent:Virtualization:KVM">


     <title>Support for AVIC (Advanced Virtual Interrupt Controller)</title>
     <para>
      <emphasis>In the past, LAPIC interrupts (Local Advanced Interrupt
      Controller) on AMD processors had to be virtualized with software
      which did not yield optimal performance.</emphasis>
     </para>
     <para>
      The version of KVM shipped with SLE 12 SP3 can use AVIC (Advanced
      Virtual Interrupt Controller), a hardware feature in recent AMD
      processors, to provide a virtualized LAPIC to the guest. This improves
      the virtualization performance.
     </para>
     <para>
      AVIC is a set of components to present a virtualized LAPIC to guests,
      thus allowing most LAPIC accesses and interrupt delivery to the guests
      directly. The AVIC architecture also leverages the existing IOMMU
      interrupt redirection mechanism to deliver peripheral device
      interrupts to guests directly.
     </para>
    </section>

   </section>
  </section>
  <section xml:id="InfraPackArch-ArchIndependent-Misc">
   <title>Miscellaneous</title>
   <para/>
   <section role="notoc" xml:id="jsc-SLE-12573">
    <title>Virtual Users support in <literal>vsftpd</literal></title>
    <para>
     Previously, this functionality was provided by the
     <literal>pam_userdb</literal> module that was part of the general
     <literal>pam</literal> package. This module has been removed and the
     functionality is now provided as part of the <literal>pam-extra</literal>
     package.
   </para>
   </section>

   <section role="notoc" xml:id="fate-321126" remap="InfraPackArch:ArchIndependent:Misc">


    <title>GNOME: Support for Chinese, Japanese, Korean Installed and Configured Automatically</title>
    <para>
     When first logging in to GNOME on SLES 12 SP3 with the Workstation
     Extension or SLED 12 SP3, <literal>gnome-initial-setup</literal> will
     ask Chinese, Japanese, and Korean users for their preferred input
     method.
    </para>
    <para>
     Because <literal>gnome-initial-setup</literal> is set up to run
     directly after the first login, it is also set up to not run before the
     GDM interface starts. This behavior is configured in the GDM
     configuration file <literal>/etc/gdm/custom.conf</literal> with the
     line <literal>InitialSetupEnable=False</literal>. Do not change this
     setting, otherwise a system without a normal user will not be able to
     provide the expected GDM log-in window.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="InfraPackArch-x86-64">
  <title>AMD64/Intel 64 (x86_64) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP3 for the AMD64/Intel 64 architectures.
  </para>
  <section xml:id="InfraPackArch-x86-64-System" remap="InfraPackArch:x86_64:System">
   <title>System and Vendor Specific Information</title>
   <para/>

   <section role="notoc" xml:id="fate-321473" remap="InfraPackArch:x86_64:System">


    <title>Intel* Omni-Path Architecture (OPA) Host Software</title>
    <para>
     Intel Omni-Path Architecture (OPA) host software is fully supported in
     SUSE Linux Enterprise Server 12 SP3.
    </para>
    <para>
     Intel OPA provides Host Fabric Interface (HFI) hardware with
     initialization and setup for high performance data transfers (high
     bandwidth, high message rate, low latency) between compute and I/O
     nodes in a clustered environment.
    </para>
    <para>
     For instructions on installing Intel Omni-Path Architecture
     documentation, see
     <link xlink:href="https://www.intel.com/content/dam/support/us/en/documents/network-and-i-o/fabric-products/Intel_OP_Software_SLES_12_3_RN_J71758.pdf"/>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321435" remap="InfraPackArch:x86_64:System">


    <title>Support for Both TPM 1.2 and 2.0</title>
    <para>
     <emphasis>Over the recent years TPM 2.0 variants have become more
     common. With a different API this needs new libraries, tools and
     bootloader support.</emphasis>
    </para>
    <para>
     SLE 12 SP2 and also SP3 provide equal support for TPM 1.2 and TPM 2.0
     utilities and booting.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="InfraPackArch-Power" remap="InfraPackArch:Power">
  <title>POWER (ppc64le) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP3 for the POWER architecture.
  </para>

  <section role="notoc" xml:id="fate-327576" remap="InfraPackArch:Power">


   <title>Support for ibmvnic Networking Driver</title>
   <para>
    The kernel device driver <literal>ibmvnic</literal> provides support for
    vNIC (virtual Network Interface Controller) which is a PowerVM virtual
    networking technology that delivers enterprise capabilities and
    simplifies network management on IBM POWER systems. It is an efficient
    high-performance technology.
   </para>
   <para>
    When combined with SR-IOV NIC, it provides bandwidth control Quality of
    Service (QoS) capabilities at the virtual NIC level. vNIC significantly
    reduces virtualization overhead resulting in lower latencies and fewer
    server resources (CPU, memory) required for network virtualization.
   </para>
   <para>
    For a detailed support statement of ibmvnic in SLES, see
    <link xlink:href="https://support.scc.suse.com/s/kb/Support-for-ibmvnic-in-SLES-1583239456478"/>.
   </para>
  </section>
  <section role="notoc" xml:id="fate-323651" remap="InfraPackArch:Power">


   <title>QEMU-virtualized PReP Partition</title>
   <para>
    <emphasis>On POWER, the PReP partition which contains the bootloader has
    no unique identifier other than the serial number of the disk on which
    it created. When virtualized with QEMU, QEMU does not provide any disk
    serial number unless you explicitly specify one.</emphasis>
   </para>
   <para>
    <emphasis>This means that when running under QEMU, the PReP partition of
    an installation does not have any unique identification. In consequence,
    the partition name can change when a disk is added or removed from the
    virtual machine or when the storage configuration otherwise changes.
    This can lead to system errors when reinstalling or updating the
    bootloader.</emphasis>
   </para>
   <para>
    If you expect the storage configuration of a QEMU virtual machine on
    POWER to change over the lifetime of the installation, we recommend
    sidestepping this issue: Before the initial installation, assign a
    unique serial number to each disk in a QEMU virtual machine.
   </para>
  </section>
  <section role="notoc" xml:id="fate-322470" remap="InfraPackArch:Power">


   <title>512 TB Virtual Address Space on POWER</title>
   <para>
    <emphasis>Certain workloads require a large virtual address space for a
    single process</emphasis>
   </para>
   <para>
    The virtual address space limit has been increased from 64 TB to 512 TB
    on the POWER architecture. To maintain compatibility with older software
    and hardware, processes are limited to 128 TB virtual address space
    unless they explicitly map memory above 128 TB.
   </para>
   <para>
    This functionality is supported starting with kernel 4.4.103. We always
    recommend using the latest kernel update to get fixes for any issues
    found over the product lifetime.
   </para>
  </section>
  <section role="notoc" xml:id="fate-321842" remap="InfraPackArch:Power">


   <title>kdump: Shorter Time to Filter and Save /proc/vmcore</title>
   <para>
    The updated <literal>makedumpfile</literal> tool shipped SLES 12 SP3
    supports multithreading. This can be leveraged to reduce the time spent
    in the capture kernel by following the below steps:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Set <literal>KDUMP_CPUS=[CPUS]</literal> in the file
      <literal>/etc/sysconfig/kdump</literal>. Replace
      <emphasis>[CPUS]</emphasis> with the number of CPUs to use in the
      Kdump kernel.
     </para>
    </listitem>
    <listitem>
     <para>
      Set <literal>MAKEDUMPFILE_OPTIONS="--num-threads [CPUs-1]"</literal>.
      Using one CPU less than there are total active CPUs can improve
      performance.
     </para>
    </listitem>
    <listitem>
     <para>
      Set <literal>KDUMPTOOL_FLAGS=NOSPLIT</literal> in the file
      <literal>/etc/sysconfig/kdump</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      Restart <literal>kdump.service</literal>.
     </para>
    </listitem>
   </orderedlist>
  </section>
  <section role="notoc" xml:id="fate-321841" remap="InfraPackArch:Power">


   <title>Parameter crashkernel Is Now Used for fadump Memory Reservation</title>
   <para>
    Starting with SLE 12 SP3, to reserve memory for
    <literal>fadump</literal>, use the parameter
    <literal>crashkernel</literal> parameter instead of the deprecated
    parameter <literal>fadump_reserve_mem</literal>. The offset for
    <literal>fadump</literal> is calculated in the kernel. Therefore, if you
    provide an offset in the parameter <literal>crashkernel=</literal>, it
    will be ignored.
   </para>
  </section>
  <section role="notoc" xml:id="fate-321642" remap="InfraPackArch:Power">


   <title>Encryption Improvements Using Hardware Optimizations</title>
   <para>
    The performance of kernel XTS mode on POWER platforms has been improved
    in SLES 12 SP3 by exploiting instruction set enhancements. On POWER8, it
    now runs up to 20 times faster than in SLES 12 SP2. Kernel CBC and CTR
    modes were already optimized in a previous release.
   </para>
   <para>
    To ensure that your kernel is using the accelerated POWER kernel crypto
    implementations, verify that the module <literal>vmx_crypto</literal>
    has been loaded:
   </para>
<screen>lsmod | grep vmx_crypto</screen>
  </section>
  <section role="notoc" xml:id="fate-321098" remap="InfraPackArch:Power,InfraPackArch:SystemZ:Storage">


   <title>Ceph Client Support on IBM Z and POWER</title>
   <para>
    On SLES 12 SP2 and SLES 12 SP3, IBM Z and POWER machines can now
    function as SUSE Enterprise Storage (Ceph) clients.
   </para>
   <para>
    This support is possible because the kernels for IBM Z and POWER now
    have the relevant modules for CephFS and RBD enabled. The Ceph client
    RPMs for IBM Z and POWER are included in SLE 12 SP3. Additionally, the
    QEMU packages for IBM Z and POWER are now built against librbd.
   </para>
  </section>
  <section role="notoc" xml:id="fate-319551" remap="InfraPackArch:Power">


   <title>Memory Reservation Support for fadump in YaST</title>
   <para>
    Memory to be reserved for firmware-assisted dumps (also known as
    <literal>fadump</literal>, available on the POWER architecture) can now
    be specified in the Kdump module of YaST.
   </para>
  </section>

  <section role="notoc" xml:id="bsc-1136157">
   <title>Speed of <literal>ibmveth</literal> Interface Not Reported Accurately</title>
   <para>
    The <literal>ibmveth</literal> interface is a paravirtualized interface.
    When communicating between LPARs within the same system, the interface's
    speed is limited only by the system's CPU and memory bandwidth.
    When the virtual Ethernet is bridged to a physical network, the
    interface's speed is limited by the speed of that physical network.
   </para>
   <para>
    Unfortunately, the <literal>ibmveth</literal> driver has no way of
    determining automatically whether it is bridged to a physical network
    and what the speed of that link is.
    <literal>ibmveth</literal> therefore reports its speed as a fixed value
    of 1 Gb/s which in many cases will be inaccurate.
    To determine the actual speed of the interface, use a benchmark.
   </para>
  </section>
 </section>
 <section xml:id="InfraPackArch-SystemZ">
  <title>IBM Z (s390x) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP3 for the IBM Z architecture.
  </para>
  <para>
   IBM zEnterprise 196 (z196) and IBM zEnterprise 114 (z114) further on
   referred to as z196 and z114.
  </para>
  <section xml:id="InfraPackArch-SystemZ-Hardware" remap="InfraPackArch:SystemZ:Hardware">
   <title>Hardware</title>
   <para/>

   <section role="notoc" xml:id="fate-321496" remap="InfraPackArch:SystemZ:Hardware,Packages:Modules">


    <title>Support for New Hardware Instructions in Toolchain</title>
    <para>
     Support for new hardware instructions in <literal>binutils</literal>,
     GCC and GDB is available through the Toolchain Module.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Virtualization" remap="InfraPackArch:SystemZ:Virtualization">
   <title>Virtualization</title>
   <para/>

   <section role="notoc" xml:id="fate-321529" remap="InfraPackArch:SystemZ:Virtualization">


    <title>qeth Device Driver Has Accelerated set_rx_mode Implementation</title>
    <para>
     Improved initialization of qeth network devices in layer 2 and layer 3
     allows for faster booting Linux instances.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Storage" remap="InfraPackArch:SystemZ:Storage">
   <title>Storage</title>
   <para/>

   <section role="notoc" xml:id="fate-321531" remap="InfraPackArch:SystemZ:Storage">


    <title>parted Augmented with Partitioning Functionality as Provided by IBM Z Tools</title>
    <para>
     The partitioning utility <literal>parted</literal> now includes
     partitioning functionality for FBA and ECKD DASDs. This brings
     <literal>parted</literal> up to par with the functionality provided by
     IBM Z tools.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321528" remap="InfraPackArch:SystemZ:Storage">


    <title>DASD Channel Path-Aware Error Recovery</title>
    <para>
     The DASD driver can now exclude paths from normal operation if other
     channel paths are available.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321527" remap="InfraPackArch:SystemZ:Storage">


    <title>New dasdfmt Quick Format Mode</title>
    <para>
     With the new <emphasis>quick format</emphasis> mode, you can define
     DASD volumes with a pre-formatted track layout. This significantly
     reduces the deployment time of DASD volumes.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321098-1" remap="InfraPackArch:Power,InfraPackArch:SystemZ:Storage">


    <title>Ceph Client Support on IBM Z and POWER</title>
    <para>
     On SLES 12 SP2 and SLES 12 SP3, IBM Z and POWER machines can now
     function as SUSE Enterprise Storage (Ceph) clients.
    </para>
    <para>
     This support is possible because the kernels for IBM Z and POWER now
     have the relevant modules for CephFS and RBD enabled. The Ceph client
     RPMs for IBM Z and POWER are included in SLE 12 SP3. Additionally, the
     QEMU packages for IBM Z and POWER are now built against librbd.
    </para>
   </section>
   <section role="notoc" xml:id="fate-318032" remap="InfraPackArch:SystemZ:Storage">


    <title>GPFS Partition Type in fdasd</title>
    <para>
     The new partition type "GPFS" in the fdasd tool supports fast
     identification and handles partitions that contain GPFS Network Shared
     Disks.
    </para>
   </section>
   <section role="notoc" xml:id="fate-317881" remap="InfraPackArch:SystemZ:Storage">


    <title>LUN Scanning Enabled by Default</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>Unlike in SLES 11, LUN scanning is enabled by default in SLES
     12 and newer. Instead of having a user-maintained whitelist of
     FibreChannel/SCSI disks that are brought online to the guest, the
     system now polls all targets on a fabric. This is especially helpful on
     systems with hundreds of zFCP disks and exclusive zoning.</emphasis>
    </para>
    <para>
     <emphasis>However, on systems with few disks and an open fabric, this
     can lead to long boot times or access to inappropriate disks. It can
     also lead to difficulties offlining and removing disks.</emphasis>
    </para>
    <para>
     To disable LUN scanning, set the boot parameter
     <literal>zfcp.allow_lun_scan=0</literal>.
    </para>
    <para>
     For LUN Scanning to work properly, the minimum storage firmware levels
     are:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       DS8000 Code Bundle Level 64.0.175.0
      </para>
     </listitem>
     <listitem>
      <para>
       DS6000 Code Bundle Level 6.2.2.108
      </para>
     </listitem>
    </itemizedlist>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Network" remap="InfraPackArch:SystemZ:Network">
   <title>Network</title>
   <para/>

   <section role="notoc" xml:id="fate-319579" remap="InfraPackArch:SystemZ:Network">


    <title>snIPL: Hardening</title>
    <para>
     Secure connections for snIPL enable Linux to remotely handle a greater
     variety of environments.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Security" remap="InfraPackArch:SystemZ:Security">
   <title>Security</title>
   <para/>

   <section role="notoc" xml:id="fate-322025" remap="InfraPackArch:SystemZ:Security">


    <title>libica with DRBG Random Number Generation</title>
    <para>
     The package <literal>libica</literal> now includes a DRBG
     (Deterministic Random Bit Generator) that is compliant with the updated
     security specification NIST SP 800-90A for pseudo-random number
     generation.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321530" remap="InfraPackArch:SystemZ:Security">


    <title>Toleration Support for New Cryptography Hardware</title>
    <para>
     SLES 12 SP3 includes support for using new cryptography hardware in
     toleration mode. This allows performing cryptographic operations as on
     older hardware which means an easier migration to new hardware.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-RAS" remap="InfraPackArch:SystemZ:RAS">
   <title>Reliability, Availability, Serviceability (RAS)</title>
   <para/>

   <section role="notoc" xml:id="fate-321534" remap="InfraPackArch:SystemZ:RAS">


    <title>Stable PCI Identifiers Using UIDs</title>
    <para>
     To maintain persistent configurations for PCI devices, SLES 12 SP3 now
     provides stable and unique identifiers for PCI functions for as long as
     the I/O configuration (IOCDS and HCD) remains stable.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319573" remap="InfraPackArch:SystemZ:RAS">


    <title>Hardware Breakpoint Support in GDB</title>
    <para>
     When code needs to be treated as read-only, software breakpoints cannot
     be used. GDB can now use hardware breakpoints for debugging.
    </para>
   </section>

  </section>
  <section xml:id="InfraPackArch-SystemZ-Performance" remap="InfraPackArch:SystemZ:Performance">
   <title>Performance</title>
   <para/>

   <section role="notoc" xml:id="fate-321651" remap="InfraPackArch:SystemZ:Performance">


    <title>Support for 2 GB Memory Pages</title>
    <para>
     Applications with huge memory sets can use 2 GB large memory pages for
     improved memory handling.
    </para>
   </section>
   <section role="notoc" xml:id="fate-321650" remap="InfraPackArch:SystemZ:Performance">


    <title>Extended CPU Topology to Support Drawers</title>
    <para>
     Addressing CPUs across drawers improves scheduling and performance
     analysis on IBM z Systems z13 and later hardware.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="InfraPackArch-AArch64" remap="InfraPackArch:AArch64">
  <title>ARM 64-Bit (AArch64) Specific Information</title>
  <para>
   Information in this section pertains to the version of SUSE Linux
   Enterprise Server 12 SP3 for the AArch64 architecture.
  </para>

  <section role="notoc" xml:id="fate-323971" remap="InfraPackArch:AArch64">


   <title>Boot and Driver Enablement for Raspberry Pi 3 Model B</title>
   <para>
    The Raspberry Pi 3 Model B is a single-board computer based on the
    Broadcom BCM2837 chipset.
   </para>
   <para>
    SUSE provides a preconfigured image, <emphasis>SUSE Linux Enterprise
    Server for ARM 12 SP3 for the Raspberry Pi</emphasis> and a Kiwi
    template to derive custom appliances,
    <literal>kiwi-templates-SLES12-RPi</literal>. The following sections
    describe requirements and support limitations of custom and modified
    images.
   </para>
   <bridgehead renderas="sect5">Boot Requirements</bridgehead>
   <para>
    To boot SUSE Linux Enterprise Server 12 SP3 on the Raspberry Pi 3 Model
    B, a special MBR partitioning scheme must be used. Firmware files for
    the VideoCore IV processor must be installed into a FAT-formatted
    partition (type <literal>0x0C</literal>) that should be mounted as
    <literal>/boot/efi</literal> (alternatively <literal>/boot/vc</literal>
    if separate from <literal>/boot/efi</literal>). To provide a
    UEFI-compatible environment for booting, as on regular server systems,
    configure the Raspberry Pi firmware to load the U-Boot bootloader.
    U-Boot in turn can then boot GRUB either from an FAT-/Ext4-formatted SD
    card or USB device (Btrfs is not supported by U-Boot) or via network.
   </para>
   <bridgehead renderas="sect5">Driver Enablement</bridgehead>
   <para>
    Not all connectors or functions of the Raspberry Pi are supported in
    this version:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      VideoCore IV GPU 3D acceleration needs to remain disabled. The SUSE
      image provides a configuration file to that effect.
     </para>
    </listitem>
    <listitem>
     <para>
      Built-in audio is not supported, neither via HDMI nor via audio jack.
     </para>
    </listitem>
    <listitem>
     <para>
      The CSI camera connector (for example, the Raspberry Pi Camera Module)
      is not supported.
     </para>
    </listitem>
    <listitem>
     <para>
      The DSI display connector (for example, the Raspberry Pi Touch
      Display) is not supported.
     </para>
    </listitem>
   </itemizedlist>
   <bridgehead renderas="sect5">Expansion Boards</bridgehead>
   <para>
    The Raspberry Pi 3 Model B offers a 40-pin General Purpose I/O
    connector, with multiple software-configurable functions such as UART,
    I²C and SPI. This pin mux configuration along with any external devices
    attached to the pins is defined in the Device Tree which is passed by
    the bootloader to the kernel.
   </para>
   <para>
    In the SUSE image, HATs and other expansion boards attached to the GPIO
    connector are not enabled by default and SUSE does not provide support
    for their use. However, insofar as drivers for pin functions and for
    attached chipsets are included in SUSE Linux Enterprise, they can be
    used. SUSE does not provide support for making changes to the Device
    Tree but successful changes will not affect the support status of the
    operating system itself. Be aware that errors in the Device Tree can
    stop the system from booting successfully or can even damage the
    hardware.
   </para>
   <para>
    In SUSE Linux Enterprise Server for ARM 12 SP3 for the Raspberry Pi, the
    Device Tree is provided by the U-Boot bootloader (not by the Raspberry
    Pi firmware), and Device Tree Overlays are not supported in this version
    of U-Boot.
   </para>
   <para>
    The recommended way to override the Device Tree in SUSE Linux Enterprise
    Server for ARM 12 SP3 for the Raspberry Pi is to place a customized
    <literal>bcm2837-rpi-3-b.dtb</literal> file into one of the directories
    U-Boot searches on the second partition of the boot medium (U-Boot
    environment variable <literal>efi_dtb_prefixes</literal>). For example,
    <literal>/boot/dtb/bcm2837-rpi-3-b.dtb</literal> from within the system.
    This requires the second partition to be readable by U-Boot, hence in
    the <emphasis>SUSE Linux Enterprise Server for ARM 12 SP3 for the
    Raspberry Pi</emphasis> image that partition does not use Btrfs. The
    source package <literal>u-boot-rpi3</literal> includes the corresponding
    Device Tree sources.
   </para>
   <para>
    For convenience, you can also access the current Flat Device Tree binary
    as <literal>/sys/firmware/fdt</literal> and use the Device Tree Compiler
    tool (<literal>dtc</literal> which is not part of SUSE Linux Enterprise
    Server for ARM 12 SP3) to convert the Device Tree binary to source. To
    generate a suitable <literal>bcm2837-rpi-3-b.dtb</literal> file:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Modify the obtained sources as needed, adding or changing Device Tree
      nodes according to the Device Tree Bindings in the kernel
      documentation. Do not forget about <literal>pinctrl</literal>
      settings!
     </para>
    </listitem>
    <listitem>
     <para>
      Compile them into the Flat Device Tree binary format.
     </para>
    </listitem>
   </orderedlist>
   <note>
    <title>Procedural Changes</title>
    <para>
     This recommendation is expected to change for future versions.
    </para>
   </note>
   <bridgehead renderas="sect5">For More Information</bridgehead>
   <para>
    For more information on how to get started, see the SUSE Best Practices
    documentation for the Raspberry Pi at
    <link xlink:href="https://documentation.suse.com/sbp/all/html/SLES12SP3-rpiquick/index.html"/>.
   </para>
  </section>
  <section role="notoc" xml:id="fate-325671" remap="InfraPackArch:AArch64">


   <title>Raspberry Pi 3 Shows Blurry HDMI Output on Some Monitors</title>
   <para>
    <emphasis>On some HDMI monitors, the Raspberry Pi will show blurry
    output on the screen. You may also see a thin purple line at the edge of
    the screen.</emphasis>
   </para>
   <para>
    To work around this issue, perform the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Blacklist the <literal>vc4</literal> kernel module: Add the line
      <literal>blacklist vc4</literal> to
      <literal>/etc/modprobe.d/50-blacklist.conf</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Delete the kernel mode setting configuration: <literal>rm
      /etc/X11/xorg.conf.d/20-kms.conf</literal>
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section role="notoc" xml:id="fate-323585" remap="InfraPackArch:AArch64">


   <title>AppliedMicro X-C1 Server Development Platform (Mustang) Firmware Requirements</title>
   <para>
    <emphasis> In between SUSE Linux Enterprise Server 12 SP2 and SP3, some
    AppliedMicro X-Gene drivers and the corresponding Device Tree bindings
    were changed in an incompatible way. X-C1 devices that successfully boot
    SUSE Linux Enterprise Server 12 SP2 may be unable to install SP3 without
    changes. Symptoms include a crash in the
    </emphasis><literal>mdio-xgene</literal><emphasis> network driver.
    </emphasis>
   </para>
   <para>
    The updated X-Gene drivers in SP3 require the Device Tree provided by
    the vendor's firmware version 3.06.25 or later. To install SLES 12 SP3,
    first need to ensure that the AppliedMicro TianoCore bootloader firmware
    is updated according to the instructions provided by the vendor. For any
    questions about obtaining and upgrading this firmware, contact the
    hardware vendor.
   </para>
   <para>
    After updating the firmware to a new version, it may in turn no longer
    be possible to run SLES 12 SP2, unless the firmware is downgraded to a
    lower version again.
   </para>
  </section>
  <section role="notoc" xml:id="fate-323366" remap="InfraPackArch:AArch64">


   <title>New System-on-Chip Driver Enablement</title>
   <para>
    Drivers for the following additional System-on-Chip platforms have been
    enabled in the SP3 kernel:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      AppliedMicro X-Gene 3
     </para>
    </listitem>
    <listitem>
     <para>
      Cavium ThunderX2 CN99xx
     </para>
    </listitem>
    <listitem>
     <para>
      HiSilicon Hi1616
     </para>
    </listitem>
    <listitem>
     <para>
      Marvell Armada 7K/8K
     </para>
    </listitem>
    <listitem>
     <para>
      Qualcomm Centriq 2400 series
     </para>
    </listitem>
    <listitem>
     <para>
      Rockchip RK3399
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section role="notoc" xml:id="fate-321977" remap="InfraPackArch:AArch64">


   <title>Support for OpenDataPlane on Cavium ThunderX and Octeon TX Platforms</title>
   <para>
    This release supports OpenDataPlane (ODP) API version 1.11.0.0, also
    known as Monarch LTS.
   </para>
   <bridgehead renderas="sect5">Platform Compatilibility</bridgehead>
   <para>
    This release is compatible with the generic AArch64 platforms and the
    following Cavium platforms:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      ThunderX (CN88XX)
     </para>
    </listitem>
    <listitem>
     <para>
      Octeon TX (CN81XX, CN83XX)
     </para>
    </listitem>
   </itemizedlist>
   <bridgehead renderas="sect5">System Requirements</bridgehead>
   <para>
    Your system needs to meet certain requirements before ODP can be used.
    The general requirements are as follows:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>Cunit</literal> shared library in rootfs (for running unit
      tests and proper configure)
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>vfio</literal>, <literal>thunder-nicpf</literal> and BGX
      modules loaded into the kernel (typically no driver needs to be loaded
      since all mentioned modules are compiled into kernel image)
     </para>
    </listitem>
    <listitem>
     <para>
      Hugetlbfs must be mounted with considerable amount of pages added to
      it (minimum 256 MB of memory). ODP ThunderX uses huge pages for
      maximum performance by eliminating TLB misses. For some
      hardware-related cases, physically contiguous memory is needed.
      Therefore the ODP ThunderX memory allocator tries to allocate
      contiguous memory area.
     </para>
     <para>
      If the ODP application has startup problems, we recommend increasing
      the hugepage pool by adding more pages to the pool than required.
     </para>
    </listitem>
    <listitem>
     <para>
      NIC VFs need to be bound to the VFIO framework
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section role="notoc" xml:id="fate-321497" remap="InfraPackArch:AArch64">


   <title>KVM on AArch64</title>
   <tip role="compact">
    <para>
     This entry has appeared in a previous release notes document.
    </para>
   </tip>
   <para>
    KVM virtualization has been enabled and is supported on some
    system-on-chip platforms for mutually agreed-upon partner-specific use
    cases. It is only supported on partner certified hardware and firmware.
    Not all QEMU options and backends are available on AArch64. The same
    statement is applicable for other virtualization tools shipped on
    AArch64.
   </para>
  </section>
  <section role="notoc" xml:id="fate-320679" remap="InfraPackArch:AArch64">


   <title>Toolchain Module Enabled in Default Installation</title>
   <tip role="compact">
    <para>
     This entry has appeared in a previous release notes document.
    </para>
   </tip>
   <para>
    <emphasis> The system compiler (</emphasis><literal>gcc4.8</literal><emphasis>) is not supported on the
    AArch64 architecture. To work around this issue, you previously had to
    enable the Toolchain module manually and use the GCC version from that
    module. </emphasis>
   </para>
   <para>
    On AArch64, the Toolchain Module is now automatically pre-selected after
    registering SLES during installation. This makes the latest SLE
    compilers available on all installations. You now only need to make sure
    to also use that compiler.
   </para>
   <important>
    <title>When Using AutoYaST, Make Sure to Enable Toolchain Module</title>
    <para>
     Be aware that when using AutoYaST to install, you have to explicitly
     add the Toolchain module into the XML installation profile.
    </para>
   </important>
  </section>

 </section>
 <section xml:id="Packages">
  <title>Packages and Functionality Changes</title>
  <para>
   This section comprises changes to packages, such as additions, updates,
   removals and changes to the package layout of software. It also contains
   information about modules available for SUSE Linux Enterprise Server. For
   information about changes to package management tools, such as Zypper or
   RPM, see
   <xref linkend="InfraPackArch-ArchIndependent-SystemsManagement"/>.
  </para>
  <section xml:id="Packages-New" remap="Packages:New">
   <title>New Packages</title>
   <remark> *** Description of the new package with direct link to the official website (need the
    URL tag) *** Version and release of the package *** Why this new package: enhancements request
    *** link to the official documentation *** link to the SUSE's documentation *** Dependencies and
    Build requires </remark>
   <para/>

   <section role="notoc" xml:id="fate-324454" remap="Packages:New">


    <title>Icinga Monitoring Server Shipped as Part of SUSE Manager</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     Fully supported packages of the Icinga monitoring server for SUSE Linux
     Enterprise Server 12 are available with a SUSE Manager subscription.
     Icinga is compatible with a previously included monitoring server.
    </para>
   </section>

  </section>
  <section xml:id="Packages-Update" remap="Packages:Update">
   <title>Updated Packages</title>
   <remark> *** Description of the new package with direct link to the official website *** version
    comparison table from previous SP/product *** Bugs fixed since previous SP/Product (link to
    Bugzilla) *** Enhancements request (link to Fate number) *** link to the official documentation
    *** link to the SUSE's documentation *** Security fixes (link to CVE or SUSE's security web
    site), link to PIF *** Changelog diff (all other information non relative to security/fate/bugs) </remark>
   <para/>
   <section role="notoc" xml:id="jsc-SLE-11596" remap="Packages:Update">
    <title>LibreOffice Has Been Updated to Version 6.4</title>
    <para>
     LibreOffice has been updated to the new major version 6.4.
    </para>
   </section>

   <section role="notoc" xml:id="fate-325659" remap="Packages:Update">


    <title>PostgreSQL Has Been Upgraded to Version 10</title>
    <para>
     <emphasis>SLES 12 SP4 and SLES 15 ship with PostgreSQL 10 by default.
     To enable an upgrade path for customers, SLE 12 SP3 now includes
     PostgreSQL 10 in addition to PostgreSQL 9.6 (the version that was
     originally shipped).</emphasis>
    </para>
    <para>
     <emphasis>To upgrade a PostgreSQL server installation from an older
     version, the database files need to be converted to the new
     version.</emphasis>
    </para>
    <important>
     <title>PostgreSQL Upgrade Needs to Be Performed Before Upgrade to New SLES Version</title>
     <para>
      Neither SLES 12 SP4 nor SLES 15 include PostgreSQL 9.6. However,
      availability of PostgreSQL 9.6 is a requirement for performing the
      database upgrade to the PostgreSQL 10 format. Therefore, you must
      upgrade the database to the PostgreSQL 10 format before upgrading to
      the desired new SLES version.
     </para>
    </important>
    <bridgehead renderas="sect5">Major New Features</bridgehead>
    <para>
     The following major new features are included in PostgreSQL 10:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Logical replication: a publish/subscribe framework for distributing
       data
      </para>
     </listitem>
     <listitem>
      <para>
       Declarative table partitioning: convenience in dividing your data
      </para>
     </listitem>
     <listitem>
      <para>
       Improved query parallelism: speed up analyses
      </para>
     </listitem>
     <listitem>
      <para>
       Quorum commit for synchronous replication: distribute data with
       confidence
      </para>
     </listitem>
     <listitem>
      <para>
       SCRAM-SHA-256 authentication: more secure data access
      </para>
     </listitem>
    </itemizedlist>
    <para>
     PostgreSQL 10 also brings an important change to the versioning scheme
     that is used for PostgreSQL: It now follows the format
     <emphasis>major.minor</emphasis>. This means that minor releases of
     PostgreSQL 10 are for example 10.1, 10.2, ... and the next major
     release will be 11. Previously, both the parts of the version number
     were significant for the major version. For example, PostgreSQL 9.3 and
     PostgreSQL 9.4 were different major versions.
    </para>
    <para>
     For the full PostgreSQL 10 release notes, see
     <link xlink:href="https://www.postgresql.org/docs/10/release-10.html">https://www.postgresql.org/docs/10/release-10.html</link>.
    </para>
    <bridgehead renderas="sect5">Upgrading</bridgehead>
    <para>
     Before starting the migration, make sure the following preconditions
     are fulfilled:
    </para>
    <orderedlist>
     <listitem>
      <para>
       The packages of your current PostgreSQL version must have been
       upgraded to their latest maintenance update.
      </para>
     </listitem>
     <listitem>
      <para>
       The packages of the new PostgreSQL major version need to be
       installed. For SLE 12, this means installing
       <literal>postgresql10-server</literal> and all the packages it
       depends on. Because <literal>pg_upgrade</literal> is contained in the
       package <literal>postgresql10-contrib</literal>, this package must
       be installed as well, at least until the migration is done.
      </para>
     </listitem>
     <listitem>
      <para>
       Unless <literal>pg_upgrade</literal> is used in link mode, the server
       must have enough free disk space to temporarily hold a copy of the
       database files. If the database instance was installed in the default
       location, the needed space in megabytes can be determined by running
       the following command as <literal>root</literal>: <literal>du -hs
       /var/lib/pgsql/data</literal>. If there is little disk space
       available, run the command <literal>VACUUM FULL</literal> SQL command
       on each database in the PostgreSQL instance that you want to migrate.
       This command can take very long.
      </para>
     </listitem>
    </orderedlist>
    <para>
     Upstream documentation about <literal>pg_upgrade</literal> including
     step-by-step instructions for performing a database migration can be
     found locally at
     <literal>file:///usr/share/doc/packages/postgresql10/html/pgupgrade.html</literal>
     (if the <literal>postgresql10-docs</literal> package is installed), or
     online at
     <link xlink:href="https://www.postgresql.org/docs/10/pgupgrade.html">https://www.postgresql.org/docs/10/pgupgrade.html</link>. The online documentation explains how you can install PostgreSQL from
     the upstream sources (which is not necessary on SLE) and also uses
     other directory names (<literal>/usr/local</literal> instead of the
     <literal>update-alternatives</literal> based path as described above).
    </para>
   </section>
   <section role="notoc" xml:id="fate-322523" remap="Packages:Update">


    <title>GnuTLS Has Been Updated to Version 3.3</title>
    <para>
     <emphasis>Some programs require GnuTLS version 3.3 or newer to
     work.</emphasis>
    </para>
    <para>
     The upgrade from GnuTLS 3.2 to GnuTLS 3.3 is an update which does not
     change of the major version of <literal>libgnutls28</literal>, so
     existing programs will continue to work.
    </para>
    <para>
     The library <literal>libgnutls-xssl.so</literal> was not used by other
     programs and has been removed.
    </para>
   </section>
   <section role="notoc" xml:id="fate-322322" remap="Packages:Update">


    <title>Postfix Has Been Updated to Version 3.2.0</title>
    <para>
     <emphasis>Postfix version 2.x is going out of support in the near
     future.</emphasis>
    </para>
    <para>
     In SUSE Linux Enterprise 12 SP3, we have upgraded Postfix to version
     3.2.0 (from Postfix 2.11.8 in SUSE Linux Enterprise 12 SP2). For
     information about major changes in the new version of Postfix, see:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <link xlink:href="https://www.postfix.org/announcements/postfix-3.0.0.html"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <link xlink:href="https://www.postfix.org/announcements/postfix-3.1.0.html"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <link xlink:href="https://www.postfix.org/announcements/postfix-3.2.0.html"/>
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-321541" remap="Packages:Update">


    <title>Open vSwitch Has Been Updated to Version 2.7.0</title>
    <para>
     Open vSwitch has been updated to 2.7.0.
    </para>
    <para>
     Important changes include:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Various OpenFlow bug fixes
      </para>
     </listitem>
     <listitem>
      <para>
       Improved support for OpenFlow
      </para>
     </listitem>
     <listitem>
      <para>
       Support for new OpenFlow extensions
      </para>
     </listitem>
     <listitem>
      <para>
       Performance improvements
      </para>
     </listitem>
     <listitem>
      <para>
       Support for IPsec tunnels has been removed
      </para>
     </listitem>
     <listitem>
      <para>
       Changes relating to DPDK:
      </para>
      <itemizedlist>
        <listitem>
         <para>
          Support for DPDK 16.11
         </para>
        </listitem>
        <listitem>
         <para>
          Support for jumbo frames
         </para>
        </listitem>
        <listitem>
         <para>
          Support for rx checksum offload
         </para>
        </listitem>
        <listitem>
         <para>
          Support for port hotplugging
         </para>
        </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
    <para>
     For more detailed information about changes between version 2.6.0 and
     2.7.0, see
     <link xlink:href="https://github.com/openvswitch/ovs/blob/master/NEWS">https://github.com/openvswitch/ovs/blob/master/NEWS</link>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-319049" remap="Packages:Update">


    <title>Upgrading PostgreSQL Installations from 9.1 to 9.4</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis>To upgrade a PostgreSQL server installation from version 9.1
     to 9.4, the database files need to be converted to the new
     version.</emphasis>
    </para>
    <para>
     Note:<emphasis> System Upgrade from SLE 11 </emphasis>
    </para>
    <para>
     <emphasis>On SLE 12, there are no PostgreSQL 8.4 or 9.1 packages. This
     means you must first migrate PostgreSQL from 8.4 or 9.1 to 9.4 on SLE
     11 before upgrading the system from SLE 11 to SLE 12.</emphasis>
    </para>
    <para>
     Newer versions of PostgreSQL come with the
     <literal>pg_upgrade</literal> tool that simplifies and speeds up the
     migration of a PostgreSQL installation to a new version. Formerly, it
     was necessary to dump and restore the database files which was much
     slower.
    </para>
    <para>
     To work, <literal>pg_upgrade</literal> needs to have the server
     binaries of both versions available. To allow this, we had to change
     the way PostgreSQL is packaged as well as the naming of the packages,
     so that two or more versions of PostgreSQL can be installed in
     parallel.
    </para>
    <para>
     Starting with version 9.1, PostgreSQL package names on SUSE Linux
     Enterprise products contain numbers indicating the major version. In
     PostgreSQL terms, the major version consists of the first two
     components of the version number, for example, 9.1, 9.3, and 9.4. So,
     the packages for PostgreSQL 9.3 are named
     <literal>postgresql93</literal>,
     <literal>postgresql93-server</literal>, etc. Inside the packages, the
     files were moved from their standard location to a versioned location
     such as <literal>/usr/lib/postgresql93/bin</literal> or
     <literal>/usr/lib/postgresql94/bin</literal>. This avoids file
     conflicts if multiple packages are installed in parallel. The
     <literal>update-alternatives</literal> mechanism creates and maintains
     symbolic links that cause one version (by default the highest installed
     version) to re-appear in the standard locations. By default, database
     data is stored under <literal>/var/lib/pgsql/data</literal> on SUSE
     Linux Enterprise.
    </para>
    <para>
     The following preconditions have to be fulfilled before data migration
     can be started:
    </para>
    <orderedlist>
     <listitem>
      <para>
       If not already done, the packages of the old PostgreSQL version (9.3)
       must be upgraded to the latest release through a maintenance update.
      </para>
     </listitem>
     <listitem>
      <para>
       The packages of the new PostgreSQL major version need to be
       installed. For SLE 12, this means installing
       <literal>postgresql94-server</literal> and all the packages it
       depends on. Because <literal>pg_upgrade</literal> is contained in the
       package <literal>postgresql94-contrib</literal>, this package must
       be installed as well, at least until the migration is done.
      </para>
     </listitem>
     <listitem>
      <para>
       Unless <literal>pg_upgrade</literal> is used in link mode, the server
       must have enough free disk space to temporarily hold a copy of the
       database files. If the database instance was installed in the default
       location, the needed space in megabytes can be determined by running
       the following command as root: <literal>du -hs
       /var/lib/pgsql/data</literal>. If space is tight, it might help to
       run the <literal>VACUUM FULL</literal> SQL command on each database
       in the PostgreSQL instance to be migrated which might take very long.
      </para>
     </listitem>
    </orderedlist>
    <para>
     Upstream documentation about <literal>pg_upgrade</literal> including
     step-by-step instructions for performing a database migration can be
     found under
     <literal>file:///usr/share/doc/packages/postgresql94/html/pgupgrade.html</literal>
     (if the postgresql94-docs package is installed), or online under
     <link xlink:href="https://www.postgresql.org/docs/9.4/static/pgupgrade.html">https://www.postgresql.org/docs/9.4/static/pgupgrade.html</link>. The online documentation explains how you can install PostgreSQL from
     the upstream sources (which is not necessary on SLE) and also uses
     other directory names (<literal>/usr/local</literal> instead of the
     <literal>update-alternatives</literal> based path as described above).
    </para>
    <para>
     For background information about the inner workings of
     <literal>pg_admin</literal> and a performance comparison with the old
     dump and restore method, see
     <link xlink:href="https://momjian.us/main/writings/pgsql/pg_upgrade.pdf"/>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-313595" remap="Packages:Update">


    <title>MariaDB Replaces MySQL</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     MariaDB is a backward-compatible replacement for MySQL.
    </para>
    <para>
     If you update from SLE 11 to SLE 12 or later, it is advisable to do a
     manual backup before the system update. This can help if a start of the
     database has issues with the storage engine's on-disk layout.
    </para>
    <para>
     After the update to SLE 12 or later, a manual step is required to
     actually get the database running (this way you quickly see if
     something goes wrong):
    </para>
<screen>touch /var/lib/mysql/.force_upgrade
rcmysql start
# =&gt; redirecting to systemctl start mysql.service
rcmysql status
# =&gt; Checking for service MySQL:
# =&gt; ...</screen>
   </section>

  </section>
  <section xml:id="Packages-Deprecated" remap="Packages:Deprecated">
   <title>Removed and Deprecated Functionality</title>
   <remark>
    ** Why this choice?
    ** How to handle this deprecation in your system
    ** specific case (LTSS etc.)
   </remark>

   <section role="notoc" xml:id="fate-323093" remap="Packages:Deprecated">


    <title>libcgroup1 Removed From SLE 12 SP4 and Later</title>
    <para>
     <emphasis> Most functionality of
     </emphasis><literal>libcgroup1</literal><emphasis> is also provided by
     systemd. In fact, the cgroup handling of
     </emphasis><literal>libcgroup1</literal><emphasis> can conflict with
     that of systemd. </emphasis>
    </para>
    <para>
     Starting with SLE 12 SP4, <literal>libcgroup1</literal> has been
     removed. Migrate to the equivalent functionality in systemd.
    </para>
    <para>
     For more information, see
     <link xlink:href="https://support.scc.suse.com/s/kb/How-to-migrate-from-libcgroup-resource-control-setting-to-systemd-unit-setting-158323943236"/>.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320740" remap="Packages:Deprecated">


    <title>Docker Compose Has Been Removed from the Containers Module</title>
    <para>
     <emphasis>Docker Compose is not supported as a part of SUSE Linux
     Enterprise Server 12. While it was temporarily included as a Technology
     Preview, testing showed that the technology was not ready for
     enterprise use.</emphasis>
    </para>
    <para>
     SUSE's focus is on Kubernetes which provides better value in terms of
     features, extensibility, stability and performance.
    </para>
   </section>
   <section role="notoc" xml:id="fate-316136" remap="Packages:Deprecated">


    <title>Nagios Monitoring Server Has Been Removed</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     The Nagios monitoring server has been removed from SLES 12.
    </para>
    <para>
     When upgrading to SLES 12 or later, installed Nagios configuration may
     be removed. Therefore, we recommend creating backups of the Nagios
     configuration before the upgrade.
    </para>
   </section>

   <section xml:id="Packages-Deprecated-Future" remap="Packages:Deprecated:Future">
    <title>Packages and Features to Be Removed in the Future</title>
    <para/>

    <section role="notoc" xml:id="fate-316268" remap="Packages:Deprecated:Future">


     <title>Use /etc/os-release Instead of /etc/SuSE-release</title>
     <tip role="compact">
      <para>
       This entry has appeared in a previous release notes document.
      </para>
     </tip>
     <para>
      <emphasis> Starting with SLE 12, the
      </emphasis><literal>/etc/SuSE-release</literal><emphasis> file has
      been deprecated. Do not use it to identify a SUSE Linux Enterprise
      system anymore. This file will be removed in a future Service Pack or
      release. </emphasis>
     </para>
     <para>
      To determine the release, use the file
      <literal>/etc/os-release</literal> instead. This file is a
      cross-distribution standard to identify Linux systems. For more
      information about the syntax, see the <literal>os-release</literal>
      man page (<literal>man os-release</literal>).
     </para>
    </section>

   </section>
  </section>
  <section xml:id="Packages-Packaging" remap="Packages:Packaging">
   <title>Changes in Packaging and Delivery</title>
   <para/>

   <section role="notoc" xml:id="fate-322112" remap="Packages:Packaging">


    <title>OFED-related Packages Replaced by Packages From New Upstream</title>
    <para>
     <emphasis>In SLE 12 SP2 and earlier, the OFED (OpenFabric Enterprise
     Distribution) stack came directly from OFED.</emphasis>
    </para>
    <para>
     <emphasis>Since the release of SLES 12 SP2, most of this stack has been
     upstreamed to the Linux RDMA project. This has resulted in an influx of
     contributions to the project and much improved source.</emphasis>
    </para>
    <para>
     With SLE 12 SP3, we have updated the OFED stack to the version from the
     new upstream. This has brought the following package changes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The package <literal>rdma</literal> is now called
       <literal>rdma-core</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       All <literal>-rdmav2</literal> (providers for specific RDMA hardware)
       are integrated into the <literal>libibverbs</literal> package
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libibverbs</literal> itself is in the
       <literal>libibverbs1</literal> package
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mlx4</literal> and <literal>mlx5</literal> are still shipped
       as separate packages, under the name <literal>libmlx4-1</literal> and
       <literal>libmlx5-1</literal>, as they can be used standalone
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libibcm-devel</literal>, <literal>libibumad-devel</literal>, <literal>librdmacm-devel</literal> and
       <literal>libibverbs-devel</literal> are all provided by the
       <literal>rdma-core-devel</literal> package
      </para>
     </listitem>
     <listitem>
      <para>
       The static libraries are not provided anymore.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section role="notoc" xml:id="fate-319240" remap="Packages:Packaging">


    <title>Support for Intel OPA Fabrics Moved to mvapich2-psm2 Package</title>
    <tip role="compact">
     <para>
      This entry has appeared in a previous release notes document.
     </para>
    </tip>
    <para>
     <emphasis> The version of the package
     </emphasis><literal>mvapich2-psm</literal><emphasis> originally shipped
     with SLES 12 SP2 and SLES 12 SP3 exclusively supported Intel Omni-Path
     Architecture (OPA) fabrics. In SLES 12 SP1 and earlier, this package
     supported the use of Intel True Scale fabrics instead. </emphasis>
    </para>
    <para>
     This issue is fixed by a maintenance update providing an additional
     package named <literal>mvapich2-psm2</literal> which only supports
     Intel OPA, whereas the original package <literal>mvapich2-psm</literal>
     only supports Intel True Scale fabrics again.
    </para>
    <para>
     If you are currently using <literal>mvapich2-psm</literal> together
     with Intel OPA fabrics, make sure to switch to the new package
     <literal>mvapich2-psm2</literal> after this maintenance update.
    </para>
   </section>


   <section role="notoc" xml:id="bsc-1143465">
    <title>Kernel Firmware Only Shipped as Part of the <package>kernel-firmware</package> Package</title>
    <para>
     In past releases, the <package>kernel-default</package> package used to
     contain firmware for in-kernel drivers.
    </para>
    <para>
     Starting with SLES 12 SP3, such firmware is now delivered as part of the
     package <package>kernel-firmware</package>.
    </para>
   </section>

  </section>
  <section xml:id="Packages-Modules" remap="Packages:Modules">
   <title>Modules</title>
   <para>
    This section contains information about important changes to modules.
    For more information about available modules, see
    <xref linkend="Intro-Module"/>.
   </para>

   <section role="notoc" xml:id="fate-321496-1" remap="InfraPackArch:SystemZ:Hardware,Packages:Modules">


    <title>Support for New Hardware Instructions in Toolchain</title>
    <para>
     Support for new hardware instructions in <literal>binutils</literal>,
     GCC and GDB is available through the Toolchain Module.
    </para>
   </section>
   <section role="notoc" xml:id="fate-320852" remap="Packages:Modules">


    <title>libgcrypt11 Available from the Legacy Module</title>
    <para>
     The Legacy module now provides a package for
     <literal>libgcrypt11</literal>. This enables running applications
     built on SLES 11 against <literal>libgcrypt11</literal> on SLES 12.
    </para>
   </section>

  </section>
 </section>
 <section xml:id="TechInfo">
  <title>Technical Information</title>
  <para>
   This section contains information about system limits, a number of
   technical changes and enhancements for the experienced user.
  </para>

  <para>
   When talking about CPUs, we use the following terminology:
  </para>
  <variablelist>
   <varlistentry>
    <term>CPU Socket</term>
    <listitem>
     <para>
      The visible physical entity, as it is typically mounted to a
      motherboard or an equivalent.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CPU Core</term>
    <listitem>
     <para>
      The (usually not visible) physical entity as reported by the CPU
      vendor.
     </para>
     <para>
      On IBM Z, this is equivalent to an IFL.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logical CPU</term>
    <listitem>
     <para>
      This is what the Linux Kernel recognizes as a "CPU".
     </para>
     <para>
      We avoid the word "thread" (which is sometimes used), as the word
      "thread" would also become ambiguous subsequently.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Virtual CPU</term>
    <listitem>
     <para>
      A logical CPU as seen from within a Virtual Machine.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <section xml:id="TechInfo-Kernel">
   <title>Kernel Limits</title>
   <para>
    This table summarizes the various limits which exist in our recent
    kernels and utilities (if related) for SUSE Linux Enterprise Server 12
    SP3.
   </para>
   <informaltable frame="all">
    <tgroup cols="5">
     <colspec colnum="1" colname="feature"/>
     <colspec colnum="2" colname="x86-64"/>
     <colspec colnum="3" colname="z"/>
     <colspec colnum="4" colname="power"/>
     <colspec colnum="5" colname="aarch64"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP3 (Linux 4.4)</emphasis>
       </entry>
       <entry>AMD64/Intel 64 (x86_64)</entry>
       <entry>IBM Z (s390x)</entry>
       <entry>POWER (ppc64le)</entry>

       <entry>AArch64 (ARMv8)</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         CPU bits
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of logical CPUs
        </para>
       </entry>

       <entry>
        <para>
         8192
        </para>
       </entry>
       <entry>
        <para>
         256
        </para>
       </entry>

       <entry>
        <para>
         2048
        </para>
       </entry>
       <entry>
        <para>
         128
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of RAM (theoretical/certified)
        </para>
       </entry>
       <entry>
        <para>
         &gt; 1 PiB/64 TiB
        </para>
       </entry>
       <entry>
        <para>
         10 TiB/256 GiB
        </para>
       </entry>

       <entry>
        <para>
         1 PiB/64 TiB
        </para>
       </entry>
       <entry>
        <para>
         256 TiB/n.a.
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of user space/kernel space
        </para>
       </entry>
       <entry>
        <para>
         128 TiB/128 TiB
        </para>
       </entry>
       <entry>
        <para>
         n.a.
        </para>
       </entry>
       <entry>
        <para>
         512 TiB <superscript>1</superscript>/2 EiB
        </para>
       </entry>
       <entry>
        <para>
         256 TiB/128 TiB
        </para>
       </entry>
      </row>
      <row>
       <entry>

        <para>
         Maximum amount of swap space
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         Up to 29 * 64 GB (x86_64) or 30 * 64 GB (other architectures)
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of processes
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         1048576
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of threads per process
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         Upper limit depends on memory and other parameters (tested with
         more than 120,000)<superscript>2</superscript>
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum size per block device
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         Up to 8 EiB
        </para>
       </entry>
      </row>
      <row>

       <entry>
        <para>
         FD_SETSIZE
        </para>
       </entry>
       <entry namest="x86-64" nameend="aarch64">
        <para>
         1024
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <para>
    <emphasis role="bold"><superscript>1</superscript></emphasis> By
    default, the userspace memory limit on the POWER architecture is
    128 TiB. However, you can explicitly request mmaps up to 512 TiB.
   </para>
   <para>
    <emphasis role="bold"><superscript>2</superscript></emphasis> The total
    number of all processes and all threads on a system may not be higher
    than the <quote>maximum number of processes</quote>.
   </para>
  </section>
  <section xml:id="TechInfo-KVM">
   <title>KVM Limits</title>
   <informaltable frame="all">
    <tgroup cols="2">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP3 Virtual Machine (VM)</emphasis>
       </entry>
       <entry>Limits</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Maximum VMs per host
        </para>
       </entry>
       <entry>
        <para>
         Unlimited (total number of virtual CPUs in all guests being no
         greater than 8 times the number of CPU cores in the host)
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum Virtual CPUs per VM
        </para>
       </entry>
       <entry>
        <para>
         288
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum Memory per VM
        </para>
       </entry>
       <entry>
        <para>
         4 TiB
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <para>
    Virtual Host Server (VHS) limits are identical to those of SUSE Linux
    Enterprise Server.
   </para>
  </section>
  <section xml:id="TechInfo-XEN">
   <title>Xen Limits</title>
   <para>
    Since SUSE Linux Enterprise Server 11 SP2, we removed the 32-bit
    hypervisor as a virtualization host. 32-bit virtual guests are not
    affected and are fully supported with the provided 64-bit hypervisor.
   </para>

   <informaltable frame="all">
    <tgroup cols="2">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP3 Virtual Machine (VM)</emphasis>
       </entry>
       <entry>Limits</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Maximum number of virtual CPUs per VM
        </para>
       </entry>
       <entry>
        <para>
         64
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of memory per VM
        </para>
       </entry>
       <entry>
        <para>
         16 GiB x86_32, 511 GiB x86_64
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <informaltable frame="all">
    <tgroup cols="2">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <thead>
      <row>
       <entry><emphasis>SLES 12 SP3 Virtual Host Server (VHS)</emphasis>
       </entry>
       <entry>Limits</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>
        <para>
         Maximum number of physical CPUs
        </para>
       </entry>
       <entry>
        <para>
         256
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of virtual CPUs
        </para>
       </entry>
       <entry>
        <para>
         256
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of physical memory
        </para>
       </entry>
       <entry>
        <para>
         5 TiB
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum amount of Dom0 physical memory
        </para>
       </entry>
       <entry>
        <para>
         500 GiB
        </para>
       </entry>
      </row>
      <row>
       <entry>
        <para>
         Maximum number of block devices
        </para>
       </entry>
       <entry>
        <para>
         12,000 SCSI logical units
        </para>
       </entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
   <itemizedlist>
    <listitem>
     <formalpara>
      <title>PV:</title>
      <para>
       Paravirtualization
      </para>
     </formalpara>
    </listitem>
    <listitem>
     <formalpara>
      <title>FV:</title>
      <para>
       Full virtualization
      </para>
     </formalpara>
    </listitem>
   </itemizedlist>
   <para>
    For more information about acronyms, see the virtualization
    documentation provided at
    <link xlink:href="https://documentation.suse.com/sles/12-SP3/"/>.
   </para>
  </section>
  <section xml:id="TechInfo-Filesystems" remap="TechInfo:Filesystems">
   <title>File Systems</title>

   <section role="notoc" xml:id="fate-325367" remap="TechInfo:Filesystems">


    <title>Unsupported Ext4 Features</title>
    <para>
     The following Ext4 features are experimental and unsupported:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       bigalloc
      </para>
     </listitem>
     <listitem>
      <para>
       metadata checksumming
      </para>
     </listitem>
    </itemizedlist>
   </section>

   <section xml:id="TechInfo-Filesystems-Comparison">
    <title>Comparison of Supported File Systems</title>
    <para>
     SUSE Linux Enterprise was the first enterprise Linux distribution to
     support journaling file systems and logical volume managers back in
     2000. Later, we introduced XFS to Linux, which today is seen as the
     primary work horse for large-scale file systems, systems with heavy
     load and multiple parallel reading and writing operations. With SUSE
     Linux Enterprise 12, we went the next step of innovation and started
     using the copy-on-write file system Btrfs as the default for the
     operating system, to support system snapshots and rollback.
    </para>

    <simplelist type="vert">
      <member><emphasis role="bold">+</emphasis> supported</member>
      <member><emphasis role="bold">–</emphasis> unsupported</member>
    </simplelist>
    <informaltable frame="all">
     <tgroup cols="6">
      <colspec colnum="1" colname="c1" colwidth="28*"/>
      <colspec colnum="2" colname="c2" colwidth="18*"/>
      <colspec colnum="3" colname="c3" colwidth="18*"/>
      <colspec colnum="4" colname="c4" colwidth="18*"/>
      <colspec colnum="5" colname="c5" colwidth="18*"/>
      <colspec colnum="6" colname="c6" colwidth="18*"/>
      <thead>
       <row>
        <entry>Feature</entry>
        <entry>Btrfs</entry>
        <entry>XFS</entry>
        <entry>Ext4</entry>
        <entry>OCFS 2 <superscript>1</superscript>
        </entry>
        <entry>ReiserFS <superscript>2</superscript>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          Support in products
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
        <entry>
         <para>
          SLE HA
         </para>
        </entry>
        <entry>
         <para>
          SLE
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Data/metadata journaling
         </para>
        </entry>
        <entry>
         <para>
          N/A <superscript>3</superscript>
         </para>
        </entry>
        <entry>
         <para>
          – / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          – / +
         </para>
        </entry>
        <entry>
         <para>
          – / +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Journal internal/external
         </para>
        </entry>
        <entry>
         <para>
          N/A <superscript>3</superscript>
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Journal checksumming
         </para>
        </entry>
        <entry>
         <para>
          N/A <superscript>3</superscript>
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Subvolumes
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Offline extend/shrink
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          – / –
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / – <superscript>4</superscript>
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Online extend/shrink
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
        <entry>
         <para>
          – / –
         </para>
        </entry>
        <entry>
         <para>
          + / –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Inode allocation map
         </para>
        </entry>
        <entry>
         <para>
          B-tree
         </para>
        </entry>
        <entry>
         <para>
          B+-tree
         </para>
        </entry>
        <entry>
         <para>
          table
         </para>
        </entry>
        <entry>
         <para>
          B-tree
         </para>
        </entry>
        <entry>
         <para>
          u. B*-tree
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Sparse files
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Tail packing
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Small files stored inline
         </para>
        </entry>
        <entry>
         <para>
          + (in metadata)
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          + (in inode)
         </para>
        </entry>
        <entry>
         <para>
          + (in inode)
         </para>
        </entry>
        <entry>
         <para>
          + (in metadata)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Defragmentation
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Extended file attributes/ACLs
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          User/group quotas
         </para>
        </entry>
        <entry>
         <para>
          – / –
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
        <entry>
         <para>
          + / +
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Project quotas
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Subvolume quotas
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          N/A
         </para>
        </entry>
        <entry>
         <para>
          N/A
         </para>
        </entry>
        <entry>
         <para>
          N/A
         </para>
        </entry>
        <entry>
         <para>
          N/A
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Data dump/restore
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          +
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
        <entry>
         <para>
          –
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Block size default
         </para>
        </entry>
        <entry namest="c2" nameend="c6">
         <para>
          4 KiB <superscript>5</superscript>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Maximum file system size
         </para>
        </entry>
        <entry>
         <para>
          16 EiB
         </para>
        </entry>
        <entry>
         <para>
          8 EiB
         </para>
        </entry>
        <entry>
         <para>
          1 EiB
         </para>
        </entry>
        <entry>
         <para>
          4 PiB
         </para>
        </entry>
        <entry>
         <para>
          16 TiB
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Maximum file size
         </para>
        </entry>
        <entry>
         <para>
          16 EiB
         </para>
        </entry>
        <entry>
         <para>
          8 EiB
         </para>
        </entry>
        <entry>
         <para>
          1 EiB
         </para>
        </entry>
        <entry>
         <para>
          4 PiB
         </para>
        </entry>
        <entry>
         <para>
          1 EiB
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
    <para>
     <emphasis role="bold"><superscript>1</superscript></emphasis> OCFS 2 is
     fully supported as part of the SUSE Linux Enterprise High Availability
     Extension.
    </para>
    <para>
     <emphasis role="bold"><superscript>2</superscript></emphasis> ReiserFS
     is supported for existing file systems. The creation of new ReiserFS
     file systems is discouraged.
    </para>
    <para>
     <emphasis role="bold"><superscript>3</superscript></emphasis> Btrfs is
     a copy-on-write file system. Instead of journaling changes before
     writing them in-place, it writes them to a new location and then links
     the new location in. Until the last write, the changes are not
     <quote>committed</quote>. Because of the nature of the file system,
     quotas are implemented based on subvolumes
     (<literal>qgroups</literal>).
    </para>
    <para>
     <emphasis role="bold"><superscript>4</superscript></emphasis> To extend
     an OCFS 2 file system, the cluster must be online but the file system
     itself must be unmounted.
    </para>
    <para>
     <emphasis role="bold"><superscript>5</superscript></emphasis> The block
     size default varies with different host architectures. 64 KiB is used
     on POWER, 4 KiB on other systems. The actual size used can be checked
     with the command <command>getconf</command> <replaceable>PAGE_SIZE</replaceable>.
    </para>
    <bridgehead renderas="sect5">Additional Notes</bridgehead>
    <para>
     Maximum file size above can be larger than the file system's actual
     size because of the use of sparse blocks. All standard file systems on
     SUSE Linux Enterprise Server have LFS, which gives a maximum file size
     of 2<superscript>63</superscript> bytes in theory.
    </para>
    <para>
     The numbers in the above table assume that the file systems are using a
     4 KiB block size which is the most common standard. When using
     different block sizes, the results are different.
    </para>
    <para>
     In this document: 1024 Bytes = 1 KiB; 1024 KiB = 1 MiB; 1024 MiB =
     1 GiB; 1024 GiB = 1 TiB; 1024 TiB = 1 PiB; 1024 PiB = 1 EiB. See
     also <link xlink:href="https://physics.nist.gov/cuu/Units/binary.html"/>.
    </para>
    <para>

     NFSv4 with IPv6 is only supported for the client side. An NFSv4 server
     with IPv6 is not supported.
    </para>
    <para>

     The version of Samba shipped with SUSE Linux Enterprise Server 12 SP3
     delivers integration with Windows Active Directory domains. In
     addition, we provide the clustered version of Samba as part of SUSE
     Linux Enterprise High Availability Extension 12 SP3.
    </para>
    <para>

     Some file system features are available in SUSE Linux Enterprise Server
     12 SP3 but are not supported by SUSE. By default, the file system
     drivers in SUSE Linux Enterprise Server 12 SP3 will refuse mounting
     file systems that use unsupported features (in particular, in
     read-write mode). To enable unsupported features, set the module
     parameter <literal>allow_unsupported=1</literal> in
     <filename>/etc/modprobe.d</filename> or write the value
     <literal>1</literal> to
     <filename>/sys/module/<replaceable>MODULE_NAME</replaceable>/parameters/allow_unsupported</filename>.
     However, note that setting this option will render your kernel and thus
     your system unsupported.
    </para>
   </section>
   <section xml:id="TechInfo-Filesystems-Btrfs">
    <title>Supported Btrfs Features</title>
    <para>
     The following table lists supported and unsupported Btrfs features
     across multiple SLES versions.
    </para>
    <simplelist type="vert">
      <member><emphasis role="bold">+</emphasis> supported</member>
      <member><emphasis role="bold">–</emphasis> unsupported</member>
    </simplelist>
    <informaltable>
     <tgroup cols="6">
      <colspec colnum="1" colname="feature" colwidth="6.5cm"/>
      <colspec colnum="2" colname="11SP4" colwidth="*"/>
      <colspec colnum="3" colname="12GA" colwidth="*"/>
      <colspec colnum="4" colname="12SP1" colwidth="*"/>
      <colspec colnum="5" colname="12SP2" colwidth="*"/>
      <colspec colnum="6" colname="12SP3" colwidth="*"/>
      <thead>
       <row>

        <entry>Feature</entry>
        <entry>SLES 11 SP4</entry>
        <entry>SLES 12 GA</entry>
        <entry>SLES 12 SP1</entry>
        <entry>SLES 12 SP2</entry>
        <entry>SLES 12 SP3</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>Copy on Write</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Snapshots/Subvolumes</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Metadata Integrity</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Data Integrity</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Online Metadata Scrubbing</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Automatic Defragmentation</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Manual Defragmentation</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>In-band Deduplication</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Out-of-band Deduplication</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Quota Groups</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Metadata Duplication</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Multiple Devices</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 0</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 1</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 10</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>RAID 5</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>RAID 6</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Hot Add/Remove</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Device Replace</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Seeding Devices</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Compression</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Big Metadata Blocks</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Skinny Metadata</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Send Without File Data</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Send/Receive</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
       <row>
        <entry>Inode Cache</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
       </row>
       <row>
        <entry>Fallocate with Hole Punch</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>–</entry>
        <entry>+</entry>
        <entry>+</entry>
       </row>
      </tbody>
     </tgroup>
    </informaltable>
   </section>
  </section>
  <section xml:id="TechInfo-Java">
   <title>Supported Java Versions</title>
   <para>
    The following table lists Java implementations available in SUSE Linux
    Enterprise Server 12 SP3:
   </para>
   <informaltable>
    <tgroup cols="4">
     <colspec colnum="1" colname="c1"/>
     <colspec colnum="2" colname="c2"/>
     <colspec colnum="3" colname="c3"/>
     <colspec colnum="4" colname="c4"/>
     <thead>
      <row>
       <entry>Name (Package Name)</entry>
       <entry>Version</entry>
       <entry>Part of SUSE Linux Enterprise Server</entry>
       <entry>Support</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>OpenJDK (<package>java-1_8_0-openjdk</package>)</entry>
       <entry>1.8.0</entry>
       <entry>SLES</entry>
       <entry>SUSE, L3</entry>
      </row>
      <row>
       <entry>OpenJDK (<package>java-1_7_0-openjdk</package>)</entry>
       <entry>1.7.0</entry>
       <entry>SLES</entry>
       <entry>SUSE, L3</entry>
      </row>
      <row>
       <entry>IBM Java (<package>java-1_8_0-ibm</package>)</entry>
       <entry>1.8.0</entry>
       <entry>SLES</entry>
       <entry>External only</entry>
      </row>
      <row>
       <entry>IBM Java (<package>java-1_7_1-ibm</package>)</entry>
       <entry>1.7.1</entry>
       <entry>SLES</entry>
       <entry>External only</entry>
      </row>
      <row>
       <entry>IBM Java (<package>java-1_6_0-ibm</package>)</entry>
       <entry>1.6.0</entry>
       <entry>Legacy Module</entry>
       <entry>External only</entry>
      </row>
     </tbody>
    </tgroup>
   </informaltable>
  </section>
 </section>


 <section xml:id="Legal">
  <title>Legal Notices</title>
  <para>
   SUSE makes no representations or warranties with respect to the contents
   or use of this documentation, and specifically disclaims any express or
   implied warranties of merchantability or fitness for any particular
   purpose. Further, SUSE reserves the right to revise this publication and
   to make changes to its content, at any time, without the obligation to
   notify any person or entity of such revisions or changes.
  </para>
  <para>
   Further, SUSE makes no representations or warranties with respect to any
   software, and specifically disclaims any express or implied warranties of
   merchantability or fitness for any particular purpose. Further, SUSE
   reserves the right to make changes to any and all parts of SUSE software,
   at any time, without any obligation to notify any person or entity of
   such changes.
  </para>
  <para>
   Any products or technical information provided under this Agreement may
   be subject to U.S. export controls and the trade laws of other countries.
   You agree to comply with all export control regulations and to obtain any
   required licenses or classifications to export, re-export, or import
   deliverables. You agree not to export or re-export to entities on the
   current U.S. export exclusion lists or to any embargoed or terrorist
   countries as specified in U.S. export laws. You agree to not use
   deliverables for prohibited nuclear, missile, or chemical/biological
   weaponry end uses. Refer to
   <link xlink:href="https://www.suse.com/company/legal/"/> for more information on
   exporting SUSE software. SUSE assumes no responsibility for your failure
   to obtain any necessary export approvals.
  </para>
  <para>
   Copyright © 2010-
<?dbtimestamp format="Y" ?>
   SUSE LLC. This release notes document is licensed under a Creative
   Commons Attribution-NoDerivs 3.0 United States License (CC-BY-ND-3.0 US,
   <link xlink:href="https://creativecommons.org/licenses/by-nd/3.0/us/"/>).
  </para>
  <para>
   SUSE has intellectual property rights relating to technology embodied in
   the product that is described in this document. In particular, and
   without limitation, these intellectual property rights may include one or
   more of the U.S. patents listed at
   <link xlink:href="https://www.suse.com/company/legal/"/> and one or more
   additional patents or pending patent applications in the U.S. and other
   countries.
  </para>
  <para>
   For SUSE trademarks, see SUSE Trademark and Service Mark list
   (<link xlink:href="https://www.suse.com/company/legal/"/>). All third-party
   trademarks are the property of their respective owners.
  </para>
 </section>
 <section>
  <title>Colophon</title>
  <para>
   Thanks for using SUSE Linux Enterprise Server in your business.
  </para>
  <para>
   The SUSE Linux Enterprise Server Team.
  </para>
 </section>
</article>
