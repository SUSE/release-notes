==== Important Notes for Upgrading Slurm Releases:

If using the slurmdbd (Slurm DataBase Daemon) you must update this first.
If using a backup DBD you must start the primary first to do any
database conversion, the backup will not start until this has happened.

[#jsc-SLE-21334]
==== Slurm version 22.05

An update to `Slurm` version 22.05 is available.

===== Important notes for upgrading to version 22.05

Slurmdbd version 22.05 will work Slurm daemons of version 20.11.
You will not need to update all clusters at the same time, but it is very
important to update slurmdbd first and having it running before updating
any other clusters making use of it.

Slurm can be upgraded from version 20.11 to version 22.05 without loss of jobs
or other state information. Upgrading directly from an earlier version
of Slurm will result in loss of state information.

For more information and a recommended upgrade procedure, see the section
"Upgrading Slurm" in the chapter "Slurm â€” utility for HPC workload management"
of the in the SLE HPC 15 "Administration Guide".

All SPANK plugins must be recompiled when upgrading from any Slurm version
prior to 22.05.

If you are using the Slurm plugin for pdsh you must make sure,
`pdsh_slurm_22_05` is installed together with slurm_22_05.

===== Highlights of version  20.11

* The template `slurmrestd.service` unit file now defaults to listen on both the
  Unix socket and the `slurmrestd` port.
* The template `slurmrestd.service` unit file now defaults to enable auth/jwt
  and the munge unit is no longer a dependency by default.
* Add extra '`EnvironmentFile=-/etc/default/$service`' setting to service files.
* Allow jobs to pack onto nodes already rebooting with the desired features.
* Reset job start time after nodes are rebooted, previously only done for
    cloud/power save boots.
* Node features (if any) are passed to `RebootProgram` if run from `slurmctld`.
* Fail srun when using invalid `--cpu-bind options` (e.g. `--cpu-bind=map_cpu:99`
  when only 10 CPUs are allocated).
* Storing batch scripts and env vars are now in indexed tables using
  substantially less disk space.  Those storing scripts in 21.08 will all
  be moved and indexed automatically.
* Run `MailProg` through `slurmscriptd` instead of directly fork+exec()'ing
  from `slurmctld`.
* Add `acct_gather_interconnect/sysfs` plugin.
* Future and Cloud nodes are treated as "Planned Down" in usage reports.
* Add new shard plugin for sharing GPUs but not with mps.
* Add support for Lenovo SD650 V2 in ``acct_gather_energy/xcc`` plugin.
* Remove `cgroup_allowed_devices_file.conf`, since the default policy in modern
  kernels is to whitelist by default. Denying specific devices must be done
  through `gres.conf`.
* Node state flags (`DRAIN`, `FAILED`, `POWERING UP`, etc.) will be cleared now if
  node state is updated to `FUTURE`.
* `srun` will no longer read in `SLURM_CPUS_PER_TASK`. This means you will
  implicitly have to specify `--cpus-per-task` on your `srun` calls, or set the
  new `SRUN_CPUS_PER_TASK` environment variable to accomplish the same thing.
* Remove `connect_timeout` and timeout options from `JobCompParams` as there's no
  longer a connectivity check happening in the `jobcomp/elasticsearch` plugin
  when setting the location off of `JobCompLoc`.
* Add support for hourly reoccurring reservations.
* Allow nodes to be dynamically added and removed from the system. Configure
  ``MaxNodeCount`` to accomodate nodes created with dynamic node registrations
  (`slurmd -Z --conf=""`) and `scontrol`.
* Added support for Cgroup Version 2.
* `sacct` - allocations made by `srun` will now always display the allocation and
  step(s). Previously, the allocation and step were combined when possible.
* `cons_tres` - change definition of the "least loaded node" (LLN) to the node
  with the greatest ratio of available CPUs to total CPUs.
* Add support to ship `Include` configuration files with configless.
* Provide a detailed reason in the job log as to why it has been terminated
  when hitting a resource limit.
* Pass and use `alias_list` through credential instead of environment variable.
* Add ability to get host addresses from `nss_slurm`.
* Enable reverse fanout for `cloud+alias_list jobs`.
* Add support to delete/update nodes by specifying nodesets or the 'ALL'
  keyword alongside the delete/update node message nodelist expression (i.e.
  `scontrol delete/update NodeName=ALL` or `scontrol delete/update NodeName=ns1,nodes[1-3]`).
* Expanded the set of environment variables accessible through Prolog/Epilog
  and `PrologSlurmctld`/`EpilogSlurmctld` to include `SLURM_JOB_COMMENT`,
  `SLURM_JOB_STDERR`, `SLURM_JOB_STDIN`, `SLURM_JOB_STDOUT`, `SLURM_JOB_PARTITION`,
  `SLURM_JOB_ACCOUNT`, `SLURM_JOB_RESERVATION`, `SLURM_JOB_CONSTRAINTS`,
  `SLURM_JOB_NUM_HOSTS`, `SLURM_JOB_CPUS_PER_NODE`, `SLURM_JOB_NTASKS`, and
  `SLURM_JOB_RESTART_COUNT`.
* Attempt to requeue jobs terminated by `slurm.conf` changes (node vanish, node
  socket/core change, etc). Processes may still be running on excised nodes.
  Admin should take precautions when removing nodes that have jobs on running
  on them.
* Add `switch/hpe_slingshot` plugin.
* Add new `SchedulerParameters` option `bf_licenses` to track licenses as
  within the backfill scheduler.

===== Configureation File changes (for details, see the appropriate man page)

* `AcctGatherEnergyType` `rsmi` is now `gpu`.
* `TaskAffinity` parameter was removed from `cgroup.conf`.
* Fatal if the mutually-exclusive `JobAcctGatherParams` options of `UsePss` and
  `NoShared` are both defined.
* `KeepAliveTime` has been moved into `CommunicationParameters`. The standalone
    option will be removed in a future version.
* `preempt/qos` - add support for WITHIN mode to allow for preemption between
  jobs within the same QOS.
* Fatal error if `CgroupReleaseAgentDir` is configured in `cgroup.conf`. The
  option has long been obsolete.
* Fatal if more than one burst buffer plugin is configured.
* Added `keepaliveinterval` and `keepaliveprobes` to `CommunicationParameters`.
* Added `new max_token_lifespan=<seconds>` to `AuthAltParameters` to allow sites
  to restrict the lifespan of any requested ticket by an unprivileged user.
* Disallow `slurm.conf` node configurations with `NodeName=ALL`.

===== Command Changes (for details, see the appropriate man page)

* Remove support for (non-functional) `--cpu-bind=boards`.
* Added `--prefer` option at job submission to allow for 'soft' constraints.
* Add `condflags=open` to sacctmgr show events to return open/currently down
  events.
* `sacct -f` flag implies `-c` flag.
* `srun --overlap` now allows the step to share all resources (CPUs, memory, and
  GRES), where previously `--overlap` only allowed the step to share CPUs with
  other steps.

===== API Changes

* `openapi/v0.0.35` - Plugin has been removed.
* `burst_buffer` plugins - `err_msg` added to `bb_p_job_validate()`.
* `openapi` - added flags to `slurm_openapi_p_get_specification()`. Existing
    plugins only need to update their prototype for the function as
    manipulating the flags pointer is optional.
* `openapi` - Added `OAS_FLAG_MANGLE_OPID` to allow plugins to request that the
    `operationId` of path methods be mangled with the full path to ensure
    uniqueness.
* `openapi/[db]v0.0.36` - Plugins have been marked as deprecated and will be
    removed in the next major release.
* switch plugins - add `switch_g_job_complete()` function.

==== Highlights of Slurm version 21.08

===== Highlights

* Removed `gres/mic` plugin used to support Xeon Phi coprocessors.
* Add `LimitFactor` to the QOS. A float that is factored into an associations
    `GrpTRES` limits.  For example, if the LimitFactor is 2, then an association
    with a `GrpTRES` of 30 CPUs, would be allowed to allocate 60 CPUs when
    running under this QOS.
* A job's `next_step_id` counter now resets to 0 after being requeued.
    Previously, the step id's would continue from the job's last run.
* API change: Removed slurm_kill_job_msg and modified the function signature
    for `slurm_kill_job2`. `slurm_kill_job2` should be used instead of
    `slurm_kill_job_msg`.
* `AccountingStoreFlags=job_script` allows you to store the job's batch script.
* `AccountingStoreFlags=job_env` allows you to store the job's env vars.
* Removed `sched/hold` plugin.
* `cli_filter/lua`, `jobcomp/lua`, `job_submit/lua` now load their scripts from the
    same directory as the `slurm.conf` file (and thus now will respect changes
    to the `SLURM_CONF` environment variable).
* `SPANK` - call `slurm_spank_init` if defined without `slurm_spank_slurmd_exit` in
    `slurmd` context.
* Add new `PLANNED` state to a node to represent when the backfill scheduler
    has it planned to be used in the future instead of showing as `IDLE`.
    sreport also has changed it's cluster utilization report column name from
    'Reserved' to 'Planned' to match this nomenclature.
* Put node into `INVAL` state upon registering with an invalid node
    configuration. Node must register with a valid configuration to continue.
* Remove `SLURM_DIST_LLLP` environment variable in favor of just
    `SLURM_DISTRIBUTION`.
* Make `--cpu-bind=threads` default for `--threads-per-core` -- can be
    overridden by the CLI or an environment variable.
* `slurmd` - allow multiple comma-separated controllers to be specified in
    configless mode with `--conf-server`
* Manually powering down of nodes with `scontrol` now ignores
    `SuspendExc<Nodes|Parts>`.
* Distinguish queued reboot requests (REBOOT@) from issued reboots (REBOOT^).
* `auth/jwt` - add support for RS256 tokens. Also permit the username in the
    'username' field in addition to the 'sun' (Slurm UserName) field.
* service files - change dependency to network-online rather than just
    network to ensure DNS and other services are available.
* Add "Extra" field to node to store extra information other than a comment.
* Add `ResumeTimeout`, `SuspendTimeout` and `SuspendTime` to Partitions.
* The `memory.force_empty` parameters is no longer set by `jobacct_gather/cgroup`
    when deleting the cgroup`. This previously caused a significant delay (~2s)
    when terminating a job, and is not believed to have provided any perceivable
    benefit. However, this may lead to slightly higher reported kernel mem page
    cache usage since the kernel cgroup memory is no longer freed immediately.
* `TaskPluginParam=verbose` is now treated as a default. Previously it would be
    applied regardless of the job specifying a `--cpu-bind`.
* Add `node_reg_mem_percent` `SlurmctldParameter` to define percentage of
    memory nodes are allowed to register with.
* Define and separate node power state transitions. Previously a powering
    down node was in both states, `POWERING_OFF` and `POWERED_OFF`. These are now
    separated.
    e.g.
       `IDLE+POWERED_OFF` (`IDLE~`)
    -> `IDLE+POWERING_UP` (`IDLE#`)   - Manual power up or allocation
    -> `IDLE`
    -> `IDLE+POWER_DOWN` (`IDLE!`)    - Node waiting for power down
    -> `IDLE+POWERING_DOWN` (`IDLE%`) - Node powering down
    -> `IDLE+POWERED_OFF` (`IDLE~`)   - Powered off
* Some node state flag names have changed. These would be noticeable for
    example if using a state flag to filter nodes with sinfo.
    e.g.
    `POWER_UP` -> `POWERING_UP`
    `POWER_DOWN` -> `POWERED_DOWN`
    `POWER_DOWN` now represents a node pending power down
* Create a new process called ``slurmscriptd`` which runs `PrologSlurmctld` and
    `EpilogSlurmctld`. This avoids `fork()` calls from `slurmctld`, and can avoid
    performance issues if the slurmctld has a large memory footprint.
* Pass JSON of job to node mappings to `ResumeProgram`.
* QOS accrue limits only apply to the job QOS, not partition QOS.
* Any return code from SPANK plugin or SPANK function that is not
    `SLURM_SUCCESS` (zero) will be considered to be an error. Previously, only
    negative return codes were considered an error.
* Add support for automatically detecting and broadcasting executable shared
    object dependencies for `sbcast` and `srun --bcast`.
* All SPANK error codes now start at 3000. Where previously SPANK would give
    a return code of 1, it will now return 3000. This change will break ABI
    compatibility with SPANK plugins compiled against older version of Slurm.
* SPANK plugins are now required to match the current Slurm release, and
    must be recompiled for each new Slurm major release. (They do not need
    to be recompiled when upgrading between maintenance releases.)
* `SLURM_NODE_ALIASES` now has brackets around the node's address to be able to
    distinguish IPv6 addresses. e.g. `<node_name>:[<node_addr>]:<node_hostname>`
* The `job_container/tmpfs` plugin now requires `PrologFlags=contain` to be set in
    `slurm.conf`.
* Limit `max_script_size` to 512 MB.

===== Configuration File Changes (for details, see the appropriate man page)

* Errors detected in the parser handlers due to invalid configurations are now
    propagated and can lead to fatal (and thus exit) the calling process.
* Enforce a valid configuration for `AccountingStorageEnforce` in `slurm.conf`.
    If the configuration is invalid, then an error message will be printed and
    the command or daemon (including `slurmctld`) will not run.
* Removed `AccountingStoreJobComment` option.  Please update your config to use
    `AccountingStoreFlags=job_comment` instead.
* Removed `DefaultStorage{Host,Loc,Pass,Port,Type,User}` options.
* Removed `CacheGroups`, `CheckpointType`, `JobCheckpointDir`, `MemLimitEnforce`,
    `SchedulerPort`, `SchedulerRootFilter` options.
* Added `Script` to `DebugFlags` for debugging `slurmscriptd` (the process that runs
    `slurmctld` scripts such as `PrologSlurmctld` and `EpilogSlurmctld`).
* Rename `SbcastParameters` to `BcastParameters`.
* systemd service files - add new '`-s`' option to each daemon which will
    change the working directory even with the `-D` option. (Ensures any core
    files are placed in an accessible location, rather than `/`.)
* Added `BcastParameters=send_libs` and `BcastExclude` options.
* Remove the (incomplete) `burst_buffer/generic` plugin.
* Make `SelectTypeParameters=CR_Core_Memory` default for `cons_tres` and `cons_res`.
* Remove support for `TaskAffinity=yes` in `cgroup.conf`. Adding `task/affinity` to
    `TaskPlugins` in `slurm.conf` is strongly recommended instead.

===== Command Changes (for details, see the appropriate man page)

* Changed the `--format` handling for negative field widths (left justified)
    to apply to the column headers as well as the printed fields.
* Invalidate multiple partition requests when using partition based
    associations.
* `scrontab` - create the temporary file under the `TMPDIR` environment variable
    (if set), otherwise continue to use `TmpFS` as configured in `slurm.conf`.
* `sbcast / srun --bcast` - removed support for zlib compression. lz4 is vastly
    superior in performance, and (counter-intuitively) zlib could provide worse
    performance than no compression at all on many systems.
* `sacctmgr` - changed column headings to `ParentID` and `ParentName` instead
    of `Par ID` and "Par Name` respectively.
* `SALLOC_THREADS_PER_CORE` and `SBATCH_THREADS_PER_CORE` have been added as
    input environment variables for `salloc` and `sbatch`, respectively. They do
    the same thing as `--threads-per-core`.
* Don't display node's comment with `scontrol show nodes` unless set.
* Added `SLURM_GPUS_ON_NODE` environment variable within each job/step.
* `sreport` - change to sorting `TopUsage` by the `--tres` option.
* `slurmrestd` - do not run allow operation as `SlurmUser`/root by default.
* `scontrol show node` now shows `State` as base_state+flags instead of
    shortened state with flags appended. eg. `IDLE#` -> `IDLE+POWERING_UP`.
    Also `POWER` state flag string is `POWERED_DOWN`.
* `scrontab` - add ability to update crontab from a file or standard input.
* `scrontab` - added ability to set and expand variables.
* Make `srun` sensitive to `BcastParameters`.
* Added `sbcast/srun --send-libs`, `sbcast --exclude` and `srun --bcast-exclude`.
* Changed `ReqMem` field in `sacct` to match memory from ReqTRES. It now shows
    the requested memory of the whole job with a letter appended indicating
    units (M for megabytes, G for gigabytes, etc.). `ReqMem` is only displayed for
    the job, since the step does not have requested TRES. Previously `ReqMem`
    was also displayed for the step but was just displaying `ReqMem` for the job.

===== API Changes

* `jobcomp` plugin: change plugin API to `jobcomp_p_*()`.
* `sched` plugin: change plugin API to `sched_p_*()` and remove
    `slurm_sched_p_initial_priority()` call.
* `step_ctx code` has been removed from the api.
* `slurm_stepd_get_info()`/`stepd_get_info()` has been removed from the api.
* The v0.0.35 OpenAPI plugin has now been marked as deprecated.
    Please convert your requests to the v0.0.37 OpenAPI plugin.
