<?xml version='1.0' encoding='UTF-8'?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook45-profile.xsl"
                 type="text/xml"
                 title="Profiling step"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
  "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd"
[
  <!ENTITY % myents SYSTEM "release-notes.ent" >
  %myents;
]>
<!-- NOTES and Questions
  Short terms: SLE HPC for 'SUSE Linux Enterprise High Performance Computing'
               SLES for 'SUSE Linux Enterprise Server'
-->
<article lang="en" id="rnotes">
 <title>&rnotes;</title>
 <articleinfo>
  &bugtracker-pi;
  <releaseinfo>&rversion;</releaseinfo>
  <productname>&product;</productname>
  <productnumber>&this-version;</productnumber>
  <date><?dbtimestamp format="Y-m-d"?></date>
  <abstract>
   <para>
    This document provides guidance and an overview to high-level general
    features and updates for the &product; &this-version;. It describes the
    capabilities and limitations of the &product; &this-version;.
   </para>
   <para condition="beta;pre">
    This product will be released in &rel-date;.
   </para>
   <para condition="maintained">
    These release notes are updated periodically. The latest version is always
    available at <ulink url="&rn-url;"/>.
    General documentation can be found at: <ulink url="&doc-url;"/>.
   </para>
   <para condition="unmaintained">
    The support period for &product; &this-version; has ended. To keep systems
    secure and supported, upgrade to current versions of &slsa; and &product;.
    Before starting the upgrade, make sure to apply all maintenance updates.
   </para>
   <para>
    General documentation for &slsa; and &product; can be found at:
    <ulink url="http://www.suse.com/documentation/"/>.
   </para>
  </abstract>
 </articleinfo>

 <section id="Intro">
  <title>&product; &this-version;</title>

  <para>
   &product; is a highly scalable, high performance open-source operating
   system designed to utilize the power of parallel computing for modeling,
   simulation and advanced analytics workloads.
  </para>
  <para>
   &product; &this-version; provides
   tools and libraries related to High Performance Computing.
   This includes:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Workload manager
    </para>
   </listitem>
   <listitem>
    <para>
     Remote and parallel shells
    </para>
   </listitem>
   <listitem>
    <para>
     Performance monitoring and measuring tools
    </para>
   </listitem>
   <listitem>
    <para>
     Serial console monitoring tool
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster power management tool
    </para>
   </listitem>
   <listitem>
    <para>
     A tool for discovering the machine hardware topology
    </para>
   </listitem>
   <listitem>
    <para>
     System monitoring
    </para>
   </listitem>
   <listitem>
    <para>
     A tool for monitoring memory errors
    </para>
   </listitem>
   <listitem>
    <para>
     A tool for determining the CPU model and its capabilities (x86-64 only)
    </para>
   </listitem>
   <listitem>
    <para>
     User-extensible heap manager capable of distinguishing between
     different kinds of memory (x86-64 only)
    </para>
   </listitem>
   <listitem>
    <para>
     Serial and parallel computational libraries providing the common
     standards BLAS, LAPACK, ...
    </para>
   </listitem>
   <listitem>
    <para>
     Various MPI implementations
    </para>
   </listitem>
   <listitem>
    <para>
     Serial and parallel libraries for the HDF5 file format
    </para>
   </listitem>
  </itemizedlist>

  </section>

  <section id="Intro.Availability">
   <title>Hardware Platform Support</title>
   <para>
    &product; &this-version; is available for the
    Intel 64/AMD64 (x86-64) and AArch64 platforms.
   </para>
  </section>

  <section id="Intro.Lifecycle">
   <title>Support and Life Cycle</title>
   <para>
    &product; &this-version; is supported throughout the life cycle of
    &slea; &this-version;. ESPOS (Extended Service Overlap Support) as well
    as Long Term Support Service are also available for this product. Any
    release package is fully maintained and supported until the availability
    of the next release.
   </para>
   <para>
    For more information, see the Support Policy page
    <ulink url="https://www.suse.com/support/policy.html"/>.
   </para>
  </section>

  <section id="Intro.Documentation">
   <title>Documentation and Other Information</title>
   <para/>
   <!--<section>
    <title>Available on the Product Media</title> -->
    <para>
     Accessing the documentation on the product media:
    </para>
    <itemizedlist>
     <listitem>
      <para>Read the READMEs on the media.</para>
     </listitem>
     <listitem>
      <para>
       Get the detailed change log information about a particular package
       from the RPM (where <filename><replaceable>FILENAME</replaceable>.rpm</filename>
       is the name of the RPM):
      </para>
      <screen>rpm --changelog -qp <replaceable>FILENAME</replaceable>.rpm</screen>
     </listitem>
     <listitem>
      <para>
       Check the <filename>ChangeLog</filename> file in the top level of the
       media for a chronological log of all changes made to the updated
       packages.
      </para>
     </listitem>
     <!-- FIXME: For the near future, there will not be documentation. -
     sknorr, 2017-03-28 -->
     <!-- <listitem>
      <para> Find more information in the <filename>docu</filename> directory of the media of
       the &product; &this-version;. This directory includes PDF versions of the
       &product; &this-version; Installation Quick Start and Deployment Guides. Documentation (if installed) is
       available below the <filename>/usr/share/doc/</filename> directory of an installed system.
      </para>
     </listitem> -->
     <listitem>
      <para>
       These &rnotes; are identical across all architectures, and the most
       recent version is always available online at
       <ulink url="https://www.suse.com/releasenotes/"/>. Some entries may be
       listed twice, if they are important and belong to more than one section.
      </para>
     </listitem>
    </itemizedlist>
   <!-- </section> -->

   <!-- FIXME: For the near future, there will not be documentation. -
   sknorr, 2017-03-28 -->
   <!-- <section>
    <title>Externally Provided Documentation</title>
    <itemizedlist>
     <listitem>
      <para>
       <ulink url="https://www.suse.com/documentation"/>
       contains additional or updated documentation for the &product; &this-version;.
      </para>
     </listitem>
     <listitem>
      <para>
       Find a collection of White Papers in the &product; Resource Library
       at <ulink
       url="https://www.suse.com/products/server/resource-library"/>.
      </para>
     </listitem>
    </itemizedlist>
   </section> -->
  </section>

  <section id="Intro.Sourcecode">
   <title>How to Obtain Source Code</title>
   <para>
    This &suse; product includes materials licensed to &suse; under the GNU
    General Public License (GPL). The GPL requires &suse; to provide the
    source code that corresponds to the GPL-licensed material.
    The source code is available for download at
    <ulink url="https://www.suse.com/download/sle-hpc/"/> on Medium 2.
    For up to three years after distribution of the &suse; product, upon request,
    &suse; will mail a copy of the source code.
    Send requests by e-mail to <ulink url="mailto:sle_source_request@suse.com"/>.
    &suse; may charge a reasonable fee to recover distribution costs.
   </para>
  </section>
  <section id="Intro.Support">
   <title>Support Statement for &product;</title>
   <para>
    To receive support, you need an appropriate subscription with &suse;. For
    more information, see
    <ulink url="https://www.suse.com/products/server/services-and-support/"/>.
   </para>
   <para>
    The following definitions apply:
   </para>
    <variablelist>
     <varlistentry>
      <term>L1</term>
      <listitem>
       <para>
        Problem determination, which means technical support designed to
        provide compatibility information, usage support, ongoing
        maintenance, information gathering and basic troubleshooting using
        available documentation.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>L2</term>
      <listitem>
       <para>
        Problem isolation, which means technical support designed to analyze
        data, reproduce customer problems, isolate problem area and provide a
        resolution for problems not resolved by Level 1 or alternatively
        prepare for Level 3.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>L3</term>
      <listitem>
       <para>
        Problem resolution, which means technical support designed to resolve
        problems by engaging engineering to resolve product defects which
        have been identified by Level 2 Support.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>

    <para>
     For contracted customers and partners, the &product; &this-version; is
     delivered with L3&#160;support for all packages, except the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Technology Previews<!--, see <xref linkend="Intro.Support.Techpreviews"/>-->
      </para>
     </listitem>
     <listitem>
      <para>
       sound, graphics, fonts and artwork
      </para>
     </listitem>
     <listitem>
      <para>
       packages that require an additional customer
       contract<!--, see <xref linkend="Intro.Support.External"/>-->
      </para>
     </listitem>
     <listitem>
      <para>
       development packages for libraries which are only delivered with
       L2&#160;support
      </para>
     </listitem>
    </itemizedlist>
    <para>
     &suse; will only support the usage of original (that is, unchanged and
     un-recompiled) packages.
    </para>
  </section>

  <!-- <section id="Intro.Support.General">
   <title>General Support</title>
   <para/>
  </section> -->

  <!-- <section id="Intro.Support.External">
   <title>Software Requiring Specific Contracts</title>
   <para/>
  </section> -->

  <!-- <section id="Intro.Support.Techpreviews">
   <title>Technology Previews</title>

   <para>
    Technology previews are packages, stacks, or features delivered by &suse;
    which are not supported. They may be functionally incomplete, unstable or
    in other ways not suitable for production use. They are included for your
    convenience and give you a chance to test new technologies within an
    enterprise environment.
   </para>

   <para>
    Whether a technology preview becomes a fully supported technology later
    depends on customer and market feedback.
    Technology previews can be dropped at any time and &suse; does not commit
    to providing a supported version of such technologies in the future.
   </para>

   <para>
    Give your &suse; representative feedback, including your experience and
    use case.
   </para>

  </section> -->

 <section id="InstUpgrade">
  <title>Installation and Upgrade</title>
  <para>
   &product; comes with a number of preconfigured system roles for HPC. These
   roles provide a set of preselected packages typical for the specific role,
   as well as an installation workflow that will configure the system to make
   the best use of system resource based on a typical role use case.
  </para>

  <section id="fate-323494">
   <title>System Roles for &product; &this-version;</title>
   <para>
    With &product; &this-version;, it is possible to choose specific roles
    for the system based on modules selected during the installation process.
    When the HPC Module is enabled, these three roles are available:
   </para>
   <variablelist>
    <varlistentry>
     <term>HPC Management Server (Head Node)</term>
     <listitem>
      <para>
       This role includes the following features:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Uses XFS as the default root file system
        </para>
       </listitem>
       <listitem>
        <para>
         Includes HPC-enabled libraries
        </para>
       </listitem>
       <listitem>
        <para>
         Disables firewall and Kdump services
        </para>
       </listitem>
       <listitem>
        <para>
         Installs controller for the Slurm Workload Manager
        </para>
       </listitem>
       <listitem>
        <para>
         Mounts a large scratch partition to <filename>/var/tmp</filename>
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>HPC Compute Node</term>
     <listitem>
      <para>
       This role includes the following features:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Uses XFS as the default root file system
        </para>
       </listitem>
       <listitem>
        <para>
         Includes HPC-enabled libraries
        </para>
       </listitem>
       <listitem>
        <para>
         Disables firewall and Kdump services
        </para>
       </listitem>
       <listitem>
        <para>
         Based from minimal setup configuration
        </para>
       </listitem>
       <listitem>
        <para>
         Installs client for the Slurm Workload Manager
        </para>
       </listitem>
       <listitem>
        <para>
         Does not create a separate home partition
        </para>
       </listitem>
       <listitem>
        <para>
         Mounts a large scratch partition to <filename>/var/tmp</filename>
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>HPC Development Node</term>
     <listitem>
      <para>
       This role includes the following features:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Includes HPC-enabled libraries
        </para>
       </listitem>
       <listitem>
        <para>
         Adds compilers and development toolchain
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The scratch partition <literal>/var/tmp/</literal> will only be created if
    there is sufficient space available on the installation medium (minimum
    32 GB).
   </para>
   <para>
    The Environment Module <literal>Lmod</literal> will be installed for all
    roles. It is required at build time and run time of the system. For more
    information, see <xref linkend="fate-321704"/>.
   </para>
   <para>
    All libraries specifically build for HPC will be installed under
    <literal>/usr/lib/hpc</literal>. They are not part of the standard search
    path, thus the <literal>Lmod</literal> environment module system is required.
   </para>
   <para>
    <literal>Munge</literal> authentication is installed for all roles. This
    requires to copy the same generated munge keys to all nodes of a cluster.
    For more information, see <xref linkend="fate-321722"/> and
    <xref linkend="sec.mungekey"/>.
   </para>
   <para>
    From the Ganglia monitoring system, the data collector
    <package>ganglia-gmod</package> is installed for every role, while the
    data aggregator <package>ganglia-gmetad</package> needs to be installed
    manually on the system which is expected to collect the data. For more
    information, see <xref linkend="fate-323979"/>.
   </para>
   <para>
    The system roles are only available for new installations of &product;.
   </para>
  </section>
  <section id="InstUpgrade.Installation">
   <title>Installation</title>
   <para>
    This section includes information related to the initial installation of
    the &product; &this-version;.
    <!-- FIXME: For the near future, there will not be documentation. -
    sknorr, 2017-03-28 -->
    <!-- For information about installing, see
    <citetitle>Deployment Guide</citetitle> at
    <ulink url="https://www.suse.com/documentation"/>. -->
   </para>
  </section>

  <section id="InstUpgrade.Upgrade">
   <title>Upgrade-Related Notes</title>
   <para>
    This section includes upgrade-related information for the &product; &this-version;.
    <!-- FIXME: For the near future, there will not be documentation. -
    sknorr, 2017-03-28 -->
    <!-- For information about general preparations and supported upgrade methods
    and paths, see the documentation at
    <ulink url="https://www.suse.com/documentation"/>. -->
   </para>
   <para>
    You can upgrade to &product; &this-version; from &slsa; 12 SP3 or &shpca;
    12 SP3. When upgrading from &slsa; 12 SP3, the upgrade will only be
    performed if the &shpca; module has been registered prior to upgrading.
    Otherwise, the system will instead be upgraded to &slsa; 15.
   </para>
   <para>
    To upgrade from &slsa; 12 to &slsa; 15, make sure to unregister the
    &shpca; module prior to upgrading. To do so, open a root shell and
    execute:
   </para>
   <screen>SUSEConnect -d -p sle-module-hpc/12/<replaceable>ARCH</replaceable></screen>
   <para>
    Replace <replaceable>ARCH</replaceable> with the architecture used
    (<literal>x86_64</literal>, <literal>aarch64</literal>).
   </para>
  <para>
   When migrating to &product; &this-version;, all modules not supported by
   the migration target need to be deregistered. This can be done by
   executing:
  </para>
<screen>SUSEConnect -d -p sle-module-<replaceable>MODULE_NAME</replaceable>/12/<replaceable>ARCH</replaceable></screen>
   <para>
    Replace <replaceable>MODULE_NAME</replaceable> by the name of the module
    and <replaceable>ARCH</replaceable> with the architecture used
    (<literal>x86_64</literal>, <literal>aarch64</literal>).
  </para>
  </section>
  <!-- <section>
   <title>For More Information</title>
   <para>
    For more information, see <xref linkend="InfraPackArch.ArchIndependent"/>
    and the sections relating to your respective hardware architecture.
   </para>
  </section>-->
 </section>

 <section id="Packages.Modules">
  <title>Functionality</title>
  <para>
   This section comprises information about packages and their functionality,
   as well as additions, updates, removals and changes to the package layout
   of software.
  </para>

  <section id="fate-319512">
   <!-- href="https://fate.novell.com/319512" -->
   <title>cpuid &mdash; x86 CPU Identification Tool</title>
   <para>
    <literal>cpuid</literal> executes the x86 CPUID instruction and decodes
    and prints the results to stdout. Its knowledge of Intel, AMD and Cyrix
    CPUs is fairly complete. It also supports Intel Knights Mill CPUs (x86-64).
   </para>
   <para>
    To install <literal>cpuid</literal>, run: <literal>zypper in
    cpuid</literal>.
   </para>
   <para>
    For information about runtime options for <literal>cpuid</literal>, see the
    man page <literal>cpuid(1)</literal>.
   </para>
   <para>
    Note that this tool is only available for x86-64.
   </para>
  </section>
  <section id="fate-321724">
   <!-- href="https://fate.novell.com/321724" -->
   <title>ConMan &mdash; The Console Manager</title>
   <para>
    ConMan is a serial console management program designed to support a
    large number of console devices and simultaneous users. It supports:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      local serial devices
     </para>
    </listitem>
    <listitem>
     <para>
      remote terminal servers (via the telnet protocol)
     </para>
    </listitem>
    <listitem>
     <para>
      IPMI Serial-Over-LAN (via FreeIPMI)
     </para>
    </listitem>
    <listitem>
     <para>
      Unix domain sockets
     </para>
    </listitem>
    <listitem>
     <para>
      external processes (for example, using 'expect' scripts for telnet,
      ssh, or ipmi-sol connections)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    ConMan can be used for monitoring, logging and optionally timestamping
    console device output.
   </para>
   <para>
    To install ConMan, run <literal>zypper in conman</literal>.
   </para>
   <important>
    <title><systemitem class="daemon">conmand</systemitem> Sends Unencrypted Data</title>
    <para>
     The daemon <systemitem class="daemon">conmand</systemitem> sends
     unencrypted data over the
     network and its connections are not authenticated. Therefore, it should
     be used locally only: Listening to the port
     <literal>localhost</literal>. However, the IPMI console does offer
     encryption. This makes <literal>conman</literal> a good tool for
     monitoring a large number of such consoles.
    </para>
   </important>
   <para>
    Usage:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      ConMan comes with a number of expect-scripts. They can be found in the
      directory <filename>/usr/lib/conman/exec</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Input to <literal>conman</literal> is not echoed in interactive mode.
      This can be changed by entering the escape sequence
      <literal>&amp;E</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      When pressing <keycap function="enter"/> in interactive mode, no line
      feed is generated. To generate a line feed, press
      <keycombo><keycap function="control"/><keycap>L</keycap></keycombo>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information about options, see the man page of ConMan.
   </para>
  </section>

  <section id="fate-323979">
   <!-- href="https://fate.novell.com/323979" -->
   <title>Ganglia &mdash; System Monitoring</title>
   <para>
    Ganglia is a scalable distributed monitoring system for high-performance
    computing systems, such as clusters and grids. It is based on a
    hierarchical design targeted at federations of clusters.
   </para>
   <bridgehead renderas="sect5">Using Ganglia</bridgehead>
   <para>
    To use Ganglia, make sure to install <package>ganglia-gmetad</package>
    on the management serve. Then start the Ganglia meta-daemon:
    <command>rcgmead start</command>. To make sure the service is started
    after a reboot, run: <command>systemctl enable gmetad</command>. On
    each cluster node which you want to monitor, install
    <package>ganglia-gmond</package>, start the service <command>rcgmond
    start</command> and make sure it is enabled to be started automatically
    after a reboot: <command>systemctl enable gmond</command>. To test
    whether the <systemitem class="daemon">gmond</systemitem> daemon has
    connected to the
    meta-daemon, run <command>gstat -a</command> and check that each node to
    be monitored is present in the output.
   </para>
   <bridgehead renderas="sect5">Ganglia on Btrfs</bridgehead>
   <para>
    When using the Btrfs file system, the monitoring data will be lost after
    a rollback and the service <systemitem class="daemon">gmetad</systemitem>
    will not start again. To fix this issue, either install the package
    <package>ganglia-gmetad-skip-bcheck</package> or create the file
    <filename>/etc/ganglia/no_btrfs_check</filename>.
   </para>
   <bridgehead renderas="sect5">Using the Ganglia Web Interface</bridgehead>
   <para>
    To use the Ganglia Web interface, it is required to add the
    <emphasis>Web and Scripting Module</emphasis> first.
    Which modules are activated and which are
    available can be checked with <command>SUSEConnect -l</command>.
    To activate the <emphasis>Web and Scripting Module</emphasis>, run:
    <command>SUSEConnect -p sle-module-web-scripting/15/x86_64</command>.
   </para>
   <para>
    Install <package>ganglia-web</package> on the management server.
    Depending on which PHP version is used (default is PHP 7), enable it in
    Apache2: <command>a2enmod php7</command>.
   </para>
   <para>
    Then start Apache2 on this
    machine: <command>rcapache2 start</command> and make sure it is started
    automatically after a reboot: <command>systemctl enable apache2</command>.
    The Ganglia Web interface should be accessible from
    <literal>http://<replaceable>MANAGEMENT_SERVER</replaceable>/ganglia-web</literal>.
   </para>
  </section>
  <section id="fate-324149">
   <!-- href="https://fate.novell.com/324149" -->
   <title>Genders &mdash; Static Cluster Configuration Database</title>
   <para>
    Support for Genders has been added to the HPC module.
   </para>
   <para>
    Genders is a static cluster configuration database used for
    configuration management. It allows grouping and addressing sets of
    hosts by attributes and is used by a variety of tools. The Genders
    database is a text file which is usually replicated on each node in a
    cluster.
   </para>
   <para>
    Perl, Python, C, and C++ bindings are supplied with Genders, the
    respective packages provide man pages or other documentation describing
    the APIs.
   </para>
   <para>
    To create the Genders database, follow the instructions and examples in
    <filename>/etc/genders</filename> and check
    <filename>/usr/share/doc/packages/genders-base/TUTORIAL</filename>.
    Testing a configuration can be done with <literal>nodeattr</literal>
    (for more information, see <command>man 1 nodeattr</command>).
   </para>
   <para>
    List of packages:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>genders</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>genders-base</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>genders-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>python-genders</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>genders-perl-compat</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libgenders0</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libgendersplusplus2</package>
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section id="fate-321705">
   <!-- href="https://fate.novell.com/321705" -->
   <title>GNU Compiler Collection for HPC</title>
   <para>
    <package>gnu-compilers-hpc</package> installs the base version of the
    GNU compiler suite and provides environment files for Lmod to select
    this compiler suite and provides environment module files for them. This
    version of the compiler suite is required to enable linking against HPC
    libraries enabled for environment modules.
   </para>
   <para>
    This package requires <package>lua-lmod</package> to supply environment
    module support.
   </para>
   <para>
    To install <package>gnu-compilers-hpc</package>, run:
   </para>
<screen>zypper in gnu-compilers-hpc</screen>
   <para>
    To set up the environment appropriately and select the GNU toolchain,
    run:
   </para>
<screen>module load gnu</screen>
   <para>
    If you have more than one version of this compiler suite installed, add
    the version number of the compiler suite. For more information,
    see <xref linkend="fate-321704"/>.
   </para>
  </section>

  <section id="fate-319511">
   <!-- href="https://fate.novell.com/319511" -->
   <title>hwloc &mdash; Portable Abstraction of Hierarchical Architectures for High-Performance Computing</title>
   <para>
    <literal>hwloc</literal> provides command-line tools and a C API to
    obtain the hierarchical map of key computing elements, such as: NUMA
    memory nodes, shared caches, processor packages, processor cores,
    processing units (logical processors or "threads") and even I/O devices.
    <literal>hwloc</literal> also gathers various attributes such as cache
    and memory information, and is portable across a variety of different
    operating systems and platforms. Additionally it may assemble the
    topologies of multiple machines into a single one, to let
    applications consult the topology of an entire fabric or cluster at
    once.
   </para>
   <para>
    In graphical mode (X11), <literal>hwloc</literal> can display the
    topology in a human-readable format. Alternatively, it can export to one
    of several formats, including plain text, PDF, PNG, and FIG. For more
    information, see the man pages provided by <literal>hwloc</literal>.
   </para>
   <para>
    It also features full support for import and export of XML-formatted
    topology files via the <literal>libxml2</literal> library.
   </para>
   <para>
    The package <literal>hwloc-devel</literal> offers a library that can be
    directly included into external programs. This requires that the
    <literal>libxml2</literal> development library (package
    <literal>libxml2-devel</literal>) is available when compiling
    <literal>hwloc</literal>.
   </para>
  </section>
  <section id="fate-321704">
   <!-- href="https://fate.novell.com/321704" -->
   <title>Lmod &mdash; Lua-based Environment Modules</title>
   <para>
    Lmod is an advanced environment module system which allows the
    installation of multiple versions of a program or shared library, and
    helps configure the system environment for the use of a specific
    version. It
    supports hierarchical library dependencies and makes sure that the
    correct version of dependent libraries are selected. Environment
    Modules-enabled library packages supplied with the HPC module support
    parallel installation of different versions and flavors of the same
    library or binary and are supplied with appropriate
    <literal>lmod</literal> module files.
   </para>
   <bridgehead renderas="sect5">Installation and Basic Usage</bridgehead>
   <para>
    To install Lmod, run: <command>zypper in lua-lmod</command>.
   </para>
   <para>
    Before Lmod can be used, an init file needs to be sourced from the
    initialization file of your interactive shell. The following init files
    are available:
   </para>
<screen>/usr/share/lmod/&lt;lmod_version&gt;/init/bash
/usr/share/lmod/&lt;lmod_version&gt;/init/ksh
/usr/share/lmod/&lt;lmod_version&gt;/init/tcsh
/usr/share/lmod/&lt;lmod_version&gt;/init/zsh
/usr/share/lmod/&lt;lmod_version&gt;/init/sh</screen>
   <para>
    Pick the one appropriate for your shell. Then add the following to the
    init file of your shell:
   </para>
<screen>. /usr/share/lmod/&lt;LMOD_VERSION&gt;/init/&lt;INIT-FILE&gt;</screen>
   <para>
    To obtain <literal>&lt;lmod_version&gt;</literal>, run:
   </para>
<screen>rpm -q lua-lmod | sed "s/.*-\([^-]\+\)-.*/\1/"</screen>
   <para>
    The init script adds the command <command>module</command>.
   </para>
   <bridgehead renderas="sect5">Listing Available Modules</bridgehead>
   <para>
    To list the available all available modules, run: <command>module
    spider</command>. To show all modules which can be loaded with the
    currently loaded modules, run: <command>module avail</command>. A
    module name consists of a name and a version string separated by a
    <literal>/</literal> character. If more than one version is available
    for a certain module name, the default version (marked by
    <literal>*</literal>) or (if this is not set) the one with the highest
    version number is loaded. To refer to a specific module version, the
    full string <literal><replaceable>NAME</replaceable>/<replaceable>VERSION</replaceable></literal>
    may be used.
   </para>
   <bridgehead renderas="sect5">Listing Loaded Modules</bridgehead>
   <para>
    <command>module list</command> shows all currently loaded modules. Refer
    to <command>module help</command> for a short help on the module command
    and <command>module help <replaceable>MODULE-NAME</replaceable></command>
    for a help on the
    particular module. Note that the <command>module</command> command is available
    only when you log in after installing <literal>lua-lmod</literal>.
   </para>
   <bridgehead renderas="sect5">Gathering Information About a Module</bridgehead>
   <para>
    To get information about a particular module, run: <command>module
    whatis <replaceable>MODULE-NAME</replaceable></command> To load a module,
    run:
    <command>module load <replaceable>MODULE-NAME</replaceable></command>. This
    will ensure
    that your environment is modified (that is, the <literal>PATH</literal> and
    <literal>LD_LIBRARY_PATH</literal> and other environment variables are
    prepended) such that binaries and libraries provided by the respective
    modules are found. To run a program compiled against this library, the
    appropriate <command>module load</command> commands must be issued
    beforehand.
   </para>
   <bridgehead renderas="sect5">Loading Modules</bridgehead>
   <para>
    The <command>module load <replaceable>MODULE</replaceable></command>
    command needs to be
    run in the shell from which the module is to be used. Some modules
    require a compiler toolchain or MPI flavor module to be loaded before
    they are available for loading.
   </para>
   <bridgehead renderas="sect5">Environment Variables</bridgehead>
   <para>
    If the respective development packages are installed, build time
    environment variables like <literal>LIBRARY_PATH</literal>,
    <literal>CPATH</literal>, <literal>C_INCLUDE_PATH</literal> and
    <literal>CPLUS_INCLUDE_PATH</literal> will be set up to include the
    directories containing the appropriate header and library files.
    However, some compiler and linker commands may not honor these. In this
    case, use the appropriate options together with the environment
    variables <literal>-I <replaceable>PACKAGE_NAME</replaceable>_INC</literal>
    and <literal>-L <replaceable>PACKAGE_NAME</replaceable>_LIB</literal>
    to add the include and library paths
    to the command lines of the compiler and linker.
   </para>
   <bridgehead renderas="sect5">For More Information</bridgehead>
   <para>
    For more information on Lmod, see
    <ulink url="https://lmod.readthedocs.org"/>.
   </para>
  </section>
  <section id="fate-320596">
   <!-- href="https://fate.novell.com/320596" -->
   <title>ohpc &mdash; OpenHPC Compatibility Macros</title>
   <para>
    <literal>ohpc</literal> contains compatibility macros to build OpenHPC
    packages on SUSE Linux Enterprise.
   </para>
   <para>
    To install <package>ohpc</package>, run: <command>zypper in
    ohpc</command>.
   </para>
  </section>
  <section id="fate-321714">
   <!-- href="https://fate.novell.com/321714" -->
   <title>pdsh &mdash; Parallel Remote Shell Program</title>
   <para>
    <literal>pdsh</literal> is a parallel remote shell which can be used
    with multiple back-ends for remote connections. It can run a command on
    multiple machines in parallel.
   </para>
   <para>
    To install pdsh, run <command>zypper in pdsh</command>.
   </para>
   <para>
    On &slsa; 12, the back-ends <literal>ssh</literal>,
    <literal>mrsh</literal>, and <literal>exec</literal> are supported. The
    <literal>ssh</literal> back-end is the default. Non-default login methods
    can be used by either setting the <literal>PDSH_RCMD_TYPE</literal>
    environment variable or by using the <literal>-R</literal> command
    argument.
   </para>
   <para>
    When using the <literal>ssh</literal> back-end, it is important that a
    non-interactive (that is, passwordless) login method is used.
   </para>
   <para>
    The <literal>mrsh</literal> back-end requires the
    <literal>mrshd</literal> to be running on the client. The
    <literal>mrsh</literal> back-end does not require the use of reserved
    sockets. Therefore, it does not suffer from port exhaustion when
    executing commands on many machines in parallel. For information about
    setting up the system to use this back-end, see
    <xref linkend="fate-321722"/>.
   </para>
   <para>
    Remote machines can either be specified on the command line or
    <command>pdsh</command> can use a <filename>machines</filename> file
    (<filename>/etc/pdsh/machines</filename>), dsh (Dancer's shell) style
    groups or netgroups. Also, it can target nodes based on the currently
    running Slurm jobs.
   </para>
   <para>
    The different ways to select target hosts are realized by modules. Some
    of these modules provide identical options to <command>pdsh</command>.
    The module loaded first will win and consume the option. Therefore, we
    recommend limiting yourself to a single method and specifying this with
    the <literal>-M</literal> option.
   </para>
   <para>
    The <filename>machines</filename> file lists all target hosts one per
    line. The appropriate netgroup can be selected with the
    <literal>-g</literal> command line option.
   </para>
   <para>
    The following host-list plugins for <command>pdsh</command> are supported:
    <literal>machines</literal>, <literal>slurm</literal>,
    <literal>netgroup</literal> and <literal>dshgroup</literal>.
    Each host-list plugin is provided in a separate package. This avoids
    conflicts between command line options for different plugins which
    happen to be identical and helps to keep installations small and free
    of unneeded dependencies. Package dependencies have been set to prevent
    installing plugins with conflicting command options. To install one of
    the plugins, run:
   </para>
   <screen>zypper in pdsh-<replaceable>PLUGIN_NAME</replaceable></screen>
   <para>
    For more information, see the man page <command>pdsh</command>.
   </para>
  </section>
  <section id="fate-321725">
   <!-- href="https://fate.novell.com/321725" -->
   <title>PowerMan &mdash; Centralized Power Control for Clusters</title>
   <para>
    PowerMan allows manipulating remote power control devices (RPC) from a
    central location. It can control:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      local devices connected to a serial port
     </para>
    </listitem>
    <listitem>
     <para>
      RPCs listening on a TCP socket
     </para>
    </listitem>
    <listitem>
     <para>
      RPCs which are accessed through an external program
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The communication to RPCs is controlled by <quote>expect</quote>-like
    scripts. For a
    list of currently supported devices, see the configuration file
    <filename>/etc/powerman/powerman.conf</filename>.
   </para>
   <para>
    To install PowerMan, run <command>zypper in powerman</command>.
   </para>
   <para>
    To configure it, include the appropriate device file for your RPC
    (<filename>/etc/powerman/*.dev</filename>) in
    <filename>/etc/powerman/powerman.conf</filename> and add devices and
    nodes. The device <quote>type</quote> needs to match the
    <quote>specification</quote> name in one
    of the included device files. The list of <quote>plugs</quote> used for
    nodes need to
    match an entry in the <quote>plug name</quote> list.
   </para>
   <para>
    After configuring PowerMan, start its service by:
   </para>
<screen>systemctl start powerman.service</screen>
   <para>
    To start PowerMan automatically after every boot, run:
   </para>
<screen>systemctl enable powerman.service</screen>
   <para>
    Optionally, PowerMan can connect to a remote PowerMan instance. To
    enable this, add the option <literal>listen</literal> to
    <filename>/etc/powerman/powerman.conf</filename>.
   </para>
   <important>
    <title>Unencrypted Transfer</title>
    <para>
     When connecting to a remote PowerMan instance, data is transferred
     unencrypted. Therefore, use this feature only if the network is
     appropriately secured.
    </para>
   </important>
  </section>
  <section id="fate-318824">
   <!-- href="https://fate.novell.com/318824" -->
   <title>rasdaemon &mdash; Utility to Log RAS Error Tracings</title>
   <para>
    <systemitem class="daemon">rasdaemon</systemitem> is a RAS
    (Reliability, Availability and
    Serviceability) logging tool. It records memory errors using the EDAC
    tracing events. EDAC drivers in the Linux kernel handle detection of ECC
    errors from memory controllers.
   </para>
   <para>
    <systemitem class="daemon">rasdaemon</systemitem> can be used on large
    memory systems to
    track, record and localize memory errors and how they evolve over time
    to detect hardware degradation. Furthermore, it can be used to localize
    a faulty DIMM on the board.
   </para>
   <para>
    To check whether the EDAC drivers are loaded, execute:
   </para>
<screen>ras-mc-ctl --status</screen>
   <para>
    The command should return <literal>ras-mc-ctl: drivers are
    loaded</literal>. If it indicates that the drivers are not loaded, EDAC
    may not be supported on your board.
   </para>
   <para>
    To start <systemitem class="daemon">rasdaemon</systemitem>, run
    <command>systemctl start rasdaemon.service</command>.
    To start <systemitem class="daemon">rasdaemon</systemitem>
    automatically at boot time, execute <command>systemctl enable
    rasdaemon.service</command>. The daemon will log information to
    <filename>/var/log/messages</filename> and to an internal database. A
    summary of the stored errors can be obtained with:
   </para>
<screen>ras-mc-ctl --summary</screen>
   <para>
    The errors stored in the database can be viewed with
   </para>
<screen>ras-mc-ctl --errors</screen>
   <para>
    Optionally, you can load the DIMM labels silk-screened on the system
    board to more easily identify the faulty DIMM. To do so, before starting
    <systemitem class="daemon">rasdaemon</systemitem>, run:
   </para>
<screen>systemctl start ras-mc-ctl start</screen>
   <para>
    For this to work, you need to set up a layout description for the board.
    There are no descriptions supplied by default. To add a layout
    description, create a file with an arbitrary name in the directory
    <filename>/etc/ras/dimm_labels.d/</filename>. The format is:
   </para>
<screen>Vendor: <replaceable>VENDOR-NAME</replaceable>
  Model: <replaceable>MODEL-NAME</replaceable>
    <replaceable>LABEL</replaceable>: <replaceable>MC</replaceable>.<replaceable>TOP</replaceable>.<replaceable>MID</replaceable>.<replaceable>LOW</replaceable></screen>
  </section>
  <section id="fate-316379">
   <!-- href="https://fate.novell.com/316379" -->
   <title>Slurm &mdash; Utility for HPC Workload Management</title>
   <para>
    Slurm is an open-source, fault-tolerant, and highly scalable cluster
    management and job scheduling system for Linux clusters containing up to
    65,536 nodes. Components include machine status, partition management,
    job management, scheduling and accounting modules.
   </para>
   <para>
    For a minimal setup to run Slurm with <emphasis>munge</emphasis> support on
    one compute
    node and multiple control nodes, follow these instructions:
   </para>
   <procedure>
    <step>
     <para>
      Before installing Slurm, create a user and a group called
      <literal>slurm</literal>.
     </para>
     <important>
      <title>Make Sure of Consistent UIDs and GIDs for Slurm's Accounts</title>
      <para>
       For security reasons, Slurm does not run as the user
       <systemitem class="username">root</systemitem> but under its own
       user. It is important that the user
       <systemitem class="username">slurm</systemitem> has the
       same UID/GID across all nodes of the cluster.
      </para>
      <para>
       If this user/group does not exist, the package
       <package>slurm</package> creates this user and group when it is
       installed. However, this does not guarantee
       that the generated UIDs/GIDs will be identical on all systems.
      </para>
      <para>
       Therefore, we strongly advise you to create the user/group
       <systemitem class="username">slurm</systemitem> before
       installing <package>slurm</package>.
       If you are using a network directory service such as LDAP for user and
       group management, you can use it to
       provide the <systemitem class="username">slurm</systemitem>
       user/group as well.
      </para>
     </important>
    </step>
    <step>
     <para>
      Install <package>slurm-munge</package> on the control and compute
      nodes: <command>zypper in slurm-munge</command>
     </para>
    </step>
    <step>
     <para>
      Configure, enable and start "munge" on the control and compute nodes
      as described in <xref linkend="fate-321722"/>.
     </para>
    </step>
    <step>
     <para>
      On the compute node, edit <filename>/etc/slurm/slurm.conf</filename>:
     </para>
     <substeps>
      <step>
       <para>
        Configure the parameter
        <literal>ControlMachine=<replaceable>CONTROL_MACHINE</replaceable></literal>
        with the host name of the control node.
       </para>
       <para>
        To find out the correct host name, run
        <command>hostname -s</command> on the control node.
       </para>
      </step>
      <step>
       <para>
        Additionally add:
       </para>
<screen>NodeName=<replaceable>NODE_LIST</replaceable> Sockets=<replaceable>SOCKETS</replaceable> \
  CoresPerSocket=<replaceable>CORES_PER_SOCKET</replaceable> \
  ThreadsPerCore=<replaceable>THREADS_PER_CORE</replaceable> \
  State=UNKNOWN</screen>
       <para>
        and
       </para>
<screen>PartitionName=normal Nodes=<replaceable>NODE_LIST</replaceable> \
  Default=YES MaxTime=24:00:00 State=UP</screen>
       <para>
        where <replaceable>NODE_LIST</replaceable> is the list of compute
        nodes (that is, the output of <command>hostname -s</command> run on
        each compute node (either comma-separated or as ranges:
        <literal>foo[1-100]</literal>). Additionally,
        <replaceable>SOCKETS</replaceable> denotes the number of sockets,
        <replaceable>CORES_PER_SOCKET</replaceable> the number of cores per
        socket, <replaceable>THREADS_PER_CORE</replaceable> the number of
        threads for CPUs which can execute more than one thread at a time.
        (Make sure that <replaceable>SOCKETS</replaceable> *
        <replaceable>CORES_PER_SOCKET</replaceable> *
        <replaceable>THREADS_PER_CORE</replaceable> does not exceed the
        number of system cores on the compute node).
       </para>
      </step>
      <step>
       <para>
        On the control node, copy <filename>/etc/slurm/slurm.conf</filename>
        to all compute nodes:
       </para>
<screen>scp /etc/slurm/slurm.conf <replaceable>COMPUTE_NODE</replaceable>:/etc/slurm/</screen>
      </step>
      <step>
       <para>
        On the control node, start <systemitem class="daemon">slurmctld</systemitem>:
       </para>
<screen>systemctl start slurmctld.service</screen>
       <para>
        Also enable it so that it starts on every boot:
       </para>
<screen>systemctl enable slurmctld.service</screen>
      </step>
      <step>
       <para>
        On the compute nodes, start and enable
        <systemitem class="daemon">slurmd</systemitem>:
       </para>
<screen>systemctl start slurmd.service
systemctl enable slurmd.service</screen>
       <para>
        The last line causes <systemitem class="daemon">slurmd</systemitem>
        to be started on
        every boot automatically.
       </para>
      </step>
     </substeps>
    </step>
   </procedure>
   <note>
    <title>Epilog Script</title>
    <para>
     The standard epilog script will kill all remaining processes of a user
     on a node. If this behavior is not wanted, disable the standard epilog
     script.
    </para>
   </note>
   <para>
    For further documentation, see the
    <ulink url="https://slurm.schedmd.com/quickstart_admin.html">Quick Start
    Administrator Guide</ulink> and
    <ulink url="https://slurm.schedmd.com/quickstart.html">Quick Start User
    Guide</ulink>. There is further in-depth documentation on the
    <ulink url="https://slurm.schedmd.com/documentation.html">Slurm
    documentation page</ulink>.
   </para>
  </section>

  <!-- <section id="pam_slurm_adopt_conf">
   <title>Enabling the <literal>pam_slurm_adopt</literal> Module</title>
   <para>
    The <literal>pam_slurm_adopt</literal> module allows restricting access to
    compute nodes to those users that have jobs running on them. It can also
    take care of <emphasis>run-away processes</emphasis> from user's jobs and
    end these processes when the job has finished.
   </para>
   <para>
    <literal>pam_slurm_adopt</literal> works by binding the login process
    of a user and all its child processes to the <literal>cgroup</literal>
    of a running job.
   </para>
   <para>
    It can be enabled with following steps:
   </para>
   <procedure>
    <step>
     <para>
      In the configuration file <filename>slurm.conf</filename>, set the option
      <literal>PrologFlags=contain</literal>.
     </para>
     <para>
      Make sure the option <literal>ProctrackType=proctrack/cgroup</literal>
      is also set.
     </para>
    </step>
    <step>
     <para>
      Restart the services
      <systemitem class="daemon">slurmctld</systemitem> and
      <systemitem class="daemon">slurmd</systemitem>.
     </para>
     <para>
      For this change to take effect, it is not sufficient to issue the command
      <command>scontrol reconfigure</command>.
     </para>
    </step>
    <step>
     <!-/- FIXME: Does limiting apply to CPU use only? -/->
     <para>
      Decide whether to limit resources:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        If resources are not limited, user processes can continue running on
        a node even after the job to which they were bound has finished.
       </para>
      </listitem>
      <listitem>
       <para>
        If resources are limited using a <literal>cgroup</literal>, user
        processes will be killed when the job finishes, and the controlling
        <literal>cgroup</literal> is deactivated.
       </para>
       <para>
        To activate resource limits via a <literal>cgroup</literal>, in the
        file <filename>/etc/slurm/cgroup.conf</filename>, set the option
        <literal>ConstrainCores=yes</literal>.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Due to the complexity of accurately determining RAM requirements of jobs,
      limiting the RAM space is not recommended.
     </para>
    </step>
    <step>
     <para>
      Install the package <package>slurm-pam_slurm</package>:
     </para>
     <screen>zypper install slurm-pam_slurm</screen>
    </step>
    <step performance="optional">
     <para>
      You can disallow logins by users who have no running job in the machine:
     </para>
    </step>
   </procedure>
   <para>
   </para>
   <itemizedlist>
    <listitem>
     <formalpara>
      <title>Disabling SSH Logins Only:</title>
      <para>
       In the file <literal>/etc/pam.d/ssh</literal>, add the option:
      </para>
     </formalpara>
     <screen>account     required pam_slurm_adopt.so</screen>
    </listitem>
    <listitem>
     <formalpara>
      <title>Disabling All Types of Logins:</title>
      <para>
       In the file <filename>/etc/pam.d/common-account</filename>, add the
       option:
      </para>
     </formalpara>
     <screen>account    required pam_slurm_adopt.so</screen>
    </listitem>
   </itemizedlist>
  </section> -->

  <section id="fate-318914">
   <!-- href="https://fate.novell.com/318914" -->
   <title>memkind &mdash; Heap Manager for Heterogeneous Memory Platforms and Mixed Memory Policies</title>
   <para>
    The <literal>memkind</literal> library is a user-extensible heap manager
    built on top of <literal>jemalloc</literal> which enables control of
    memory characteristics and a partitioning of the heap between kinds of
    memory. The kinds of memory are defined by operating system memory
    policies that have been applied to virtual address ranges. Memory
    characteristics supported by <literal>memkind</literal> without user
    extension include control of NUMA and page size features.
   </para>
   <para>
    For more information, see:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      the man pages <literal>memkind</literal> and
      <literal>hbwallow</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="https://github.com/memkind/memkind"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <ulink url="https://memkind.github.io/memkind/"/>
     </para>
    </listitem>
   </itemizedlist>
   <note role="compact">
    <para>
     This tool is only available for x86-64.
    </para>
   </note>
  </section>

  <section id="sec.mungekey">
   <title><emphasis>munge</emphasis> Authentication</title>
   <para>
    <emphasis>munge</emphasis>
    allows users to connect as the same user from a machine to any other
    machine which shares the same secret key. This can be used to set up a
    cluster of machines between which the user can connect and execute
    commands without any additional authentication.
   </para>
   <para>
    The <emphasis>munge</emphasis> authentication is based on a single shared key.
    This key is
    located under <filename>/etc/munge/munge.key</filename>. At the installation
    time of the <package>munge</package> package an individual munge key is
    created from the random
    source <filename>/dev/urandom</filename>. This key has to be the same on
    all systems that should allow login to each other:
    To set up <literal>munge</literal> authentication on these machines copy
    the <emphasis>munge</emphasis> key from one machine (ideally a head node of
    the cluster)
    to the other machines within this cluster:
   </para>
<screen>scp /etc/munge/munge.key root@<replaceable>NODE_N</replaceable>:/etc/munge/munge.key</screen>
   <para>
    Then enable and start the service munge on each machine that users are
    expected to log in to:
   </para>
<screen>systemctl enable munge.service
systemctl start munge.service</screen>
   <para>
    If several nodes are installed, you must select and synchronize one key
    to all other nodes in the cluster. This key file should belong to the
    <systemitem class="username">munge</systemitem> user and must have the
    access rights <literal>0400</literal>.
   </para>
  </section>

  <section id="fate-321722">
<!-- href="https://fate.novell.com/321722" -->
   <title>mrsh/mrlogin &mdash; Remote Login Using <emphasis>munge</emphasis> Authentication</title>
   <para>
    <emphasis>mrsh</emphasis> is a set of remote shell programs using the
    <emphasis>munge</emphasis> authentication system instead of reserved ports
    for security.
   </para>
   <para>
    It can be used as a drop-in replacement for <literal>rsh</literal> and
    <literal>rlogin</literal>.
   </para>
   <para>
    To install <emphasis>mrsh</emphasis>, do the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      If only the mrsh client is required (without allowing remote login to
      this machine), use: <command>zypper in mrsh</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      To allow logging in to a machine, the server needs to be installed:
      <literal>zypper in mrsh-server</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      To get a drop-in replacement for <command>rsh</command> and
      <command>rlogin</command>, run: <command>zypper in
      mrsh-rsh-server-compat</command> or <command>zypper in
      mrsh-rsh-compat</command>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To set up a cluster of machines allowing remote login from each other,
    first follow the instructions for setting up and starting
    <emphasis>munge</emphasis> authentication in <xref linkend="sec.mungekey"/>.
    After <emphasis>munge</emphasis> has been successfully
    started, enable and start <command>mrlogin</command> on each machine on
    which the user will log in:
   </para>
<screen>systemctl enable mrlogind.socket mrshd.socket
systemctl start mrlogind.socket mrshd.socket</screen>
   <para>
    To start mrsh support at boot, run:
   </para>
<screen>systemctl enable munge.service
systemctl enable mrlogin.service</screen>
   <para>
    We do not recommend using <emphasis>mrsh</emphasis> when logged in as the
    user <systemitem class="username">root</systemitem>. This is disabled by
    default. To enable it anyway, run:
   </para>
<screen>echo "mrsh" &gt;&gt; /etc/securetty
echo "mrlogin" &gt;&gt; /etc/securetty</screen>
  </section>
 </section>

 <section id="Packages.HPCLibraries">
  <title>HPC Libraries</title>
  <para>
   Library packages which support environment modules follow a distinctive
   naming scheme: all packages have the compiler suite and, if built with
   MPI support, the MPI flavor in their name:
   <literal>*-[<replaceable>MPI_FLAVOR</replaceable>]-<replaceable>COMPILER</replaceable>-hpc*</literal>. To
   support a parallel installation of multiple versions of a library
   package, the package name contains the version number (with dots
   <literal>.</literal> replaced by underscores <literal>_</literal>). To
   simplify the installation of a library, <literal>master</literal>
   -packages are supplied which will ensure that the latest version of a
   package is installed. When these <literal>master</literal> packages are
   updated, the latest
   version of the respective library packages will be installed while
   leaving previous versions installed. Library packages are split between
   runtime and compile time packages. The compile time packages typically
   supply include files and .so-files for shared libraries. Compile time
   package names end with <literal>-devel</literal>. For some libraries
   static (<literal>.a</literal>) libraries are supplied as well, package
   names for these end with <literal>-devel-static</literal>.
  </para>
  <para>
   As an example: Package names of the ScaLAPACK library version 2.0.2 built
   with GCC for Open MPI v1:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     library package:
     <package>libscalapack2_2_0_2-gnu-openmpi1-hpc</package>
    </para>
   </listitem>
   <listitem>
    <para>
     library master package: <package>libscalapack2-gnu-openmpi1-hpc</package>
    </para>
   </listitem>
   <listitem>
    <para>
     development package:
     <package>libscalapack2_2_0_2-gnu-openmpi1-hpc-devel</package>
    </para>
   </listitem>
   <listitem>
    <para>
     development master package:
     <package>libscalapack2-gnu-openmpi1-hpc-devel</package>
    </para>
   </listitem>
   <listitem>
    <para>
     static library package:
     <package>libscalapack2_2_0_2-gnu-openmpi1-hpc-devel-static</package>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   (Note that the digit <literal>2</literal> appended to the library name
   denotes the <literal>.so</literal> version of the library).
  </para>
  <para>
   To install a library packages run <command>zypper in
   <replaceable>LIBRARY-MASTER-PACKAGE</replaceable></command>. To install a
   development file,
   run <command>zypper in <replaceable>LIBRARY-DEVEL-MASTER-PACKAGE</replaceable></command>.
  </para>
  <para>
   Presently, the GNU compiler collection version 4.8 as provided with SUSE
   Linux Enterprise 15 and the MPI flavors Open MPI v.2 and MVAPICH2 are
   supported.
  </para>
  <section id="fate-321716">
   <!-- href="https://fate.novell.com/321716" -->
   <title>FFTW HPC Library &mdash; Discrete Fourier Transforms</title>
   <para>
    <literal>FFTW</literal> is a C subroutine library for computing the
    Discrete Fourier Transform (DFT) in one or more dimensions, of both real
    and complex data, and of arbitrary input size.
   </para>
   <para>
    This library is available as both a serial and an MPI-enabled variant.
    This module requires a compiler toolchain module loaded. To select an
    MPI variant, the respective MPI module needs to be loaded beforehand. To
    load this module, run:
   </para>
<screen>module load fftw3</screen>
   <para>
    List of master packages:
    <itemizedlist>
     <listitem>
      <para>
       <literal>libfftw3-gnu-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>fftw3-gnu-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libfftw3-gnu-openmpi1-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>fftw3-gnu-openmpi1-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libfftw3-gnu-mvapich2-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>fftw3-gnu-mvapich2-hpc-devel</literal>
      </para>
     </listitem>
    </itemizedlist>
   </para>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
  <section id="fate-321710">
   <!-- href="https://fate.novell.com/321710" -->
   <title>HDF5 HPC Library &mdash; Model, Library, File Format for Storing and Managing Data</title>
   <para>
    HDF5 is a data model, library, and file format for storing and managing
    data. It supports an unlimited variety of data types, and is designed for
    flexible and efficient I/O and for high volume and complex data. HDF5 is
    portable and extensible, allowing applications to evolve in their use of
    HDF5.
   </para>
   <para>
    There are serial and MPI variants of this library available. All flavors
    require loading a compiler toolchain module beforehand. The MPI variants
    also require loading the correct MPI flavor module.
   </para>
   <para>
    To load the highest available serial version of this module run:
   </para>
<screen>module load hdf5</screen>
   <para>
    When an MPI flavor is loaded, the MPI version of this module can be
    loaded by:
   </para>
<screen>module load phpdf5</screen>
   <para>
    List of master packages:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>hdf5-examples</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>hdf5-gnu-hpc-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_cpp-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_fortran-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_hl_cpp-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_hl_fortran-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>hdf5-gnu-openmpi1-hpc-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5-gnu-openmpi1-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_fortran-gnu-openmpi1-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_hl_fortran-gnu-openmpi1-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>hdf5-gnu-mvapich2-hpc-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5-gnu-mvapich2-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_fortran-gnu-mvapich2-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>libhdf5_hl_fortran-gnu-mvapich2-hpc</package>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
  <section id="fate-321719">
   <!-- href="https://fate.novell.com/321719" -->
   <title>NetCDF HPC Library &mdash; Implementation of Self-Describing Data Formats</title>
   <para>
    The NetCDF software libraries for C, C++, FORTRAN, and Perl are a set of
    software libraries and self-describing, machine-independent data formats
    that support the creation, access, and sharing of array-oriented
    scientific data.
   </para>
   <bridgehead renderas="sect5"><literal>netcdf</literal> Packages</bridgehead>
   <para>
    The packages with names starting with <literal>netcdf</literal> provide
    C bindings for the NetCDF API. These are available with and without MPI
    support.
   </para>
   <para>
    There are serial and MPI variants of this library available. All flavors
    require loading a compiler toolchain module beforehand. The MPI variants
    also require loading the correct MPI flavor module.
   </para>
   <para>
    The MPI variant becomes available when the MPI module is loaded. Both
    variants require loading a compiler toolchain module beforehand. To
    load the highest version of the non-MPI <literal>netcdf</literal> module,
    run:
   </para>
<screen>module load netcdf</screen>
   <para>
    To load the highest available MPI version of this module, run:
   </para>
<screen>module load pnetcdf</screen>
   <para>
    List of master packages:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>netcdf-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-hpc-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-hpc-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-openmpi1-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-openmpi1-hpc-devel</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-mvapich2-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>netcdf-gnu-mvapich2-hpc-devel</package>
     </para>
    </listitem>
   </itemizedlist>
   <bridgehead renderas="sect5"><literal>netcdf-cxx</literal> Packages</bridgehead>
   <para>
    <package>netcdf-cxx4</package> provides a C++ binding for the NetCDF
    API.
   </para>
   <para>
    This module requires loading a compiler toolchain module beforehand. To
    load this module, run:
   </para>
<screen>module load netcdf-cxx4</screen>
   <para>
    List of master packages:
    <itemizedlist>
     <listitem>
      <para>
       <literal>libnetcdf-cxx4-gnu-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libnetcdf-cxx4-gnu-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>netcdf-cxx4-gnu-hpc-tools</literal>
      </para>
     </listitem>
    </itemizedlist>
   </para>
   <bridgehead renderas="sect5"><literal>netcdf-fortran</literal> Packages</bridgehead>
   <para>
    The <literal>netcdf-fortran</literal> packages provide FORTRAN bindings
    for the NetCDF API, with and without MPI support.
   </para>
   <bridgehead renderas="sect5">For More Information</bridgehead>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
  <section id="fate-321709">
   <!-- href="https://fate.novell.com/321709" -->
   <title>NumPy Python Library</title>
   <para>
    NumPy is a general-purpose array-processing package designed to
    efficiently manipulate large multi-dimensional arrays of arbitrary
    records without sacrificing too much speed for small multi-dimensional
    arrays.
   </para>
   <para>
    NumPy is built on the Numeric code base and adds features introduced by
    numarray as well as an extended C API and the ability to create arrays
    of arbitrary type which also makes NumPy suitable for interfacing with
    general-purpose data-base applications.
   </para>
   <para>
    There are also basic facilities for discrete Fourier transform, basic
    linear algebra and random number generation.
   </para>
   <para>
    This package is available both for Python 2 and Python 3. The specific
    compiler toolchain and MPI library flavor modules must be loaded for this
    library. The correct library module for the Python version used needs to be
    specified when loading this module. To load this module, run:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      for Python 2: <literal>module load python2-numpy</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      for Python 3: <literal>module load python3-numpy</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    List of master packages:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>python2-numpy-gnu-hpc</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>python2-numpy-gnu-hpc-devel</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>python3-numpy-gnu-hpc</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>python3-numpy-gnu-hpc-devel</literal>
     </para>
    </listitem>
   </itemizedlist>
  </section>
  <section id="fate-321708">
   <!-- href="https://fate.novell.com/321708" -->
   <title>OpenBLAS Library &mdash; Optimized BLAS Library</title>
   <para>
    OpenBLAS is an optimized BLAS (Basic Linear Algebra Subprograms) library
    based on GotoBLAS2 1.3, BSD version. It provides the BLAS API. It is
    shipped as a package enabled for environment modules and thus requires
    using Lmod to select a version. There are two variants of this library,
    an OpenMP-enabled variant and a pthreads variant.
   </para>
   <bridgehead renderas="sect5">OpenMP-Enabled Variant</bridgehead>
   <para>
    The OpenMP variant covers all use cases:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Programs using OpenMP.</emphasis> This requires
      the OpenMP-enabled library version to function correctly.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Programs using pthreads.</emphasis> This
      requires an OpenBLAS library without pthread support. This can be
      achieved with the OpenMP-version. We recommend limiting the number of
      threads that are used to 1 by setting the environment variable
      <literal>OMP_NUM_THREADS=1</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Programs without pthreads and without
      OpenMP.</emphasis> Such programs can still take advantage of the
      OpenMP optimization in the library by linking against the OpenMP
      variant of the library.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    When linking statically, ensure that <literal>libgomp.a</literal> is
    included by adding the linker flag <literal>-lgomp</literal>.
   </para>
   <bridgehead renderas="sect5">pthreads Variant</bridgehead>
   <para>
    The pthreads variant of the OpenBLAS library can improve the performance
    of single-threaded programs. The number of threads used can be
    controlled with the environment variable
    <literal>OPENBLAS_NUM_THREADS</literal>.
   </para>
   <bridgehead renderas="sect5">Installation and Usage</bridgehead>
   <para>
    This module requires loading a compiler toolchain beforehand. To select
    the latest version of this module provided, run:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      OpenMP version:
     </para>
<screen>module load openblas-pthreads</screen>
    </listitem>
    <listitem>
     <para>
      pthreads version:
     </para>
<screen>module load openblas</screen>
    </listitem>
   </itemizedlist>
   <para>
    List of master package for:
    <itemizedlist>
     <listitem>
      <para>
       <literal>libopenblas-gnu-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libopenblas-gnu-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libopenblas-pthreads-gnu-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libopenblas-pthreads-gnu-hpc-devel</literal>
      </para>
     </listitem>
    </itemizedlist>
   </para>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
  <section id="fate-321720">
   <!-- href="https://fate.novell.com/321720" -->
   <title>PAPI HPC Library &mdash; Consistent Interface for Hardware Performance Counters</title>
   <para>
    PAPI (package <package>papi</package>) provides a tool with a
    consistent interface and methodology for use of the performance counter
    hardware found in most major microprocessors.
   </para>
   <para>
    This package serves all compiler toolchains and does not require a
    compiler toolchain to be selected. The latest version provided can be
    selected by running:
   </para>
<screen>module load papi</screen>
   <para>
    List of master packages:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <package>papi-hpc</package>
     </para>
    </listitem>
    <listitem>
     <para>
      <package>papi-hpc-devel</package>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
  <section id="fate-321718">
   <!-- href="https://fate.novell.com/321718" -->
   <title>PETSc HPC Library &mdash; Solver for Partial Differential Equations</title>
   <para>
    PETSc is a suite of data structures and routines for the scalable
    (parallel) solution of scientific applications modeled by partial
    differential equations.
   </para>
   <para>
    This module requires loading a compiler toolchain as well as an MPI
    library flavor beforehand. To load this module, run:
   </para>
<screen>module load petsc</screen>
   <para>
    List of master packages:
    <itemizedlist>
     <listitem>
      <para>
       <literal>libpetsc-gnu-openmpi1-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>petsc-gnu-openmpi1-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libpetsc-gnu-mvapich2-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>petsc-gnu-mvapich2-hpc-devel</literal>
      </para>
     </listitem>
    </itemizedlist>
   </para>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
  <section id="fate-321715">
   <!-- href="https://fate.novell.com/321715" -->
   <title>ScaLAPACK HPC Library &mdash; LAPACK Routines</title>
   <para>
    The library ScaLAPACK (short for "Scalable LAPACK") includes a subset of
    LAPACK routines designed for distributed memory MIMD-parallel computers.
   </para>
   <para>
    This library requires loading both a compiler toolchain and an MPI
    library flavor beforehand. To load this library, run:
   </para>
<screen>module load scalapack</screen>
   <para>
    List of master packages:
    <itemizedlist>
     <listitem>
      <para>
       <literal>libblacs2-gnu-openmpi1-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libblacs2-gnu-openmpi1-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libscalapack2-gnu-openmpi1-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libscalapack2-gnu-openmpi1-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libblacs2-gnu-mvapich2-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libblacs2-gnu-mvapich2-hpc-devel</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libscalapack2-gnu-mvapich2-hpc</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>libscalapack2-gnu-mvapich2-hpc-devel</literal>
      </para>
     </listitem>
    </itemizedlist>
   </para>
   <para>
    For general information about Lmod and modules, see
    <xref linkend="fate-321704"/>.
   </para>
  </section>
 </section>

 <!-- <section id="InfraPackArch.ArchIndependent">
  <title>Architecture Independent Information</title>

  <para>
   Information in this section pertains to all architectures supported by
   the &product; &this-version;.
  </para>
 </section> -->

 <!-- <section id="InfraPackArch.x86_64">
  <title>AMD64/Intel 64 (x86_64) Specific Information</title>
  <para>
   Information in this section pertains to the version of the &product; &this-version;
   for the AMD64/Intel 64 architectures.
  </para>

 </section> -->

 <!-- <section id="InfraPackArch.Power">
  <title>POWER (ppc64le) Specific Information</title>
  <para>
   Information in this section pertains to the version of the &product; &this-version;
   for the POWER architecture.
  </para>
 </section> -->

 <!-- <section id="InfraPackArch.SystemZ">
  <title>&ibmz; (s390x) Specific Information</title>
  <para>
   Information in this section pertains to the version of the &product; &this-version;
   for the &ibmz; architecture.
   For more information, see
   <ulink url="http://www.ibm.com/developerworks/linux/linux390/documentation_novell_suse.html"/>
  </para>
 </section> -->

 <!-- <section id="InfraPackArch.AArch64">
  <title>ARM 64-Bit (AArch64) Specific Information</title>
  <para>
   Information in this section pertains to the version of the &product; &this-version;
   for the AArch64 architecture.
  </para>
 </section> -->

 <!-- <section id="Packages">
  <title>Packages</title>
  <para/>
 </section> -->

 <!-- <section id="Packages.New">
  <title>New Packages</title>
  <para/>
 </section> -->

<section id="Packages.Update">
  <title>Updated Packages</title>
  <para/>
  <!-- <section id="update_slurm_18_08_4">
   <title>Slurm Has Been Updated From 17.02.11 to 18.08.4</title>
   <section id="slurm_config_changes">
    <title>Configuration Changes in <filename>slurm.conf</filename></title>
    <para>
     When updating from Slurm 17.02.11 to 18.08.4, make sure to review the
     following important changes to the configuration file
     <filename>/etc/slurm/slurm.conf</filename>:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The epilog script <filename>epilog-clean.sh</filename> was removed
       because of its
       inconsistent behavior when a job finished. To limit the access to
       compute nodes to users who have jobs running on them, use the PAM
       module <literal>pam_slurm_adopt</literal>. For more information, see
       <xref linkend="pam_slurm_adopt_conf"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       The options <literal>ControlMachine</literal>,
       <literal>ControlAddr</literal>, <literal>BackupController</literal>, or
       <literal>BackupAddr</literal> are deprecated and may be removed in the
       future. Replace these options by an
       ordered list of <literal>SlurmCtldHost</literal> records.
      </para>
     </listitem>
     <listitem>
      <para>
       The <literal>PreemptType=preempt/job_prio</literal> has been removed,
       use <literal>PreemptType=preempt/qos</literal> instead.
      </para>
     </listitem>
    </itemizedlist>
   </section>
   <section id="slurm_update_instructions">
    <title>Updating</title>
    <para>
     To update <literal>slurm</literal> from 17.03 to
     18.08.4, proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       Stop all Slurm-related services:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <systemitem class="daemon">slurmctld</systemitem>
        </para>
       </listitem>
       <listitem>
        <para>
         <systemitem class="daemon">slurmd</systemitem>
        </para>
       </listitem>
       <listitem>
        <para>
         <systemitem class="daemon">slurmdbd</systemitem> (if running)
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Create a backup of the configuration files in
       <filename>/etc/slurm</filename>, the <emphasis>Saved State</emphasis>
       directory (defined in
       <literal>StateSaveLocation</literal>), <literal>/var/lib/slurm</literal>
       and also the munge key <filename>/etc/munge/munge.key</filename>.
      </para>
     </step>
     <step performance="optional">
      <!-/- FIXME: Does accounting db server mean a software or does it mean an actual server? -/->
      <para>
       If you are using an accounting database, back up, update, and restart
       the database server:
      </para>
      <substeps>
       <step>
        <para>
         Create a backup of the accounting database, as the update irreversibly
         converts the database.
        </para>
       </step>
       <step>
        <para>
         Update the package <literal>slurm-slurmdbd</literal> with the command:
        </para>
        <screen>zypper update slurm-slurmdbd</screen>
       </step>
       <step>
        <para>
         The conversion begins automatically when
         <systemitem class="daemon">slurmdbd</systemitem> is started the next
         time. However, when starting
         <systemitem class="daemon">slurmdbd</systemitem> as a systemd
         service, the conversion process will likely take so long that the
         command would run into a systemd timeout.
        </para>
        <para>
         Therefore, trigger the database conversion by manually start the
         daemon <systemitem class="daemon">slurmdbd</systemitem> with:
        </para>
        <screen>slurmdbd -D</screen>
        <para>
         Monitor the process until the database conversion is complete.
         When the conversion succeeds, the message
         <literal>Conversion done: success!</literal> will be shown.
        </para>
       </step>
       <step>
        <para>
         Restart the service <systemitem class="daemon">slurmdbd</systemitem>:
        </para>
        <screen>systemctl start slurmdbd</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Update the package <package>slurm-slurmctld</package> and other
       Slurm-related packages:
      </para>
      <screen>zypper update slurm slurm-slurmctld slurm-node</screen>
     </step>
     <step>
      <para>
       Restart the service <systemitem class="daemon">slurmctld</systemitem>:
      </para>
      <screen>systemctl start slurmctld</screen>
     </step>
     <step>
      <para>
       Finally, restart the service <literal>slurmd</literal>:
      </para>
      <screen>systemctl start slurmd</screen>
     </step>
    </procedure>
   </section>
  </section> -->
 </section>

 <!-- <section id="Packages.Removed">
  <title>Removed Packages and Functionality</title>
  <para/>
 </section> -->

 <!-- <section id="Packages.Deprecated.Future">
  <title>Deprecated Packages and Functionality</title>
  <para/>
 </section> -->

 <!-- <section id="Packages.Packaging">
  <title>Changes in Packaging and Delivery</title>
  <para/>
 </section> -->

 <!-- <section id="Miscellaneous">
  <title>Miscellaneous</title>
  <para/>
 </section> -->

 <section id="Legal">
  <title>Legal Notices</title>

  <para>
   &suse; makes no representations or warranties with respect to the
   contents or use of this documentation, and specifically disclaims any
   express or implied warranties of merchantability or fitness for any
   particular purpose. Further, &suse; reserves the right to revise this
   publication and to make changes to its content, at any time, without
   the obligation to notify any person or entity of such revisions or
   changes.
  </para>

  <para>
   Further, &suse; makes no representations or warranties with respect to
   any software, and specifically disclaims any express or implied
   warranties of merchantability or fitness for any particular
   purpose. Further, &suse; reserves the right to make changes to any and
   all parts of &suse; software, at any time, without any obligation to
   notify any person or entity of such changes.
  </para>

  <para>
   Any products or technical information provided under this Agreement may be
   subject to U.S. export controls and the trade laws of other countries. You
   agree to comply with all export control regulations and to obtain any
   required licenses or classifications to export, re-export, or import
   deliverables. You agree not to export or re-export to entities on the
   current U.S. export exclusion lists or to any embargoed or terrorist
   countries as specified in U.S. export laws. You agree to not use
   deliverables for prohibited nuclear, missile, or chemical/biological
   weaponry end uses. Refer to
   <ulink url="https://www.suse.com/company/legal/"/> for more information on
   exporting &suse; software. &suse; assumes no responsibility for your
   failure to obtain any necessary export approvals.
  </para>

  <para>
   Copyright © 2017-<?dbtimestamp format="Y" ?> SUSE LLC. This release notes
   document is licensed under a Creative Commons Attribution-NoDerivs 3.0
   United States License (CC-BY-ND-3.0 US,
   <ulink url="https://creativecommons.org/licenses/by-nd/3.0/us/"/>).
  </para>

  <para>
   &suse; has intellectual property rights relating to technology
   embodied in the product that is described in this document. In
   particular, and without limitation, these intellectual property
   rights may include one or more of the U.S. patents listed at <ulink
   url="https://www.suse.com/company/legal/"/> and one or more
   additional patents or pending patent applications in the U.S. and
   other countries.
  </para>

  <para>
   For &suse; trademarks, see &suse; Trademark and Service Mark list
   (<ulink url="https://www.suse.com/company/legal/"/>). All
   third-party trademarks are the property of their respective
   owners.
  </para>

 </section>
</article>
